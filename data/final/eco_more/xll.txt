Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policymaking. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making. 1. introduction Economic policy formulation is a domain fraught with complexity, with traditional economic models providing limited foresight into the outcomes of policy decisions. Policymakers must not only understand the immediate implications of individual policies but also their aggregate and longterm effects. In addition, human policy-maker incentives are often not aligned with the interests of the general public, and may instead prioritize special interests or reelection (de Figueiredo & Richter, 2014). In light of this, AI-based approaches to policy design that can simulate economies and target different objectives, hold the potential for improved policy understanding and formulation (Zheng et al., 2022; Koster et al., 2022). In an era where AI is gaining increasing attention in governments (House, 2023; Engstrom et al., 2020), it is timely to understand its potential influence on future policy-making. 1Harvard University 2Founding 3Oxford University 4Asari AI 5Google Research. Correspondence to: Edwin Zhang <ezhang@g.harvard.edu>. Figure 1: The proposed framework. The process begins with voting, where human or AI players report preferences on social welfare objectives to a voting mechanism (1). This defines an objective for the Principal, who designs a parameterized N -player Partially Observable Markov Game (POMG) (2). The players are the same as the voters. This POMG unfolds over several timesteps T (3). Following the POMG, game state information is extracted to initiate a new round of voting, with the last POMG state used as the first game state of the new round. This whole process is repeated for τ timesteps. Ideally, such a framework should satisfy the following desiderata:
1. Alignment of policy-makers to the values of constituents, whilst ensuring fair and equitable representation (Barocas et al., 2023). 2. Sufficient model expressivity (Patig, 2004) to accurately represent the intricate governance structures found in the real world, capturing the subtleties and variances of socio-economic interactions. 3. Balance expressiveness with computational tractability, making it feasible to scale to systems with a large number of agents. 4. Build theoretical understanding, enabling systematic analysis and offering a new lens on complex economic models. In this paper, we propose a new framework, Social Environment Design, that lays out an agenda in making progress
ar X
iv :2
40 2. 14 09
0v 1
[ cs
.A I]
2 1
Fe b
20 24
towards these desiderata. In our framework, illustrated in Figure 1, we suggest addressing the concern of a misaligned policy-maker with “Voting on Values (Hanson, 2013),” coupled with a Principal policy-maker who seeks to achieve suggested policy goals. We capture the complexity of a general economic environment whilst maintaining computational tractability by modeling the economy as a Partially Observable Markov Game (POMG), which maintains a fixed observation space for each agent. Finally, we structure our framework as repeatedly finding Stackelberg Equilbria, enabling theoretical understanding by allowing reduction to simpler subproblems. In advancing a research agenda situated in connection with the RL, EconCS, and Computational Social Choice communities, we discuss several open problems of practical and theoretical interest. By introducing this framework, we open a dialogue on AI’s application to economic policy design, aspiring to someday help leverage AI to assist policymakers in enhancing economic resilience and governance effectiveness. In summary, we list our core contributions : 1) We propose the Social Environment Design framework to enable future research in AI-led policymaking in complex economic systems; 2) We release a core implementation of our framework as a Sequential Social Dilemma Environment along with code; and 3) We provide a characterization of open problems, along with prospective solution concepts and algorithmic approaches to forward the dialogue on AI’s application in economic policy design. 2. preliminaries Here we give some preliminaries on several foundational games and solution concepts. Definition 2.1. A (n+ 1)-player Stackelberg-Nash Game S = (n,m,X ,Y,u) comprises one player called the leader and n ∈ N \ {0} players called followers. In a StackelbergNash game, the leader first commits to an action x ∈ X from action space X ⊂ Rm. Then, having observed the leader’s action, each follower i ∈ [n], responds with an action yi in their action space Yi ⊂ Rm. We define the followers’ joint action space Y =×i∈[n] Yi. We refer to a collection of actions y = (y1, · · · , yn) ∈ Y as a followers’ action profile, and to a collection (x, y) ∈ X × Y as an action profile. After all players choose an action, the leader receives payoff uo(x,y) ∈ R, while each follower i ∈ [n] receives payoff ui(x,y) ∈ R. Each player i ∈ [n] aims to maximize her payoff, and the leader aims to maximize her payoff assuming the followers will best respond. Fixing the leader’s action x ∈ X , a Stackelberg Nash game S induces a lower-level Nash game GS =
(n,m,Y,u−0(x, ·)) among the followers. Definition 2.2. A Partially Observable Markov Game (POMG) M with n agents is a tuple (S,A, T, r,Ω, B, γ, µ0). Here, S is a shared state space for all agents; A = ×i∈[n] Ai is the joint action space; T : S × S × A → [0, 1] is a stochastic transition function; r : S × A → Rn is the reward function with r = (r1, · · · , rn); Ω =×i∈[n] Ωi is the joint observation space; B : Ω×S×A → [0, 1] is the stochastic observation function; γ ∈ [0, 1) is a discount factor; µ0 ∈ ∆(S) is the initial state distribution. An agent’s behavior in this game is characterized by its policy πi : Ω → A, which maps observations to actions. Definition 2.3. A (n + 1)-player Stackelberg-Markov Game S = (n,m,Φ,Π,u) comprises one player called the leader and n ∈ N \ {0} players called followers. In a Stackelberg-Markov game, the leader first commits to an action ϕ ∈ Φ from action space Φ ⊂ Rm which induces a n-player low-level (Partially Observable) Markov Game Mϕ = (S,Aϕ, Tϕ, rϕ,Ωϕ, Bϕ, γϕ, µϕ0 ). Then, having observed the leader’s action, each follower i ∈ [n], responds with an policy πi : Ω → Ai in their policy space Πi. We define the followers’ joint action space Π =×i∈[n] Πi. We refer to a collection of policies π = (π1, · · · , πn) ∈ Π as a followers’ policy profile. After all players choose an action, the leader receives payoff uo(ϕ, π) ∈ R, while each follower i ∈ [n] receives payoff ui(ϕ, π) = EM ϕ,π[ ∑∞ t=0(γ ϕ)trϕ(st, at)] ∈ R. Each player i ∈ [n] aims to maximize her payoff, and the leader aims to maximize her payoff assuming the followers will best respond. For all followers i ∈ [n], we define the δ-bestresponse correspondence BRδi (ϕ, π−i) = {πi ∈ Πi | ui(ϕ, π) ≥ maxπi∈Πi ui(ϕ, (πi, π−i)) − δ} and the joint δ-best-response correspondence BRδ(ϕ, π) = ×i∈[n] BRδi (ϕ, π−i). Definition 2.4. A (ε, δ)-strong Stackelberg-MarkovNash equilibrium (SSMNE) in a Stackelberg-Markov game S = (n,m,Φ,Π,u) is an action profile (ϕ∗, π∗) ∈ Φ×Π such that u0(ϕ∗, π∗) ≥ maxϕ∈Φ maxBRδ u0(ϕ, π)− ε and ui(ϕ∗, π∗) ≥ maxπi∈Πi ui(ϕ∗, (πi, π∗−i))− δ, for all i ∈ [n]. Definition 2.5. A (One-Shot) Mechanism Design problem P = (n,m, T , S, t,u, u0, f) comprises of n agents, each i ∈ [n] owns a private type ti ∈ Ti from a set of possible types Ti ⊂ Rm . An agent’s preferences over outcomes s ∈ S, for a set S of outcomes, can be expressed in terms of a utility function that is parameterized by the type. Let ui(s, ti) denote the utility of agent i for outcome s ∈ S given type ti. A strategy (policy in RL) si : Ti → Ai is a complete decision
rule, that defines the action an agent will select in every distinguishable state of the world. Let ai = si(ti) ∈ Ai denote the action of agent i given type ti, where Ai is the set of all possible actions available to agent i. A mechanism M = ({Ai}i∈[n], g) defines the set of actions Ai available to each agent i, and an outcome rule g : ×i∈[n] Ai → S, such that g(a) is the outcome implemented by the mechanism for action profile a = (a1, · · · , an). u0 : M××i∈[n] Ai → R is a principal objective function, where u0(M,a) represents the expected utility/revenue of the principal when mechanism designer chooses mechanism M and agents choose action profile a. Note that u0 is an alternative way of defining the social choice function. The goal of the mechanism designer is to design a mechanism M = ({Ai}i∈[n], g) ∈ M that maximizes u0(M,a
∗), where strategy profile a∗ = (a∗1, · · · , a∗n) is an (Nash, Bayesian-Nash, dominant-strategy) equilibrium to the game induced by M . 3. formal definition of social environment design game Definition 3.1. A Social Environment Design Game S = (Φ, P, ϕ0, D, δ,Θ,O, f) is a one-leader-nfollower onlinea Stackelberg-Markov Game, where
• Φ ⊆ Rk is the principal action space;
• P : Φ 7→ Mϕ is a policy implementation map that maps from a principal action ϕ ∈ Φ to a parameterized POMG Mϕ = (S,Aϕ, Tϕ, rϕ,Ωϕ, Bϕ, γϕ, µϕ0 );
• ϕ0 ∈ Φ is some initial action;
• D : Φ × Φ 7→ R≥0 is a divergence measure on the leader action space;
• δ > 0 is the divergence constraint;
• Θ ⊆ R(n+1)×m is the type space. • O = {Oi}i∈[m] is some set of predefined social welfare functions, where each O maps Φ×Π 7→ R. We give examples of several possible choices of objectives below in Social Welfare Examples. Π here refers to the set of all possible policy profiles in the parameterized POMG;
• f : Θ 7→ O is a social choice function representing the voting mechanism. aHere, online means that the Stackelberg-Markov Game is repeatedly played, with the first state of a new round made equal to the final state of the last round. We define the Social Environment Design Game formally as repeatedly finding a Stackelberg Equilibrium in a Markov Game (Gerstgrasser & Parkes, 2023; Brero et al., 2022), iterated over several rounds of voting. At a high level, we frame the economic design problem as a Stackelberg game between the policy designer and economic participants. The economic participants first vote for a given objective, or values to optimize for. Subsequently, the Principal (leader) attempts to maximize this objective by designing the rules of an economic system, which induce an environment for the participants. We model this environment as a Partially Observable Markov Game (POMG) with theparticipants as the agents. We refer to this as the Social Environment Design Game because it generalizes mechanism design in a number of ways; e.g., it involves voting on goals, and it involves the design of an economic policy for an economic environment in which agents take actions and report types. Further analysis and breakdown of Definition 3.1. First we make a note regarding our type space. Since we define the first row of a specific type instantiation θ ∈ Θ to be the type of the principal, Θ has (n + 1) rows. We thus refer to Θ1 to be the type space of the principal and Θ−1 to be the type space of all participants. In addition, Θ can be added to the state space of the POMG, which allows dynamic types that change over time in response to the state of the game. We do not allow the Principal to directly manipulate or observe the state space. Thus, we can embed the type space within the state space to hide it from the Principal. Even with elements of the POMG that the Principal does have control over, such as the state transition function Tϕ, one can enforce hard constraints on how much power the Principal has to change the function explicitly through the divergence D or implicitly through the implementation map P . We remark that both the infinite-horizon and finite-horizon version of the Social Environment Design Game can be considered. In contrast to standard Reinforcement Learning (RL), we do not need to introduce a discount factor for the infinite-horizon version, as we consider the Principal only maximizing for the objective at the current voting round since our model’s objective changes at each round. This is perhaps a naive objective, and other continual objectives could instead be considered. Exploring the tradeoffs between the greedy objective and more complex continual objectives is left as an important open problem for future work. In the finite horizon case, we add an additional time horizon T to our game. We now proceed to a detailed breakdown of our game. The Social Environment Design Game can be cleanly divided into a Voting Mechanism and Stackelberg Game,
which is played with the Principal’s objective determined by the Voting Mechanism. Definition 3.2. The Voting Mechanism is defined as V = (O, f,Θ). We use the standard axiomatic model (Arrow, 2012), where O is the set of alternatives, f is the social choice function, and Θ is the type space, or set of all preference profiles. Intuitively, a specific agent i’s type θi for row i in θ ∈ Θ, can be thought of as some latent vector which represents the agent’s values. This type contains all information necessary for recovering a partial ordering over alternatives, a more specific way of defining preferences. The goal of the Voting Mechanism is then to define a objective for the Principal to optimize, given these types. To do so, we define the Voting Mechanism f and ask the players for a preference report θ−1 ∈ Θ−1, which does not necessarily have to be truthful. It remains an open problem whether some notion of approximate incentive compatibility can be achieved by the principal. The Voting Mechanism then computes the objective O = f(θ1, θ−1) as a result of the vote. Here, we set θ1 ∈ Θ1 to be the preferences of the Principal. Importantly, the objective function includes the full θ, which allows expressing preferences of the principal if one wished to encoded a form of ”moral objectivity”, or other biases. We also make this modeling choice for generality, as it allows our model to express mechanisms such as auctions where the objective of the principal may be entirely selfish and not depend at all on the participant’s types. Social Welfare Examples. Examples of social welfare functions that could be included in the voting set are the Utilitarian objective O(ϕ, π) = ∑ i J(π ϕ i ), where J is the
expected discounted return J = ∑
t(γ ϕ)trϕi (st, ai,t, a−i,t),
π is the tuple of all agents π = (πi)i∈[n], and πi are singular agents that map Ωϕi → A ϕ i . Other possible choices include
the Nash Welfare objective O = ( ΠiJ(π ϕ i ) )1/n , as well as the Egalitarian objective O = mini J(π ϕ i ). These are perhaps the most commonly discussed objectives, but bespoke or custom welfare functions could also be considered and added to the set of alternatives. Definition 3.3. The Stackelberg Game I = (Φ, P,D, δ, ϕ0) is a Stackelberg-Markov Game. The Stackelberg Game game is played subsequently after the Voting Mechanism, and can be thought of as a single timestep of the full game. The Principal (leader) will choose action ϕ ∈ Φ which induces a parameterized Induced Economy Mϕ = (S,Aϕ, Tϕ, rϕ,Ωϕ, Bϕ, γϕ, µϕ0 ) through the policy implementation map P : ϕ 7→ Mϕ. Note that if agent preferences change over time, this can be modeled by adding agent types into the state space of the POMG. The transition function T would then be able to express changes
in preferences over time. Thus, the objective of the leader in the Stackelberg Game game is to design a POMG, given the objective O decided prior in the Voting Mechanism:
max ϕ O(ϕ, π)
s.t. D(ϕ0, ϕ) ≤ δ
µ P (ϕ) 0 = ∆(sT ). (1)
Again, π here is the tuple of all agents π = (πi)i∈[n], and πi individual agents that map Ωϕi → A ϕ i . Our notation µ P (ϕ) 0 denotes the µ0 of the tuple P (ϕ), and ∆(st) refers to a Delta Dirac distribution centered on sT . Therefore, the second constraint µP (ϕ)0 = ∆(sT ) forces the ϕ to choose a POMG that has the same initial state as the terminal state of the last round, so that continuity is kept between rounds. Lastly, we remark that this constrained optimization can also be transformed into an unconstrained problem by using an additional reparameterization R : ξ 7→ Φ̂, where ξ ∈ Ξ := RL and Φ̂ := {ϕ | D(ϕ0, ϕ) ≤ δ}. The optimization can then proceed in RL with no constraints. In this case, the Stackelberg game would reduce to I = (Ξ, P ′), where P ′ = P ◦ R. Definition 3.4. The Induced Economy is a Partially Observable Markov Game Mϕ = (S,Aϕ, Tϕ, rϕ,Ωϕ, Bϕ, γϕ, µϕ0 ). Finally, the Induced Economy is defined as the POMG produced as the output of the principal. Agents within the POMG interact with one another and attempt to maximize their utility according to their true preferences. The n economic participants (followers) will play strategically in the parameterized POMG Mϕ. At each step t of the game, every follower i chooses an action ai,t from their action space Ai, the game state evolves according to the joint action at = (a1,t, · · · , an,t) and the transition function T , and agents receive observations and reward according to B and r. An agent’s behavior in the game is characterized by its policy πi : Ω ϕ i → A ϕ i , which maps observations to actions. Each follower in the POMG Mϕ individually seeks to maximize its own (discounted) total return∑
t(γ ϕ)trϕi (st, ai,t, a−i,t). 4. example: apple picking game In order to give a motivating example for how preference elicitation for the principal in Social Environment Design can be used to align policy-maker incentives, we have created a Sequential Social Dilemma Game inspired by the Harvest Game proposed in Perolat et al. (2017). The game is illustrated in Figure 2. The aim in Harvest Game is to collect apples, with each apple yielding a reward. If all apples
in an area are harvested, they never grow back. The dilemma arises when individual self-interest drives rapid harvesting, which could permanently deplete resources. Thus, agents must sacrifice personal benefit and cooperate for collective well-being. One potential solution to this dilemma is through the use of a central government that taxes and redistributes apples. Thus, we have created a new game inspired by Zheng et al. (2022) in which a principal designs tax rates on apple collection and players vote on Utilitarian (productivity) vs. Egalitarian (equality) objectives for the principal. As players interact within this evolving environment, the principal faces the challenge of crafting policies that balance immediate economic incentives with sustainability goals. In order to achieve this, the principal must foster cooperation among players, guiding them towards the Pareto-Efficient equilibrium that the players have chosen. To build intuition for how the Apple Picking Game maps onto the theoretical framework described in section 3, we give a more formal definition of the game here. Recall the definition of a Social Environment Design Game S = (Φ, P, ϕ0, D, δ,Θ,O, f). The action space for the environment designer Φ is defined as tax weights 0 ≤ Φi ≤ 1, determining the percentage of income to be taxed for each bracket. For simplicity we evenly redistribute the taxes, and set the number of tax brackets to three. In this environment, the Principal can only change the reward function of the induced POMG, so Mϕ = (S,A, T, rϕ,Ω, B, γ, µ0). The
type of each agent is defined as (σ, β), where σ refers to the selfishness of the agent and β refers to the trust the agent holds in the Principal. In some sense this is a rough approximation for the political spectrum (Heywood, 1998). Finally, let ai be the number of apples collected for a given agent i. We can now define the policy implementation map P , which in this case reduces to the parameterization of the reward function:
ri(a, ϕ) = βirextrinsic(a, ϕ) + (1− βi)ri,intrinsic(a). (2)
Here, the intrinsic reward is an average between the apples an agent collects and the apples all other agents collect within its field of view weighted by the selfishness of the agent. On the other hand, the extrinsic reward is the amount of apples after tax an agent collects plus an equal share of the redistributed total tax:
ri,intrinsic(a) = σia+ (1− σi)( ∑ j aj − a),
rextrinsic(a, ϕ) = (a− T (a, ϕ)) + 1
n ∑ j T (aj , ϕ),
where tax T (a, ϕ) = B−1∑ b=0 ϕb · ((τb+1 − τb)1[a > τb+1]
+ (a− τb)1[τb < a ≤ τb+1]). Here, [τb, τb+1] refer to the tax brackets. Importantly, the principal can only incentivize agents through the extrinsic reward and cannot directly observe the intrinsic reward. The degree to which the principal holds power over the agent depends on the agent’s trust or belief in the principal. We sample both selfishness and trust uniformly over [0, 1], and keep them fixed during training. Allowing them to change over time either randomly or in some fashion dependent on the performance of the Principal is left for future work. The objective space of the Principal is defined as O = {η ∑ i,t ai,t + (1 − η)mini ∑ t a(i, t) | 0 ≤ η ≤ 1}, an interpolation between the Utilitarian and Egalitarian objective. A simple social choice function f(σ) = η can be defined as the average of agent selfishness: f(σ) = 1 n ∑ i σi. In this setting, we leave the Principal optimization unconstrained and thus do not need to define D or δ. ϕ0 is initialized to 0, or no tax. We run several tax periods per voting round, and at the end of each the principal decides on a new tax rate, for each bracket, for the next period - as well as calculating, applying and redistributing tax to the players for that entire period, delivered in the players’ final extrinsic reward. We release a high quality version of our code designed for fast experimentation and further research in the supplementary material, and include environment hyperparameters and training details in Appendix A. We do not include results as the primary purpose of this paper is to propose a future research agenda
and illustrate open problems within it. Preliminary results were promising, but further analysis is warranted. 5. challenges and open problems Based on the AI-led economic policy-making framework presented, the following key open problems of our framework are proposed for further exploration: Preference aggregation and democratic representation in voting mechanisms is a complex challenge that requires advanced algorithms to reflect collective preferences while respecting minority views, as well as ensuring that the simulated population is representative and their preferences correctly modeled. Modeling human behavior within the simulator is another key challenge, and points towards possibly incorporating bounded-rationality into MARL (Wen et al., 2019a) or role-based modeling (Wang et al., 2020; 2021b). To ensure responsible AI governance and accountability, responsible oversight mechanisms must be established. Furthermore, exploring socioeconomic interactions within these systems is critical, especially in understanding and deriving the conditions for convergence to and definition of the Principal’s objective. As our framework is positioned within a continual learning setting, it is important to redefine what an optimal Principal looks like in this context. Finally, scaling laws of the framework should be analyzed in order to fully model real-world complexities. Can the framework handle simulating economies with thousands or millions of agents? What is the role of scale? When is simulation useful, and when does it fail? These remain important open problems for future research. We give a more detailed outline for several of the above problems for further consideration. Preference Aggregation and Democratic Representation. Aggregation algorithms within the Voting Mechanism: The development of sophisticated algorithms that can effectively aggregate disparate and potentially conflicting preferences of diverse agent populations is a significant challenge. These algorithms must ensure that the outcomes represent collective preferences without overwhelming the minority views. Incorporating diverse decision-making models: The framework must be flexible enough to respect various cultural, ethical, and socioeconomic decision-making paradigms that different groups of agents might exhibit. In addition, such agents should imitate humans well, which we expand on further in the next section. Modeling Human Behavior. Bounded Rationality: Human decision-making in economic settings often demonstrates bounded rationality, where decisions are made based on satisficing rather than optimizing behavior. Further research is required to develop AI agents that can capture such nuances in human decision-making. Cognitive and Behavioral Biases: Human economic behavior is influenced by a variety of cognitive biases. For instance, time-inconsistent preferences can lead to procrastination and problems with self-control, and loss aversion can skew risk preferences. AI agents within this framework need to capture these biases for accurate representation of human economic behavior. Interaction and Network Effects: Humans do not make economic decisions in isolation; their decisions are profoundly influenced by their interactions with others. This opens another avenue of research in modeling these network effects accurately within the agent behavior models. Higher order effects are oftentimes essential to understanding the behavior real-world systems. ai governance and accountability. Transparent decision-making processes: AI systems involved in policy-making need to have their decision-making processes be fully transparent. The creation of interpretable AI models that can provide explanations for suggested policies is essential for trust and accountability. Legal and ethical frameworks for AI decisions: There is an urgent need to establish legal and ethical frameworks that delineate the responsibilities and liabilities associated with AI-driven decision-making. These frameworks should set guidelines for what constitutes fair and lawful AI behavior in an economic context. Oversight and human-AI collaboration: Establishing effective oversight mechanisms that involve both AI and human collaboration is critical. The role of human experts in supervising and guiding AI decisions, and their ability to intervene when AI-driven policies deviate from desired outcomes is still to be determined. Convergence to Desired Outcomes. Existence and characterization of forms of convergence or equilbria: Can we characterize the conditions under which an equilibrium will exist in such complex socioeconomic interactions. The uniqueness or multiplicity of equilibria and the conditions under which they are attained are also interesting to study. Also, conventional game-theoretic equilibria may not be the right object of study, as empirically these economic systems may never converge to a single, stable behavior. Influence of dynamic changes on convergence points: The complex dynamics of economic systems call for a deep understanding of the sensitivity of equilibria to shocks and changes in the environment and agent behavior from variables that may have been unforeseen by the principal. Ensuring the robustness and stability of the principal to be able to recover from such shocks is also of importance. Scaling Laws and Computational Efficiency. Scaling up the model to larger systems: The proposed framework needs to be scaled to simulate economies of increasingly complexities. This comprises accommodating an increasing number of agents and more intricate interactions among them. Scaling laws of the model parameters and the computational resources required need to be examined. Efficient learning and decision-making algorithms: Efficient algorithms for learning agent behavior and optimizing the policy design are crucial for the practicality of the framework. Particularly, the principal must be sample efficient, as every step it takes induces an entire MARL optimization. Massive parallelization: To tackle real-world complex systems, embracing the advantage of high-performance computing is necessary. This includes implementing the framework with massively parallel computations for both the learning and the decision-making processes. Techniques for splitting these processes into smaller tasks that can be processed simultaneously, as well as the efficient management of these tasks, represent challenging aspects to be addressed. 6. related work The concept of environment design was first proposed by Zhang et al. (2009) and focused on the single agent setting. In contrast, our framework resides between various strands of research, including but not limited to economic policy design, Stackelberg game learning, multi-agent reinforcement learning, mechanism design, and computational social choice. In this section, we delve into a comprehensive exploration of its connections with prior research. 6.1. economic policy design and simulation Several approaches to automated economic policy design have been proposed in the past (Liu et al., 2022; Curry et al., 2023; Yang et al., 2021), and how usage of AI may span both participation in and design of economic systems (Parkes & Wellman, 2015). Here we cite several that are most related to our proposed framework and research agenda. Perhaps most related to our approach is the Human Centered Mechanism Design line of work from Koster et al. (2022); Balaguer et al. (2022). They propose learning mechanisms from behavioral models trained on human data, with the mechanism objective attempting to satisfy a majoritarian vote of the human participants. However, their work differs from ours in several key ways; firstly, they do not consider a fully general economic environment and limit their scope only to a generalization of the linear public goods setting. In other words, our framework encompasses Environment Design whilst theirs encompasses only Mechanism Design. Secondly, the voting that is defined within their framework is taken over actual mechanisms proposed by the designer and
is by majority, whereas our voting is taken explicitly over Principal objectives and does not specify a majority vote, which allows potentially addressing issues such tyranny of the masses in future work. A more general game environment is illustrated in the AI Economist (Zheng et al., 2022), although they make strong assumptions on the goal of the Principal is and do not allow participants to adjust it. 6.2. stackelberg game From the perspective of the Principal, it plays a Stackelberg game with agents of different types. Stackelberg games model many real-world problems that exhibit a hierarchical order of play by different players, including taxation (Zheng et al., 2022), security games (Jiang et al., 2013; Gan et al., 2020), and commercial decision-making (Naghizadeh & Liu, 2014; Zhang et al., 2016; Aussel et al., 2020). In the simplest case, a Stackelberg game contains one leader and one follower. For these games with discrete action spaces, Conitzer & Sandholm (2006) show that linear programming approaches can obtain Stackelberg equilibria in polynomial time in terms of the pure strategy space of the leader and follower. To find Stackelberg equilibria in continuous action spaces, Jin et al. (2020); Fiez et al. (2020) propose the notion of local Stackelberg equilibria and characterize them using first- and second-order conditions. Moreover, Jin et al. (2020) show that common gradient descent-ascent approaches can converge to local Stackelberg equilibria (except for some degenerate points) if the learning rate of the leader is much smaller than that of the follower. Fiez et al. (2020) give update rules with convergence guarantees. Different from these works, in this paper, we consider Stackelberg games with multiple followers. More sophisticated than its single-follower counterpart, unless the followers are independent (Calvete & Galé, 2007), computing Stackelberg equilibria with multiple followers becomes NP-hard even when assuming equilibria with a special structure for the followers (Basilico et al., 2017). Recently, Wang et al. (2021a) propose to deal with an arbitrary equilibrium which can be reached by the follower via differentiating though it. Gerstgrasser & Parkes (2023) proposes a meta-learning framework among different policies of followers to enable fast adaption of the principal, which builds upon prior work done by Brero et al. (2022) who first introduced the Stackelberg-POMDP framework. Multi-agent reinforcement learning holds the promise to extend Stackelberg learning to more general and realistic problems. Tharakunnel & Bhattacharyya (2007) propose Leader-Follower Semi-Markov Decision Process to model the sequential Stackelberg learning problem. Cheng et al. (2017) propose Stackelberg Q-learning but without any convergence guarantee. Shu & Tian (2019); Shi et al. (2019) study leader-follower problems from an empirical perspec-
tive, where the leader learns deep models to predict the followers’ behavior. 6.3. multi-agent reinforcement learning Another important component of the proposed framework is the followers’ behavior learning. Deep multi-agent reinforcement learning algorithms have seen considerable advancements in recent years. Notable contributions such as COMA (Foerster et al., 2018), MADDPG (Lowe et al., 2017), PR2 (Wen et al., 2019b), and DOP (Wang et al., 2021d) address policy-based MARL challenges. These approaches leverage a (decomposed) centralized critic for computing gradients to decentralized actors. Conversely, valuebased algorithms decompose the joint Q-function into individual Q-functions, facilitating efficient optimization and decentralized implementation. Techniques like VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), QTRAN (Son et al., 2019), and Weighted QMIX (Rashid et al., 2020) incrementally enhance the mixing network’s representational capacity. Additional investigations explore MARL through coordination graphs (Guestrin et al., 2002b;a; Böhmer et al., 2020; Kang et al., 2022; Wang et al., 2021c; Yang et al., 2022), communicative strategies (Singh et al., 2019; Mao et al., 2020; Wang et al., 2019; Kang et al., 2020), diversity (Li et al., 2021), and also expressive neural network architectures like Transformer (Wen et al., 2022), offering insights for participant learning without directly addressing human behavior modeling. For modeling behavior, role-based learning frameworks (Wang et al., 2020; 2021b) are the most related to our work. They learn the roles of different agents autonomously and enhance learning efficiency by decomposing the task and learning sub-task-specific policies. However, these works are majorly studied in the setting of the Decentralized Partially Observable Markov Decision Process (Dec-POMDP), and are thus different from our work by two points: (1) The reward is shared among agents; and (2) The dynamics, including reward and transition dynamics, are static in these models. There likely would exist significant challenges in generalizing these to non-shared reward settings that are essential for many economic applications. 6.4. computational social choice Computational social choice is an interdisciplinary field combining computer science and social choice theory, focusing on the application of computational techniques to social choice mechanisms (such as voting rules or fair allocation procedures) and the theoretical analysis of these mechanisms with computational tools (Brandt et al., 2016). A fundamental component of the field is the study of manipulative behavior in elections and other collective decisionmaking processes, as well as the design of systems resistant
to manipulation (Elkind et al., 2010; Procaccia, 2010). This area of study will likely inform the development of the Voting Mechanism, and thus merits much consideration in our framework. Additionally, computational social choice attempts to optimize the fair distribution of resources, often involving complex allocation problems (Thomson, 2016; Procaccia, 2016), another area for drawing inspiration from for development of human baselines to compare against the Principal. However, our work also differs significantly in considering individual values within elections. Computational social choice typically assumes a discrete set of alternatives. This requires voters to express their values through support of a candidate that shares similar values. On the other hand, our framework enables voters to directly report their values in a continuous type space θ. This allows the voters to more precisely express values, without having to rely on a discrete set of candidates or policies who may not be exactly aligned with their personal θ. 6.5. automated mechanism design Automated Mechanism Design has a rich history somewhat similar in motivation to ours, and was first introduced by Sandholm (2003), where search algorithms are used to computationally create specific rule sets (mechanisms) for games that lead to desirable outcomes even when participants act in self-interest. More recently, the work of automated mechanism design has been advanced through deep learning, in the framework known as differentiable economics. Dütting et al. (Forthcoming 2023) use deep neural network to learn the allocation and payment rules of auctions. Since then, a line of follow-up work has been introduced, extending the framework to make the architecture more powerful and general (Shen et al., 2021; Ivanov et al., 2022; Duan et al., 2023; Curry et al., 2022; Wang et al., 2023). Deep learning methods have also been explored in equilibrium calculation (Kohring et al., 2023; Bichler et al., 2023; 2021). While these techniques are applied to settings much less general than ours, architectural details may be useful in building a Principal. 7. conclusion In this paper, we present a theoretical framework for both policy design and simulation that merges economic policy design with AI to potentially help better inform economic policy-making. It is designed to tackle issues such as preference aggregation and counterfactual testing in complex economic systems. Significant challenges, including democratic representation and accountability in AI-driven systems, are highlighted. We hope to engage interdisciplinary expertise and foster collaborative innovation, and aspire to help create AI systems that not only enhance economic resilience and governance effectiveness but also uphold democratic ideals and ethical standards. 8. impact statement This paper sets out an agenda in Social Environment Design, suggesting that AI holds promise in improving policy design by proposing a general framework that can simulate general, socioeconomic phenomena and scale to large settings. There are several relevant considerations that are important to take-up in advancing this framework towards adoption by policy-makers. For example, its effectiveness depends on capturing all pertinent stakeholders within a given scenario. Related, is to ensure that agent modeling is consistent with the diverse motivations and incentives of people, firms, and other entities. Lastly, any real-world trial of this initiative should engage vigorously and faithfully with non-technical stakeholders. a. environment hyperparameters and training details Here we give a detailed breakdown of several key hyperparameters and Training Details within our environment in section 4. We use PPO (Schulman et al., 2017) player agents with parameter sharing and GAE (Schulman et al., 2015), collecting samples at a horizon shorter than the episode length to perform multiple policy update iterations per episode. The principal has separate, discrete, action subspaces for each tax bracket, and is also trained by standard PPO at the same time-scale as the player agents. We follow a two-phase curriculum with tax annealing, as suggested in Zheng et al. (2022). This annealing can be formalized as a constraint in the policy implementation map by simply bounding the maximum tax percentage that can be set. It is worth noting, however, that training the principal in this way is susceptible to issues of non-stationarity, and we refer to Yang et al. (2021) for a discussion on alternatives.