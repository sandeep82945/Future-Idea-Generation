In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their “cognitive” processes opaque, challenging even their designers’ understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine. Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM. Further enriching our analysis, we procure an additional dataset through interactions with the GPT-4 based knowledge retrieval API, unveiling a congruent text preference between the RAG API and Bing Chat. This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat’s developers. Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines. Collectively, these findings illuminate the unique sources, preferences, and potential economic implications of chat-based search engines’ content selection behavior, providing fresh perspectives for both academic research and practical applications within the search engine domain. crafting knowledge: exploring the creative  mechanisms of chat-based search engines Lijia Ma* Michael G. Foster School of Business, University of Washington, Seattle, WA 98195, lijiam@uw.edu
Xingchen (Cedric) Xu* Michael G. Foster School of Business, University of Washington, Seattle, WA 98195, xcxu21@uw.edu
Yong Tan† Michael G. Foster School of Business, University of Washington, Seattle, WA 98195, ytan@uw.edu
In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their “cognitive” processes opaque, challenging even their designers’ understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine. Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM. Further enriching our analysis, we procure an additional dataset through interactions with the GPT-4 based knowledge retrieval API, unveiling a congruent text preference between the RAG API and Bing Chat. This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat’s developers. Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines. Collectively, these findings illuminate the unique sources, preferences, and potential economic implications of chat-based search engines’ content selection behavior, providing fresh perspectives for both academic research and practical applications within the search engine domain. Key words : Generative AI, RAG, AI Creativity, AI Metacognition, AI Alignment, Search Engine, NLP
1
ar X
iv :2
40 2. 19 42
1v 1
[ cs
.I R
] 2
9 Fe
b 20
24 1. introduction In the contemporary digital landscape, the volume and variety of information are expanding rapidly. To bridge the gap between information providers and seekers, search engines have become indispensable tools in everyday life. Data from Serpwatch reveals that 87% of mobile device users engage with search engines at least once daily. Google, for instance, processes an estimated 63,000 search queries per second, translating to approximately 2 trillion global searches each year1. Given the dominant role of search engines in facilitating information access, website owners aspire to gain increased visibility on these platforms, whether for the consumption of their textual content or the purchase of products offered on their websites (Berman and Katona 2013, Jansen et al. 2008). However, the search engine landscape is currently experiencing a paradigmatic shift owing to the emergence of Large Language Models (LLMs) and associated applications, especially those based on Retrieval Augmented Generation (RAG). On November 30, 2022, OpenAI introduced ChatGPT, a tool designed to address user queries in a conversational manner. Subsequently, Microsoft incorporated GPT-4 into its search engine, culminating in the debut of New Bing on February 7, 20232. This iteration of Bing integrates a chatbot, hereafter referred to as Bing Chat3, enabling online searches and the delivery of responses in comprehensive natural language. Concurrently, on March 21, 2023, Google commenced experimental trials of its chatbot-enhanced search engine, Bard, which has seen its monthly user base surpass 142 million as of early 20244. This novel category of chatbots, exemplified by Bing Chat and Bard, merges search engine capabilities with LLM-based generative artificial intelligence (AI), and are hence termed “chat-based search engines”. In contrast, their predecessors are hereafter denoted as “conventional search engines”. ∗ These authors contributed equally to the manuscript and are listed alphabetically. † Corresponding author 1 See https://serpwatch.io/blog/search-engine-statistics/. 2 See https://techcrunch.com/2023/03/14/microsofts-new-bing-was-using-gpt-4-all-along/. 3 It is now part of Microsoft Copilot. See https://copilot.microsoft.com/ 4 See https://www.demandsage.com/google-bard-statistics/. initiate their process by comprehending the query, subsequently assimilating information from pertinent websites, and ultimately delivering a coherent response that integrates references to multiple websites for substantiation, during which they demonstrate human-like metacognitive skills, including the acquisition of new knowledge and the demonstration of creativity. Nevertheless, the “cognitive” procedure through which the chatbot discerns pertinent information and formulates the final responses remains largely inscrutable, attributed to the complexity of its foundational model. As the parameter size of LLMs increases, elucidating the mechanism becomes increasingly unfeasible, even for the system’s architects (Kosinski 2023). Delving into this machine-driven creative process is not only of scientific intrigue but also bears significant economic implications. Prior research in the realm of search engines has shed light on the considerable impact of search engine visibility on both the decision-making processes of individuals (Gong et al. 2018, Ghose et al. 2014) and the distribution of welfare among stakeholders within an economic equilibrium (Berman and Katona 2013). Within this novel search paradigm, the visibility of a website hinges on its citation by the chatbot. As chat-based search engines evolve and enhance their ability to modulate users’ exposure to diverse websites5, the economic ramifications can escalate correspondingly. Historically, conventional search engines also intentionally introduce opacity to their ranking algorithms, serving as a deterrent against manipulation by website proprietors. This obscurity, juxtaposed with the economically pivotal role of visibility through search engines, has given rise to the domain of Search Engine Optimization (SEO). SEO endeavors to decode the myriad factors that influence website standings within search engine results, aiming to tailor content to enhance its visibility (Reisenbichler et al. 2022, Danaher et al. 2006). Nonetheless, these insights pertaining to the ranking criteria of conventional search engines may not be directly applicable to comprehending the citation criteria of chat-based search engines, given their distinct attributes. To begin with, leading LLMs, such as those within the GPT series, are constructed using a human-in-theloop methodology, wherein human evaluators determine the optimal responses to specific queries 5 See https://www.demandsage.com/google-gemini-statistics/. linguistic styles (Guo et al. 2023), potentially influencing their source preferences during information retrieval tasks. Furthermore, when referencing websites, LLMs must choose suitable textual content. However, LLMs demonstrate varied levels of understanding for different text types, typically evaluated through perplexity metrics (Gonen et al. 2022). This disparity in comprehension may also shape their preferences in the source selection process. Given such inherent uncertainties, we posit our first research question. RQ1: How do chat-based search engines select websites to cite in response to a query? In order to address our first research question, we procure an extensive dataset by executing queries on New Bing and harvesting responses via Bing Chat (the chat-based interface), alongside the websites referenced therein and those listed by Bing (its conventional counterpart). Following a sequence of preprocessing and text mining procedures, we conduct a comparative analysis of the linguistic characteristics of websites cited within Bing Chat’s responses against those that are not cited. Our findings indicate that the chat-based search engine exhibits a preference for source content that is more readable and analytical, while demonstrating lower levels of polarity and a reduced conversational tone. Furthermore, our analysis reveals a noteworthy trend: content deemed more “readable” by GPT, as quantified by perplexity scores, is more likely to be cited in support of the responses. This suggests the possibility that the criteria for citation are influenced by the characteristics of the underlying LLM, rather than being exclusively determined by the New Bing engineers. Nonetheless, it is conceivable that these criteria may be derived from the conventional search engine engine, especially considering that Bing Chat initiates its process by first identifying relevant websites prior to learning and generating responses6. Prior investigations into conventional search engines have unveiled distinct linguistic predilections within their ranking algorithms (Reisenbichler et al. 2022). Should Bing Chat predominantly extract information from websites that are highly
6 See https://www.microsoft.com/en-us/bing/do-more-with-ai/how-bing-chat-works. perpetuated, thus confounding the distinct preferences attributable to the foundational LLM. To discount this alternative explanation and gain a comprehensive understanding of how chat-based search engines diverge from traditional ranking algorithms, we propose our second research question. RQ2: How do the selection criteria employed by chat-based search engines diverge from the ranking
criteria utilized by conventional search engines? Utilizing the same dataset, we investigate the ranking criteria employing an identical set of variables. Our findings reveal that, while there are similarities between the ranking criteria of Bing Chat’s citation process (such as readability), significant differences also exist in terms of other linguistic styles (e.g., certitude) and perplexity. These outcomes again underscore the distinct nature of chat-based search engines. While our perplexity analysis hints at the influence of the foundational LLM on specific criteria, it remains uncertain whether these criteria are the product of deliberate design choices by Bing Chat’s designers, akin to the dynamic anti-gaming measures employed in traditional ranking algorithms. However, compared with these traditional counterparts, the LLM used for chat-based search engines poses a greater challenge for manual design. Existing literature underscores that LLMs can manifest unforeseen behaviors once the scale of their parameters surpasses certain thresholds, with the underpinnings of these emergent phenomena eluding even the models’ creators (Wei et al. 2022a, Kosinski 2023). In light of these considerations, we advance our third research question. RQ3: Do chat-based search engine’s criteria originate through manual curation or naturally emerge
from underlying language models? To address this research inquiry, we delve into the information retrieval and content creation processes executed by Bing Chat’s foundational model, GPT-4, with a direct examination. Should the predilection for sourcing citation texts be attributed to the intrinsic model, analogous trends are expected to manifest in scenarios utilizing GPT-4 for Retrieval Augmented Generation (RAG). In pursuit of this objective, we gather an additional dataset through the use of OpenAI’s knowledge
ing our examination of the citation criteria within this RAG context, we uncover that the criteria employed by the GPT-4 based RAG are qualitatively aligned with those observed in Bing Chat. This consistency further suggests that the citation preferences of Bing Chat are derived from the LLM’s inherent capabilities, rather than being artificially engineered by the search engine’s developers. Furthermore, the texts cited by RAG API demonstrate greater similarities when compared with the top-ranked websites presented by conventional search engines. The disparity provides another intriguing yet justifiable distinctiveness of the LLM-based information retrieval, given its responsibility for furnishing a singular coherent response, thereby requiring an elevated level of information congruity. In the subsequent sections of this paper, we integrate relevant literature and highlight our theoretical contributions in Section 2. Section 3 describes our methodology for data acquisition and the construction of variables. Section 4 presents our data analysis, addressing each research question sequentially. Finally, Section 5 concludes with a discussion on practical implications and suggests directions for future research. 2. related literature Our research builds on and contributes to three streams of literature: i) AI Performance, AI Metacognition, and AI Alignment; ii)search engine ranking, optimization, and marketing; iii) information retrieval with generative AI. 2.1. ai performance, ai metacognition, and ai alignment Broadly speaking, our paper is related to the research on AI performance, AI metacognition, and AI alignment, especially for LLM-based generative AI and related applications. Given the intricate architecture of LLM-based generative AI, the output and performance of such models is not entirely predictable by their designers. Particularly when the models scale up to a high number of parameters, they begin to exhibit emergent capabilities, which include unanticipated enhancements in performance on novel tasks, as discussed in the literature (Kosinski 2023, Johnston
devising novel metrics, tasks, and methodologies to comprehensively assess the efficacy of generative AI across various dimensions (Zhao et al. 2023). For instance, Wang et al. (2022) compiles an extensive list of over 1,600 NLP tasks to evaluate the adaptability of universal language models. More interestingly, recent advancements in LLMs have notably highlighted their potential in paving the way toward Artificial General Intelligence (AGI). According to a classification proposed by Google DeepMind, AI systems can be segmented into ten diverse categories, evaluated across five performance levels (emerging, competent, expert, virtuoso, or superhuman) and categorized by their breadth of application (narrow or general) (Morris et al. 2023). Although earlier iterations of AI have evolved from rudimentary rule-based systems to highly advanced entities exemplified by AlphaFold Jumper et al. (2021), they are predominantly narrow in their focus and designed for specific tasks. In stark contrast, current generative AI models based on LLMs, like ChatGPT and Llama 2, demonstrate a broad spectrum of metacognitive capabilities, including the acquisition of new skills and creativity (Morris et al. 2023). Motivated by these developments, scholars are now endeavoring to dissect the inherent cognitive processes of AI, such as personality and rationality (Mei et al. 2024, Chen et al. 2023b, Wang et al. 2023), and to utilize AI’s cognitive functions in simulating human responses (Brand et al. 2023, Horton 2023). As AI systems exhibit increasingly sophisticated cognitive capabilities and the capacity for creativity that surpasses human expectations, the concept of AI alignment has garnered heightened scrutiny (Gabriel 2020). The objective of AI alignment is to ensure that AI systems act in accordance with human intentions and values, promoting robustness, interpretability, controllability, and ethicality in AI (Ji et al. 2023). Throughout the development and application of Large Language Models (LLMs), alignment principles are integrally applied. For instance, during the development phase, Reinforcement Learning from Human Feedback (RLHF) is employed to align AI with human’s goals (Ouyang et al. 2022). Subsequent to model development, AI outputs and their underlying mechanisms undergo evaluation and interpretation (Gilpin et al. 2018). Moreover, Red Teaming
deployment of AI systems, it is imperative from a societal alignment perspective to assess the systems’ impacts on the broader society (Korinek and Balwit 2022). Within this expansive framework, previous research into the economics of AI can be subsumed under AI alignment, such as studies on the effects of Generative AI on labor markets (Liu et al. 2023a, Hui et al. 2023), social communities (Borwankar and Khern-am nuai 2023, Xue et al. 2023), and software development (Peng et al. 2023, Dohmke et al. 2023). Pursuant to this line of inquiry, our paper delves into the creative process underpinning the responses of chat-based search engines, assessing their outputs via an array of textual characteristics. Beyond identifying the distinct textual predilections of chat-based search engines, our study furnishes empirical support for the proposition that these preferences arise naturally from the foundational LLM, rather than being the product of manual curation. Furthermore, our findings elucidate potential mechanisms by which chat-based search engines may wield economic influence, specifically through their unique exposure allocation processes based on textual preferences, coupled with a reduction in information diversity. 2.2. Search Engine Ranking, Optimization, and Marketing In terms of the application context, our inquiry also contributes to the extensive academic discourse on search engine rankings, optimization, and marketing. This body of work is multifaceted, encompassing a diverse array of subtopics. Primarily, certain studies concentrate on the commercial values of search engine rankings and advertisements. These investigations endeavor to decode consumer reactions to search advertisements, examining phenomena such as rank-order effects, and to evaluate the efficacy of such advertisements (Ghose et al. 2014, Goldfarb and Tucker 2011, Ghose and Yang 2009). In addition, there is scholarly effort dedicated to elucidating the dynamics between various modes of information acquisition for consumers, such as organic versus sponsored searches (Yang and Ghose 2010), generic versus branded searches (Rutz and Bucklin 2011), and the utilization of search engines in conjunction with social media platforms (Ghose et al. 2019). the perspective of website proprietors (advertisers). It has been proposed that entities positioned at the pinnacle of organic search results may experience a reduction in their motivation to compete for sponsored clicks (Katona and Sarvary 2010). Furthermore, modeling such competition among websites also necessitates the consideration of the actions of consumers and search engines, such as the search engine’s incentive to prioritize low-quality websites to maximize its advertising revenue (Athey and Ellison 2011, Xu et al. 2011, 2012). Berman and Katona (2013) presents a framework that examines the impact of Search Engine Optimization (SEO) on the competitive landscape among advertisers and the resulting welfare for visitors, advertisers, and the search engine. Finally, given the significance of search engine ranking, scholars also seek to employ techniques to comprehend and enhance website exposure. For organic search, previous research tends to investigate factors that can influence website ranking and subsequently optimize website content accordingly (Reisenbichler et al. 2022, Erdmann et al. 2022, Ziakis et al. 2019, Su et al. 2014, Danaher et al. 2006). Regarding sponsored search, scholars concentrate on optimizing websites’ keyword auction strategies to maximize revenue while adhering to specific constraints in a competitive environment (Liu and Toubia 2018, Du et al. 2017, Shin 2015, Abhishek and Hosanagar 2013, Liu et al. 2010). Our investigation principally situates itself within the last stream of literature, as it too aims to elucidate the determinants influencing the content selection mechanism of search engines. Specifically, our study concentrates on the nascent domain of chat-based search engines, analyzing the criteria they employ to select websites for citation and how these criteria diverge from those of conventional search engines’ ranking algorithms. This differentiation could engender a novel competitive environment owing to the unique traffic channel these chat-based search engines direct toward websites. Such an evolution also necessitates consideration in future SEO and platform strategies for both researchers and industry experts. 2.3. Information Retrieval with Generative AI More specifically, our research further intersects with the rapidly expanding domain concerning information retrieval facilitated by generative AI. Despite the longstanding history of information
generative AI and its associated applications are revolutionizing information retrieval processes. Consequently, scholars are endeavoring to design and evaluate the information retrieval techniques in this new context. On the design side, the advent of LLM-based generative AI heralds a significant shift—or potentially a paradigmatic transformation—in the domain of information retrieval. These generative AI systems, trained on extensive datasets, are capable of delivering substantiated responses to queries, thereby facilitating direct access to information encapsulated within the original training corpus (Bénédict et al. 2023). Moreover, to embed domain-specific insights and contemporary information within these responses, designers may employ several strategies: (1) fine-tuning the LLMs with updated datasets (Hu et al. 2023); (2) leveraging retrieval-augmented generation that combines the original model with new data (Hofstätter et al. 2023); or (3) utilizing straightforward prompt engineering techniques with the incorporation of new data (Liu et al. 2023b). Significantly, LLM-based generative AI can be instrumental across various stages of information retrieval, including query reformulation, content recall, content re-ranking, and the synthesis of content for the final output (Zhu et al. 2023). This multifaceted utility paves the way for the enhancement or creation of novel applications powered by generative AI, such as LLM-based recommendation engines and search agents (Hua et al. 2023, Nakano et al. 2021). Regarding the evaluation aspect, the intricacies inherent in both the foundational model and the unstructured information sources render the assessment of information retrieval techniques involving generative AI a formidable challenge. To address this, computer scientists devise an array of baseline tasks spanning multiple domains to gauge their efficiency (Chen et al. 2023a). Additionally, human evaluations are conducted to appraise the systems’ outputs (Lewis et al. 2020). In certain instances, behavioral researchers engage human participants to undertake information-seeking tasks employing various information retrieval techniques, facilitating a comparative analysis of performance with human involvement (Xu et al. 2023). LLM-based information retrieval systems. Employing a variety of NLP techniques and metrics, this study elucidates the selection criteria employed by Bing Chat for citing websites and delineates the divergence of these criteria from those utilized by traditional search engines’ ranking algorithms. More crucially, our research reveals that the observed disparities stem from the foundational large language models, rather than being artifacts manually introduced by the designers of Bing Chat. Specifically, we observe that: (1) Bing Chat exhibits a preference for content that is more readily interpretable (i.e., perplexity) by models within the GPT family; (2) the application of retrievalaugmented generation with GPT-4 demonstrates qualitative consistency in selection criteria akin to that of Bing Chat. Consequently, our research also advocates for future scholarly endeavors to adopt a more integrative approach towards the evaluation of LLM-based applications and the underlying large language models. 3. data and variables  3.1. data sources, data collection, and preprocessing procedures 3.1.1. Data Source 1: New Bing We source the first raw dataset from New Bing, an AI-driven search engine developed by Microsoft, officially released on February 7, 2023. In contrast to traditional methods of listing pertinent web content, hereafter denoted as “conventional search engine,” New Bing offers users an additional chatbot known as “Bing Chat” which can provide consolidated responses by aggregating information from different websites7. Figure 1 offers a graphical depiction of the search output produced by the New Bing platform. To the left, conventional search outcomes are presented, each typically encompassing three elements: the website’s title, its corresponding hyperlinks, and an excerpt from the site identified as most pertinent to the search query by the search algorithm. In juxtaposition, on the right-hand side, Bing Chat provides a composite response, integrating its pre-established knowledge database with 7 See https://www.microsoft.com/en-us/edge/features/the-new-bing. dialogue box’s base. Among these hyperlinks, some are explicitly referenced within the composite answer, whereas others are simply enumerated without direct citation. It is noteworthy that a single sentence may reference none to multiple websites, while a singular website may also be cited in support of none to multiple sentences. To commence our analysis, we first extract and randomize all queries from the Human ChatGPT Comparison Corpus (HC3), a dataset introduced by Guo et al. (2023), which is extensively utilized in subsequent Generative AI research for benchmarking and comparative studies (Zhao et al. 2023). Each query is individually inputted into the search engine, acting as the keyword for information retrieval. A subset of queries fails to generate search results, attributable to the inclusion of sensitive or prohibited content, such as hate speech or violent material. Additionally, instances where the Bing Chat fails to produce responses due to the aforementioned reasons, or presents responses devoid of citations to websites, albeit infrequent, are also classified as unsuccessful queries. Constrained by the
yield results. We then collect the conventional search outcomes displayed on the initial two pages of
search results, encompassing titles, excerpts, and hyperlinks of the websites listed. Afterwards, for
each Bing Chat response, we gather the textual content, the hyperlinks cited within these responses,
and any specific locational markers referencing these hyperlinks. Furthermore, we proceed to capture
and archive the complete textual content of all websites listed by the conventional search engine or
cited by Bing Chat, culminating in a repository of 13,428 distinct websites. To offer a foundational
understanding of the dataset’s hierarchical structure, we present summary statistics delineated by
various levels of granularity within the raw dataset in Table 1. For every query submitted, we show three variables that capture the scope of our data
collection: NumTotalWebs, denoting the aggregate number of websites gathered per query;
NumCitedSentences, signifying the count of sentences within each query’s response that reference
at least one website; and NumCitedWebs, specifying the number of websites cited by at least one
sentence in Bing Chat’s responses. The value of NumCitedWebs naturally varies across different
queries, reflecting Bing Chat’s selection criteria to substantiate its responses. It is pertinent to
highlight that the count of websites collected from the conventional search engine may also exhibit
variability, attributed to the exclusion of duplicate websites that are displayed in the first two pages
of conventional search outcomes. Delving further, among the 700 responses generated by Bing Chat, a total of 2,586 sentences
are identified that cite at least one website for reference. This observation is encapsulated in the
variable NumCitedWebsSent, representing the count of websites cited at the sentence level. Corre-
spondingly, for each website, we ascertain the frequency with which it is referenced across different
sentences, thereby defining the variable NumCitedSentencesWeb to quantify this metric. Statistic N Mean St. Dev. Min Median Max Query-level Statistics NumTotalWebs 700 19.18 1.18 12 20 20 NumCitedSentences 700 3.69 1.73 1 3 14 NumCitedWebs 700 3.37 1.31 1 3 10 Sentence-level Statistics NumCitedWebsSent 2,586 1.38 0.67 1 1 5 Website-level Statistics NumCitedSentencesWeb 13,428 0.27 0.68 0 0 8
However, the dataset cannot be immediately employed for the investigation of Bing Chat’s criteria, given that each website may comprise tens of thousands of words, with only a minor fraction being referenced to substantiate sentences in Bing Chat’s responses. To find the most relevant texts, as delineated in the pseudocode Algorithm 1 and Figure 2, we undertake a series of preprocessing steps to compile dataset 1A: For every query, we pinpoint sentences within Bing Chat’s response that cite at least one website. Subsequently, for each website sourced from this query, we identify the chunk exhibiting the highest similarity to each sentence with references and archive this chunk. This procedure involves dividing the websites into uniform chunks (128 characters each), employing the Universal Sentence Encoder to generate embeddings for both the sentences and all chunks from each website (Cer et al. 2018), and utilizing cosine distance to gauge their similarities. This methodology yields a query-sentence-website level dataset, where each entry corresponds to a website chunk most
2 sentences that have citations, this results in 40 (20×2) distinct entries within dataset 1A. Using this method, we get 49,917 observations in total. Algorithm 1 Chunk Selection Process for Dataset 1A: Pseudocode Require: Bing Chat’s Responses, Website Content
Initialize Dataset 1A. for each query i: do
Retrieve all the sentences that cite at least one website Ji. Retrieve all the related websites Ki. for each sentence j ∈ Ji: do
for each website k ∈Ki: do
Embed sentence j using Universal Sentence Encoder. Segment website k into Mk chunks of equal length. Embed all the chunks m∈Mk. Find the chunk m̂= argmaxm∈Mk cosine_similarity(m ′’s embedding, j’s embedding). Add i, j, k, and m̂’s content to Dataset 1A. end for
end for
end for
Regarding our second research question, which seeks to elucidate the ranking criteria and compare them with Bing Chat’s criteria, we assemble Dataset 1B from the websites cataloged by the conventional search engine. Given that the search engine highlights one excerpt for each listed website it deems most pertinent to the search query, we employ this excerpt as the basis for our ranking analysis. To extract the original content of these excerpts, we identify the chunk that includes the excerpt for each website listed by the conventional search engine, thereby creating Dataset 1B following the
the 11,741 websites. This quantity is less than the total number of websites gathered (i.e., 13,428), attributable to the fact that some websites are exclusively cited by Bing Chat and do not appear in the conventional search engine listings, thus precluding our ability to ascertain their ranking. Algorithm 2 Chunk Selection Process for Dataset 1B: Pseudocode Require: Excerpts, Website Content
Initialize Dataset 1B. for each query i: do
Retrieve all the related websites Ki. Retrieve all the corresponding excerpts Ek where k ∈CKi, where CKi ⊆Ki contains websites
that are listed in the conventional search engine results of query i.
for each website k ∈Ki: do
Segment website k into Mk chunks of equal length. Find the chunk m̂ that has the longest common subsequence with the excerpt. Add i, k, and m̂’s content to Dataset 2.
end for
end for
3.1.2. Data Source 2: GPT4-based RAG To address our third research question, which endeavors to elucidate the origins of Bing Chat’s preference for specific textual content – questioning whether it is attributable to the inherent capabilities of the LLM or the result of deliberate modifications by the search engine’s developers – we propose a direct engagement with the foundational model, specifically GPT-4. OpenAI’s Knowledge Retrieval API presents a viable means to accomplish this, offering the ability to systematically segment and access pertinent information from files provided by users, and to formulate responses that consider both the newly uploaded content and the pre-existing data repository8. By circumventing 8 See https://platform.openai.com/docs/assistants/tools/knowledge-retrieval. engine tasks, this API may more authentically reveal the underlying LLM’s linguistic predispositions. To achieve our research objectives, it is imperative to construct an additional dataset, labeled as Dataset 2, building upon the queries and websites acquired from the first data source. For each query, we aim to upload the content of all the relevant websites and ascertain which of these are referenced by the aforementioned RAG API in formulating its responses. This approach, however, introduces a challenge analogous to the one encountered during the assembly of datasets 1A and 1B, namely, the prohibitive length and resultant computational cost of processing all the websites entirely. Consequently, a methodology to distill a more concise segment from each website is necessitated. Adhering to Algorithm 3, we divide each of the 13,428 websites into segments of uniform length (128 tokens in this instance). Subsequently, for each website indexed by the conventional search engines, we leverage the search result excerpt to identify and extract the chunk encompassing this excerpt as the representative chunk for the website9. Conversely, if the website is exclusively cited by Bing Chat—thus lacking a corresponding excerpt—we utilize the sentence within Bing Chat’s response that references the website as a focal sentence. The chunk bearing the highest similarity to this focal sentence, as quantified by cosine similarity between their embeddings, is then selected as the representative chunk for the website. Employing this methodology across the 13,428 websites, we successfully compile an equivalent number of corresponding chunks. Subsequently, for each query, a compilation of websites is gathered alongside the query itself, with an equivalent number of chunks (one per website) extracted from these sites. These chunks are then amalgamated into a singular text document for each query, which functions as the retrieval source document. Upon submitting both the query content and the corresponding source text document to the OpenAI Knowledge Retrieval API, a synthesized response, inclusive of citations, is generated10. Each citation delineates the original sentence along with its location within the submitted 9 If a website is both listed by the conventional search engine and cited by Bing Chat, we also use this method. 10 A detailed illustration of the prompt, query, and resultant outputs is provided in Appendix A.
quently, Dataset 2 comprises 13,428 entries, each accompanied with an outcome variable indicating
its citation status by the RAG algorithm. Algorithm 3 Chunk Selection Process for Dataset 2: Pseudocode Require: Bing Chat’s Responses, Excerpts, Website Content
Initialize Dataset 2. for each query i: do
Retrieve all the sentences that cite at least one website Ji. Retrieve all the related websites Ki. Retrieve all the corresponding excerpts Ek where k ∈CKi, where CKi ⊆Ki contains websites
that are listed in the conventional search engine results of query i.
for each website k ∈Ki: do
Segment website k into Mk chunks of equal length. if website k ∈CKi then
Find the chunk m̂ that has the longest common subsequence with the excerpt. else
Find all the sentences from Ji that cite website k. Merge these sentences into a new sentence jall Embed the sentence jall using Universal Sentence Encoder. Embed all the chunks m∈Mk. Find the chunk m̂= argmaxm∈Mk cosine_similarity(m ′’s embedding, jall’s embedding). end if Add i, k, and m̂’s content to Dataset 2.
end for
end for
In the process of assembling the aforementioned three datasets (Datasets 1A, 1B, and 2) sourced from two distinct data repositories (New Bing and OpenAI’s Knowledge Retrieval API), we systematically acquire the textual content alongside the corresponding outcome variables. This section elaborates on the variables derived from these datasets and presents summary statistics for each dataset. We initiate by elucidating the outcome variables associated with each dataset. In Dataset 1A, for each sentence with references, we define the set of relevant chunks from all websites as the citation consideration set. Consequently, we obtain 49,917 observations, calculated as the product of the number of sentences with references and the number of websites retrieved for each query, summed across all queries, denoted as ∑
i Number of Sentencesi × Number of Websitesi. In this
dataset, the variable Cited signifies whether a particular sentence cites the corresponding chunk, which, by extension, is equivalent to citing the website, given that a singular chunk from each website is considered. For Dataset 1B, we document 11,741 observations, where each encompasses the content of a website and its position as determined by a conventional search engine, encapsulated by the variable Rank. The content refers to the chunk within the website that contains the excerpt highlighted by the conventional search engine. In Dataset 2, consisting of 13,428 observations, each observation includes the content of a website and a variable Cited denoting whether it is cited by the RAG API. Here, the content corresponds to a chunk which either contains the excerpt (if listed by the conventional search engine) or bears resemblance to sentences within Bing Chat’s responses (if not listed). In addressing the research questions pertinent to the citation and ranking behaviors of different systems, we also extract various features from website content (i.e., chunks) to serve as independent variables. Since our research is anchored in the domain of search engine ranking literature, prompting the inclusion of the Readability metric, a widely recognized measure within this field (Reisenbichler et al. 2022). Readability evaluates the ease with which text can be understood by human readers. To quantify readability, we utilize the New Dale-Chall Readability Formula (Chall and Dale 1995),
a negative sign to the formula’s output, whereby a higher score implies enhanced readability. Additionally, given the relevance of our research to assessing the outputs of generative AI, which aims to replicate human-like text, we examine several linguistic features commonly employed in such evaluations. The Analytic variable quantifies the extent of analytical thought in the text, with higher values denoting greater logical consistency. The Subjectivity measures the propensity to favor objective over subjective content, with higher scores reflecting greater subjectivity. These variables primarily concern the informational content’s logicality and objectivity. Moreover, we incorporate measures of expressive styles, such as Polarity, which gauges the emotional polarity (absolute value) of the text, with higher values indicating increased emotional polarization. The Certitude variable assesses the text’s level of assertiveness, for instance, texts employing phrases like "of course" may be deemed to exhibit higher certitude. The Conversational measure evaluates the text’s colloquialism, including the use of internet slang (e.g., “lmao", “lol") or discourse markers (e.g., “um", “oh"). The calculation of Subjectivity and Polarity is facilitated through TextBlob (Loria et al. 2018), while the remaining metrics are derived using the Linguistic Inquiry and Word Count (LIWC) tool (Boyd et al. 2022). Finally, based on the research focus on LLM-based generative AI, we are particularly intrigued by the possibility that certain criteria might naturally emerge from these models. Of specific interest is the influence of “readability" from the perspective of language models on the content selection process. To explore this, we introduce the concept of Perplexity, which is formally defined as the exponentiated average negative log-likelihood of a sequence as predicted by a language model (Azzopardi et al. 2003). Intuitively, it can measure the extent to which a model anticipates the input content, wherein a sentence that aligns closely with the model’s expectations exhibits lower perplexity, and conversely, a more unexpected sentence exhibits higher perplexity. Following the above steps, for every observation within the datasets, we derive a single outcome variable and seven independent variables, the summary statistics of which are presented
investigating research questions 1, 2, and 3, respectively. In this section, we systematically address each of our research questions. First, in Section 4.1, we scrutinize the criteria by which the chat-based search engine, specifically Bing Chat in our study, selects websites for inclusion in its responses. Following this, Section 4.2 delineates the ranking criteria employed by conventional search engines and contrasts them with those utilized by Bing Chat. Finally, Section 4.3 elucidates the source of citation criteria by utilizing the RAG model based on GPT-4 and juxtaposing them with those employed by Bing Chat. 4.1. chat-based search engine’s content selection criteria We initiate our exploration with the first research question, which investigates the criteria Bing Chat utilizes to select websites for substantiating its responses, employing Dataset 1A as elaborated in Section 3. For each query i, Bing Chat generates a response comprising at least one sentence that incorporates references. The crux of our analysis lies in discerning the textual predilections that predispose the selection of certain websites over others to underpin these referenced sentences. Given the voluminous content available on each website, we delineate a single relevant chunk from each website k to constitute the consideration set for a given sentence j. Consequently, Dataset 1A forms a query-sentence-website level dataset, encompassing 49,917 entries. The variable of interest, Citedijk, indicates whether a chunk from website k has been referenced to support sentence j in Bing Chat’s response to query i. To decipher the relationship between the textual features of a website’s chunk and its likelihood of citation, we incorporate a suite of independent variables, as delineated on the right side of Equation 1. To strengthen the robustness of our results, we employ three distinct regression models: Ordinary Least Squares (OLS) regression (i.e., linear probability model), logistic regression, and probit regression. Citedijk =β0 +β1 ∗Readabilityijk +β2 ∗Analyticijk +β3 ∗Certitudeijk+
β4 ∗Subjectivityijk +β5 ∗Polarityijk +β6 ∗Conversationijk +β7 ∗Perplexityijk + ϵijk
(1)
Dependent variable:
Cited
OLS Logistic Probit
(1) (2) (3)
Readability 0.0009∗∗∗ 0.0110∗∗∗ 0.0057∗∗∗ (0.0001) (0.0016) (0.0008) Analytic 0.0004∗∗∗ 0.0073∗∗∗ 0.0034∗∗∗ (0.0001) (0.0010) (0.0005) Certitude −0.0014 −0.0241 −0.0105 (0.0015) (0.0260) (0.0124) Subjectivity 0.0124 0.2084 0.0927 (0.0109) (0.1618) (0.0790) Polarity −0.0240∗ −0.3874∗ −0.1689∗ (0.0133) (0.2118) (0.1022) Conversation −0.0032∗∗ −0.0637∗∗ −0.0311∗∗ (0.0014) (0.0324) (0.0154) Perplexity −0.0302∗∗∗ −0.4185∗∗∗ −0.2082∗∗∗ (0.0036) (0.0477) (0.0240) Constant 0.2030∗∗∗ −0.9414∗∗∗ −0.6306∗∗∗ (0.0184) (0.2560) (0.1254)
Observations 49,917 49,917 49,917
Note: Robust standard errors in parentheses; *** p < 0.01, ** p < 0.05, * p < 0.1
The regression outcomes, as presented in Table 5, manifest consistency across the three employed models, underscoring robust findings. First, congruent with prior literature on search engines (Reisenbichler et al. 2022), chat-based search engines also exhibit a preference for content that is more readily comprehensible (β1 > 0, p < 0.01). Additionally, Bing Chat demonstrate a proclivity for content characterized by higher degrees of logical reasoning and analytical thought (β2 > 0, p < 0.01). Beyond these preferences more related to substantive information, Bing Chat also displays discernible inclination regarding stylistic attributes of content. Specifically, there is a tendency to reference websites manifesting reduced emotional polarity and a lower degree of conversationality (β5 < 0, β6 < 0), signaling a preference for more logical and formal expressions. nificantly lower perplexity scores (β7 < 0, p < 0.01). Given that perplexity serves as a measure of text predictiveness from the perspective of the language model, this trend suggests an inclination towards sourcing content that resonates more closely with the foundational language model’s training corpus. This also implies that the textual preferences exhibited by chat-based search engines may not be manually curated but rather emerge spontaneously from the underlying large language model, which is trained on extensive datasets. 4.2. comparison with conventional search engines In the previous subsection, we delineate the selection criteria employed by Bing Chat for citing content in support of its responses. The objective of this subsection is to elucidate the ranking criteria utilized by conventional search engines, employing Dataset 1B for this purpose, and to undertake a comparative analysis with the criteria observed in Bing Chat. From a total collection of 13,428 websites, 11,741 are indexed by the conventional search engine and thus included in our analysis. This dataset also encompasses the websites that are both cited by Bing Chat and listed by the conventional search engine. For each website k listed by the conventional search engine in response to query i, we identify the chunk containing the excerpt highlighted by the engine for our ranking analysis. The dependent variable, Rankik, signifies the rank position of website k within the conventional search engine results for query i. Notably, the ranking system commences at one, with lower values indicating superior rank positions. Following the same logic, we extract an identical set of independent variables for each website’s chunk, as incorporated in the regression model delineated in Equation 2. The ordered logit and ordered probit regression models are employed to ascertain the impact of these independent variables on the websites’ ranking outcomes. Rankik =Thresholds+β1 ∗Readabilityik +β2 ∗Analyticik +β3 ∗Certitudeik+
β4 ∗Subjectivityik +β5 ∗Polarityik +β6 ∗Conversationik +β7 ∗Perplexityik + ϵik (2)
Dependent variable:
Rank
Ordered Logit Ordered Probit
(1) (2)
Readability −0.0035∗∗ −0.0018∗∗ (0.0018) (0.0010) Analytic 0.0004 0.0001 (0.0008) (0.0005) Certitude −0.0412∗∗ −0.0247∗∗ (0.0208) (0.0124) Subjectivity 0.1841 0.1053 (0.1504) (0.0869) Polarity 0.1886 0.1619 (0.1823) (0.1062) Conversation 0.0690∗∗ 0.0409∗∗ (0.0280) (0.0170) Perplexity −0.0512 −0.0240 (0.0457) (0.0266)
Observations 11,741 11,741
Note: Robust standard errors in parentheses; *** p < 0.01, ** p < 0.05, * p < 0.1
Given that a higher rank value corresponds to a lower priority within the ranking system, it necessitates an inverse interpretation of the coefficients. As evidenced in Table 6, there are some observed similarities in preference between the conventional and chat-based search engines. Specifically, conventional search engines also favor websites characterized by greater readability (β1 < 0, p < 0.01) and those exhibiting a less conversational tone (β6 > 0, p < 0.05). However, contrary to Bing Chat, there is no discernible preference for content imbued with analytical elements or content with diminished emotional polarity. Moreover, the conventional search engine demonstrates a preference for content with a higher degree of certitude (β3 < 0, p < 0.05), a tendency not mirrored by the chat-based counterpart. Notably, the analysis reveals no significant correlation between perplexity and ranking, further highlighting the distinctiveness of LLM-based, chat-based search engines. the imperative to revisit and reevaluate search engine research, particularly as this novel form of
information retrieval continues to gain prominence. 4.3. criteria foundation exploration via retrieval-augmented generation In the preceding two subsections, we have identified distinctive selection criteria employed by the
chat-based search engine, which notably diverges from the criteria of conventional search engine
rankings. Intriguingly, Bing Chat’s preference for content exhibiting lower perplexity intimates that
such uniqueness may originate from the foundational large language model (specifically, GPT-4 in
this study) rather than being deliberately programmed by New Bing’s designers. Should this be the
case, similar text selection criteria are anticipated to be observable during direct interactions with
GPT-4 as well. To further investigate this hypothesis and address our third research question, we compile Dataset
2 by utilizing OpenAI’s knowledge retrieval API. This process entails the submission of queries
alongside documents containing segments of content from various websites (the methodology for
which is detailed in Section 3.1.2). The API’s response, which includes references to specific segments
of the document, enables us to ascertain whether the content from website k is cited in response
to query i. The outcome variable Citeik is thus defined. Analogous to the approach outlined in the
previous subsections, we employ the same spectrum of textual features extracted from the website
chunks as independent variables. The analysis also encompasses three regression models (Linear
probability, Logistic, and Probit regressions) with the specification summarized in Equation 3. Citedik =β0 +β1 ∗Readabilityik +β2 ∗Analyticik +β3 ∗Certitudeik+
β4 ∗Subjectivityik +β5 ∗Polarityik +β6 ∗Conversationik +β7 ∗Perplexityik + ϵik (3)
as isolated entities. Nonetheless, the similarity of content across websites constitutes a significant
dimension for evaluating search engine outputs. As underscored in previous studies (Reisenbichler
et al. 2022), content similarity can offer insights into the diversity of information made available to
users and elucidate the exposure relationship among various websites. This metric assumes greater
relevance in the context of chat-based search engines, which are tasked with providing a singular
answer that references multiple supporting websites, thereby creating an indirect linkage among
these sites through the response. Hence, in addition to website-level textual disparities, chat-based
search engines may also exhibit unique “market-level” content similarities. Employing Dataset 2, our investigation extends to the analysis of content similarity among web-
sites cited through the RAG process in contrast to those ranked highly by traditional search engines. For each query i involving citations from Ni websites, we calculate the mean cosine similarity among the embeddings of these cited sites. In a parallel fashion, for the top Ni websites as per conventional search rankings, their average cosine similarities are computed. Out of 700 queries, 607 that feature
at least two citations during the RAG process are selected for this similarity analysis, resulting in
two sets of 607 similarity metrics—one derived from RAG-cited websites and the other from websites
top-ranked by conventional search engines. A t-test between these datasets unveils a notable finding:
websites cited by RAG display a higher degree of similarity compared to those in the top-ranked
group by traditional search engines (t− stats= 5.32, p < 0.01), with the former’s mean similarity
at 0.570 (std= 0.117) and the latter’s at 0.533 (std= 0.128). Thus, in addition to distinctive criteria based on textual features at the individual website level,
applications through RAG also manifest a reduction in information diversity among cited websites
relative to an equivalent number of top-ranked websites. These findings collectively underscore
the unique operational mechanisms and outputs of chat-based search engines, underscoring the
imperative for revised industry practices and further research inputs. With the advent of LLMs, a novel chat-based search engine paradigm has been introduced, leveraging LLMs to assimilate information from web sources and articulate responses in natural language, thereby exhibiting human-like cognitive faculties such as information assimilation and the generation of novel content. In pursuit of delineating the criteria for information selection and the foundational principles thereof, this study compiled two datasets through engagements with New Bing and the Knowledge Retrieval API powered by GPT-4. Our findings elucidate Bing Chat’s inclination for content that is not only readable, formal, and logical but also exhibits lower perplexity, indicating a preference for text congruent with the anticipations of the underlying LLM. This preference is not adequately elucidated by traditional search engine ranking algorithms, underscoring the distinctive nature of chat-based search engine outputs. Additionally, through the employment of an ancillary dataset acquired via the RAG API, our research substantiates a consistent textual preference that aligns between the RAG API and Bing Chat, suggesting that these preferences are inherent to the language models rather than being explicitly configured by Bing Chat developers. Beyond the aforementioned website-level distinctions, our investigation discerns a greater similarity among websites referenced by RAG technologies vis-à-vis those prioritized by conventional search engines, signaling a broader market-level uniqueness. Our research intersects with and contributes to multiple domains within the scholarly literature. Predominantly, our study falls into the AI alignment framework, which emphasizes the interpretation of AI systems’ behavior. We enhance this body of work by assessing the distinctive outputs generated by a LLM-based AI system and delineating the mechanisms driving these outputs. More specifically, our investigation not only reveals how chat-based search engines reference websites based on textual characteristics, but also furnishs evidence that such selection criteria are influenced by the foundational LLM. This insight extends the literature on evaluating information retrieval systems powered by generative AI. Additionally, we underscore the importance of considering both LLMbased applications and the underlying LLMs in tandem to gain a comprehensive understanding of
the criteria of a novel form of search engine, marking a preliminary exploration into SEO practices for chat-based search engine. We also demonstrate that the chat-based search engine’s imperative to deliver coherent responses, as opposed to simply listing websites, reduces the information diversity among highly visible websites. This observation may inform future economic analyses within the search engine ecosystem. Our study is not merely of scientific interest but also offers substantive insights for a diverse array of stakeholders. For developers of LLM-based RAG systems, including chat-based search engines, direct interaction with the underlying model is advised. This approach allows for a nuanced understanding of behaviors potentially attributed to the foundational LLM, enhancing system testing and refinement. Similarly, this holds significance for SEO professionals and website proprietors aiming to amplify their visibility within chat-based search engine results. By engaging directly with the LLM, they can fine-tune and evaluate their content prior to website updates. Furthermore, given the chat-based search engine’s response format, it could be beneficial to optimize for compatibility with frequently cited websites. Users, on the other hand, should be cognizant of the limited information diversity inherent to chat-based search engine’s responses, necessitating a balance between search efficiency and the breadth of information. Furthermore, the coexistence of chat-based and traditional search engines necessitates adaptive behavior from both website owners and users, given the distinct criteria of each search modality. This dynamic introduces additional market complexities, urging market administrators and regulatory bodies to re-evaluate market designs, such as auction mechanisms, to facilitate information dissemination and balance stakeholder welfare effectively. Abhishek V, Hosanagar K (2013) Optimal bidding in multi-item multislot sponsored search auctions. Oper-
ations Research 61(4):855–873. Athey S, Ellison G (2011) Position auctions with consumer search. The Quarterly Journal of Economics
126(3):1213–1270. Azzopardi L, Girolami M, Van Risjbergen K (2003) Investigating the relationship between language model
perplexity and ir precision-recall measures. Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, 369–370. Bénédict G, Zhang R, Metzler D (2023) Gen-ir@ sigir 2023: The first workshop on generative information
retrieval. Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, 3460–3463. Berman R, Katona Z (2013) The role of search engine optimization in search marketing. Marketing Science
32(4):644–651. Borwankar S, Khern-am nuai W (2023) Unraveling the impact: An empirical investigation of chatgpt’s
exclusion from stack overflow. Available at SSRN 4481959 . Boyd RL, Ashokkumar A, Seraj S, Pennebaker JW (2022) The development and psychometric properties of
liwc-22. Austin, TX: University of Texas at Austin 1–47. Brand J, Israeli A, Ngwe D (2023) Using gpt for market research. Available at SSRN 4395751 . Cer D, Yang Y, Kong Sy, Hua N, Limtiaco N, John RS, Constant N, Guajardo-Cespedes M, Yuan S, Tar C,
et al. (2018) Universal sentence encoder for english. Proceedings of the 2018 conference on empirical methods in natural language processing: system demonstrations, 169–174. Chall JS, Dale E (1995) Readability revisited: The new Dale-Chall readability formula (Brookline Books). Chen J, Lin H, Han X, Sun L (2023a) Benchmarking large language models in retrieval-augmented generation. arXiv preprint arXiv:2309.01431 . Chen Y, Liu TX, Shan Y, Zhong S (2023b) The emergence of economic rationality of gpt. arXiv preprint
arXiv:2305.12763 . analysis. Journal of Marketing Research 43(2):182–194. Dohmke T, Iansiti M, Richards G (2023) Sea change in software development: Economic and productivity
analysis of the ai-powered developer lifecycle. arXiv preprint arXiv:2306.15033 . Du X, Su M, Zhang X, Zheng X (2017) Bidding for multiple keywords in sponsored search advertising:
Keyword categories and match types. Information Systems Research 28(4):711–722. Erdmann A, Arilla R, Ponzoa JM (2022) Search engine optimization: The long-term strategy of keyword
choice. Journal of Business Research 144:650–662. Gabriel I (2020) Artificial intelligence, values, and alignment. Minds and machines 30(3):411–437. Ganguli D, Lovitt L, Kernion J, Askell A, Bai Y, Kadavath S, Mann B, Perez E, Schiefer N, Ndousse K,
et al. (2022) Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 . Ghose A, Ipeirotis PG, Li B (2014) Examining the impact of ranking on consumer behavior and search
engine revenue. Management Science 60(7):1632–1654. Ghose A, Ipeirotis PG, Li B (2019) Modeling consumer footprints on search engines: An interplay with social
media. Management Science 65(3):1363–1385. Ghose A, Yang S (2009) An empirical analysis of search engine advertising: Sponsored search in electronic
markets. Management science 55(10):1605–1622. Gilpin LH, Bau D, Yuan BZ, Bajwa A, Specter M, Kagal L (2018) Explaining explanations: An overview
of interpretability of machine learning. 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA), 80–89 (IEEE). Goldfarb A, Tucker C (2011) Online display advertising: Targeting and obtrusiveness. Marketing Science
30(3):389–404. Gonen H, Iyer S, Blevins T, Smith NA, Zettlemoyer L (2022) Demystifying prompts in language models via
perplexity estimation. arXiv preprint arXiv:2212.04037 . Gong J, Abhishek V, Li B (2018) Examining the impact of keyword ambiguity on search advertising perfor-
mance. MIS Quarterly 42(3):805–A14. experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597 . Hofstätter S, Chen J, Raman K, Zamani H (2023) Fid-light: Efficient and effective retrieval-augmented text
generation. Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1437–1447. Horton JJ (2023) Large language models as simulated economic agents: What can we learn from homo
silicus? Technical report, National Bureau of Economic Research. Hu Z, Lan Y, Wang L, Xu W, Lim EP, Lee RKW, Bing L, Poria S (2023) Llm-adapters: An adapter family
for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933 . Hua W, Li L, Xu S, Chen L, Zhang Y (2023) Tutorial on large language models for recommendation. Proceedings of the 17th ACM Conference on Recommender Systems, 1281–1283. Hui X, Reshef O, Zhou L (2023) The short-term effects of generative artificial intelligence on employment:
Evidence from an online labor market. Available at SSRN 4527336 . Jansen BJ, Booth DL, Spink A (2008) Determining the informational, navigational, and transactional intent
of web queries. Information Processing & Management 44(3):1251–1266. Ji J, Qiu T, Chen B, Zhang B, Lou H, Wang K, Duan Y, He Z, Zhou J, Zhang Z, et al. (2023) Ai alignment:
A comprehensive survey. arXiv preprint arXiv:2310.19852 . Johnston WJ, Fusi S (2023) Abstract representations emerge naturally in neural networks trained to perform
multiple tasks. Nature Communications 14(1):1040. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Žídek
A, Potapenko A, et al. (2021) Highly accurate protein structure prediction with alphafold. Nature 596(7873):583–589. Katona Z, Sarvary M (2010) The race for sponsored links: Bidding patterns for search advertising. Marketing
Science 29(2):199–215. Korinek A, Balwit A (2022) Aligned with whom? direct and social goals for ai systems. Technical report,
National Bureau of Economic Research. arXiv:2302.02083 . Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, Küttler H, Lewis M, Yih Wt, Rocktäschel
T, et al. (2020) Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33:9459–9474. Liu D, Chen J, Whinston AB (2010) Ex ante information and the design of keyword auctions. Information
Systems Research 21(1):133–153. Liu J, Toubia O (2018) A semantic approach for estimating consumer content preferences from online search
queries. Marketing Science 37(6):930–952. Liu J, Xu X, Li Y, Tan Y (2023a) "generate" the future of work through ai: Empirical evidence from online
labor markets. arXiv preprint arXiv:2308.05201 . Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G (2023b) Pre-train, prompt, and predict: A systematic
survey of prompting methods in natural language processing. ACM Computing Surveys 55(9):1–35. Loria S, et al. (2018) textblob documentation. Release 0.15 2(8):269. McLuhan M (1964) Understanding Media: The Extensions of Man (Routledge). Mei Q, Xie Y, Yuan W, Jackson MO (2024) A turing test of whether ai chatbots are behaviorally similar to
humans. Proceedings of the National Academy of Sciences 121(9):e2313925121. Morris MR, Sohl-dickstein J, Fiedel N, Warkentin T, Dafoe A, Faust A, Farabet C, Legg S (2023) Levels of
agi: Operationalizing progress on the path to agi. arXiv preprint arXiv:2311.02462 . Nakano R, Hilton J, Balaji S, Wu J, Ouyang L, Kim C, Hesse C, Jain S, Kosaraju, et al. (2021) Webgpt:
Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 . Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, Zhang C, Agarwal S, Slama K, Ray A,
et al. (2022) Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35:27730–27744. Peng S, Kalliamvakou E, Cihon P, Demirer M (2023) The impact of ai on developer productivity: Evidence
from github copilot. arXiv preprint arXiv:2302.06590 . natural language generation. Marketing Science 41(3):441–452. Rutz OJ, Bucklin RE (2011) From generic to branded: A model of spillover in paid search advertising. Journal of Marketing Research 48(1):87–102. Schütze H, Manning CD, Raghavan P (2008) Introduction to information retrieval, volume 39 (Cambridge
University Press Cambridge). Shin W (2015) Keyword search advertising and limited budgets. Marketing Science 34(6):882–896. Singhal A, et al. (2001) Modern information retrieval: A brief overview. IEEE Data Eng. Bull. 24(4):35–43. Su AJ, Hu YC, Kuzmanovic A, Koh CK (2014) How to improve your search engine ranking: Myths and
reality. ACM Transactions on the Web (TWEB) 8(2):1–25. Wang W, Pei S, Sun T (2023) Unraveling generative ai from a human intelligence perspective: A battery of
experiments. Available at SSRN 4543351 . Wang Y, Mishra S, Alipoormolabashi P, Kordi Y, Mirzaei A, Arunkumar A, Ashok A, Dhanasekaran AS,
Naik A, Stap D, et al. (2022) Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705 . Wei J, Tay Y, Bommasani R, Raffel C, Zoph B, Borgeaud S, Yogatama D, Bosma M, Zhou D, Metzler D,
et al. (2022a) Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 . Wei J, Wang X, Schuurmans D, Bosma M, Chi E, Le Q, Zhou D (2022b) Chain of thought prompting elicits
reasoning in large language models. arXiv preprint arXiv:2201.11903 . Xu L, Chen J, Whinston A (2011) Price competition and endogenous valuation in search advertising. Journal
of marketing Research 48(3):566–586. Xu L, Chen J, Whinston A (2012) Effects of the presence of organic listing in search advertising. Information
Systems Research 23(4):1284–1302. Xu R, Feng Y, Chen H (2023) Chatgpt vs. google: A comparative study of search performance and user
experience. arXiv preprint arXiv:2307.01135 . Xue J, Wang L, Zheng J, Li Y, Tan Y (2023) Can chatgpt kill user-generated q&a platforms? Available at
SSRN 4448938 . Positive, negative, or zero interdependence? Marketing science 29(4):602–623. Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang J, Dong Z, et al. (2023) A survey
of large language models. arXiv preprint arXiv:2303.18223 . Zhu Y, Yuan H, Wang S, Liu J, Liu W, Deng C, Dou Z, Wen JR (2023) Large language models for information
retrieval: A survey. arXiv preprint arXiv:2308.07107 . Ziakis C, Vlachopoulou M, Kyrkoudis T, Karagkiozidou M (2019) Important factors for improving google
search rank. Future internet 11(2):32. sample prompt Please answer the question based on the content in the file attached: { query content } Please answer the question with several clear references to the original files. sample query Why do we wear ties? Like why as in the origins, not just to look professional. Who said it made us look professional? sample answer from gpt-4 api The origins of wearing ties trace back to various historical points and represent different meanings and purposes throughout history:
- **Ancient Roots in China**: Ties, in a form akin to scarves, were worn by ancient Chinese warriors; terracotta statues that are around 2000 years old display warriors with silk around their necks, indicating that ties were a sign of power and to establish identity even in ancient times【7†source】. - **Croatian Mercenaries’ Influence**: The tie gained popularity during the 30-year war in the 17th century when Croatian mercenaries who fought alongside King Louis XIII of France wore pieces of cloth around their neck to tie their jackets; King Louis XIII himself took a liking to this practice, which helped to popularize the accessory【8†source】. - **Roman Era Representation**: Soldiers depicted on Trajan’s Column, a monument commemorating Roman Emperor Trajan’s victory over the Dacians, are shown wearing ties as a symbol of a warrior’s courageous character, providing further historical evidence of ties being associated with military and strength
【9†source】. - **Elitism and Professionalism**: In Victorian England, guidelines on how to properly wear and tie a necktie emerged, such as the advice found in the book "Clothes And The Man," which could imply a sense of professionalism and elitism associated with the proper way to wear a tie【10†source】. - **Designer Trends in the 1920s**: The modern notion of the tie as a professional and decorative accessory might be traced to the 1920s, where French fashion designers innovated the "designer tie" utilizing expensive fabrics and textile patterns to create stylish and coveted pieces【11†source】. fication to part of military uniform, and finally into a sophisticated fashion item that carries connotations of professionalism, status, and social identity. The tradition of wearing ties has been influenced by various cultures and periods, eventually leading to the contemporary association of ties with professional attire. Sample Annotations 【7†source】 Quote: Ancient Chinese warriors are the first known wearers of scarves. A discovery of 2000-year-old terracotta statues shows the warriors wearing silk around their necks. This shows that even back then, ties were a sign of power and to show identity. 【8†source】 Quote: During the 30 year war, Croatian mercenaries fought alongside the King in battle. To keep their jackets tied they used a piece of cloth which King Louis XIII took a strong liking to ... 【9†source】 Quote: When Roman emperor Trajan defeated the Dacians, he erected a marble column to celebrate his victory. This column featured thousands of soldiers, many of whom were wearing ties, again as a symbol of a warrior’s courageous character. 【10†source】 Quote: When it came to bow ties, the major was very clear: “Of course, no gentleman ever does wear a made-up tie.” 【11†source】 Quote: In the 1920s, French fashion designers innovated the “designer tie,” which was made from expensive fabrics and decorated with textile patterns. interpretation of the results In this interaction, we submit a query along with a document comprising chunks extracted from all the corresponding websites. The API subsequently furnishes a cohesive response, supported by four distinct references, each delineating the original portions (i.e., annotations) sourced from the uploaded document. Subsequently, we leverage these annotations to identify the websites containing these four annotations, designating them as cited websites, while categorizing the remaining websites as uncited.