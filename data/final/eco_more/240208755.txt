Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating reinforcement learning models or collecting data that involves human subjects. Existing work highlights the ability of Large Language Models (LLMs) to address complex reasoning tasks and mimic human communication, while simulation using LLMs as agents shows emergent social behaviors, potentially improving our comprehension of human conduct. In this paper, we propose to investigate the use of LLMs to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning. We make an assumption that LLMs can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from LLMs to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion). We experimentally evaluate the ability of our framework to model sub-rationality through four simple scenarios, including the well-researched ultimatum game and marshmallow experiment. To gain confidence in our framework, we are able to replicate well-established findings from prior human studies associated with the above scenarios. We conclude by discussing the potential benefits, challenges and limitations of our framework. Equal contribution Applied Research Team (ART) IT Department, Bank of Italy, Rome, Italy. This research work was carried out when Andrea Coletta was employed at J.P. Morgan AI Research. The views expressed in this paper are those of the authors and do not necessarily reflect those of the Bank of Italy. J.P. Morgan AI Research, California, USA. J.P. Morgan AI Research, New York, USA. Correspondence to: Andrea Coletta <andrea.coletta@bancaditalia.it>, Kshama Dwarakanath <kshama.dwarakanath@jpmorgan.com>, Penghang Liu <penghang.liu@jpmchase.com>. 1. introduction In the recent years, Large Language Models (LLMs) have been among the most impressive achievements in Artificial intelligence (AI) and Machine Learning (ML), raising questions about whether we are close to an Artificial General Intelligence (AGI) system (Bubeck et al., 2023; Fei et al., 2022). Foundation models like ChatGPT (OpenAI, 2023b) have shown remarkable performance across a wide range of novel and complex tasks, including coding, vision, healthcare, law, education, and psychology (Bubeck et al., 2023). These models perform close to human experts, without needing additional re-training or fine-tuning, apparently mimicking human abilities and reasoning (Guo et al., 2023; Min et al., 2021; Bubeck et al., 2023). In particular, recent work has demonstrated how LLMs can emulate human reasoning in solving complex problems: LLMs can simulate chain of thoughts, generating a reasoning path to decompose complex problems into multiple easier steps to solve (Kojima et al., 2022; Wei et al., 2022b; Ho et al., 2022). We could conclude that LLMs are able to replicate high-level cognitive and reasoning capabilities of humans (Huang & Chang, 2022; Hagendorff et al., 2022), especially with bigger foundation models like GPT-4 (Wei et al., 2022a). If we assume this is true, a natural question arises: “Can we leverage LLMs to simulate aspects of human behavior that deviate from perfect rationality so as to eventually improve our understanding of diverse human conduct?”
Modeling human behavior has seen significant interest in various domains, ranging from robotics for human-robot collaboration (Dragan et al., 2015), to finance and economics for modeling human investors and consumers (Liu et al., 2022; Akerlof & Shiller, 2010). In studies of human-robot interactions, it is commonly embraced that humans must not be modeled as optimal agents (Carroll et al., 2019), and an irrational human when correctly modeled, can communicate more information about their objectives than a perfectly rational human can (Chan et al., 2021). For instance, humans exhibit bounded rationality (Simon, 1997), often making satisfactory but not optimal decisions due to their limited knowledge and processing power. They are also known to be myopic in their decision making as they care more about short-term rewards than long-term rewards. While the discount factor in reinforcement learning (RL) may capture myopicity through exponential discounting, there is ample
ar X
iv :2
40 2. 08 75
5v 1
[ cs
.A I]
1 3
Fe b
20 24
evidence to show that other temporal discounting models such as hyperbolic discounting (Ainslie, 1992) and quasihyperbolic discounting (Laibson, 1994) are more realistic in capturing time-inconsistent human behavior (Chabris et al., 2010). The latter models are of particular interest in modeling household consumption and saving behavior in economic studies (Angeletos et al., 2001). Empirical evidence from humans also demonstrates that the rate of discounting reduces with an increase in the amount of reward (Green et al., 1997). When combined with the variability of the rate parameter across humans, this makes the calibration of parameters challenging for different humans, reward amounts, and scenarios. For example, works on estimating parameters for temporal discounting models from human studies exhibit disagreement among each other due to additional experiment-specific considerations (Frederick et al., 2002). The non-exponential discounting models, along with the dependence of discount factor on the reward pose challenges to classical RL algorithms relying on the validity of the Bellman equation. Other models have been proposed to encompass different human biases, including prospect theory, illusion of control, and optimism, which often employ nonlinear regression techniques to calibrate their parameters (Langer, 1975; Tversky & Kahneman, 1992; Sharot et al., 2007). However, such approaches demand substantial effort in collecting samples of human behavior, and it is impractical to obtain a set of parameters capable of representing individuals with different levels of bias and rationality. Also, recruiting human subjects for behavioral experiments comes with concerns surrounding ethics, privacy and general subject disinterest in the absence of monetary incentives (Desposato, 2015; Abeler & Nosenzo, 2015). Recent work studies the possibility of using LLMs to simulate and analyze human behaviors (Ziems et al., 2023; Chiang & Lee, 2023; Aher et al., 2023; Sejnowski, 2023; Horton, 2023; Argyle et al., 2023; Korinek, 2023; Williams et al., 2023). G. Aher et al. (Aher et al., 2023) evaluate how LLMs effectively simulate different aspects of human behavior, reproducing findings from some classic economic and psychology experiments. J. J. Horton (Horton, 2023) investigates the computational capabilities of GPT-3 to model economic agents according to classical behavioral economics literature. Other work employs interactive LLM agents to simulate more sophisticated societal contexts, where they are eventually able to reproduce realistic social interactions and emergent behaviors (Park et al., 2023; 2022; Gao et al., 2023). Such remarkable performance in agentbased simulation could be related to the ability of LLMs to emulate different individuals, as studied in (Argyle et al., 2023). Their work shows how LLMs effectively emulate different sociodemographic human groups under appropriate conditioning procedures. Similarly, P. Schramowski et al. (Schramowski et al., 2022) discover that LLMs, because
of how they are trained and designed, eventually contain human-like biases reflecting our behavior and morality. In fact, LLMs naturally encode a wide range of human behavior seen in the training data (Bommasani et al., 2021; Brown et al., 2020), potentially representing implicit computational models of humans. The ability to display such human qualities is what makes LLMs a disruptive tool, possibly leading to a new paradigm shift: scientists could be able to easily conduct new experiments that previously would have been time-consuming or just impossible, by using LLMs for simulation (Kuhn, 1964; Ziems et al., 2023). In this work, we investigate the ability of LLMs to synthesize a broad range of subrational human behavior in temporal decision making tasks. Our framework learns behavioral policies through Imitation Learning (IL) (Hussein et al., 2017) using synthetic human-like demonstrations generated by LLMs. This addresses the limitations of classical RL techniques in modeling subrational behavior (Ng et al., 2000) by overcoming the need for human demonstration data, which is expensive to gather for different human subgroups. Our approach assumes that LLMs represent implicit computational models of humans. Therefore, we can generate synthetic demonstrations to model subrational human behaviors, including irrational and myopic behaviors. Our primary goal is to initiate a novel research area that uses LLMs to synthesize subrational behavior, which has received limited preliminary attention (Aher et al., 2023; Horton, 2023; Korinek, 2023) albeit with enormous potential. Specifically, we summarize our contributions as follows: • We propose a novel research direction of using LLMs to calibrate subrational decision making models; • We propose a framework to leverage the ability of LLMs to generate synthetic demonstrations in order to learn subrational agent policies through IL; • We evaluate the ability of our framework to produce subrational behavior as observed in established human studies; • Finally, we discuss the potential benefits, opportunities and pitfalls of utilizing LLMs for human modeling, outlining future research directions in this domain. 2. literature overview Deep learning models have recently found extensive application in simulating complex systems and analyzing agent behaviors (Ha & Schmidhuber, 2018; Matsuo et al., 2022). They have been employed in various domains, including complex financial markets (Coletta et al., 2022; 2023), as well as urban driving scenarios (Hu et al., 2022). The advent of LLMs has created more opportunities for modelling complex systems, especially focused on human behaviors. Recent research efforts like (Park et al., 2023) and (Gao et al., 2023) have utilized LLMs to create sophisticated agent-based simulators. These simulators contain LLM
based agents that interact with others through natural language prompts carefully designed by incorporating world perception, reflection and planning within a defined environment. They demonstrate the ability of agents to generate believable behavior as assessed by human evaluators, while giving rise to emergent outcomes such as information diffusion, relationship formation, and agent coordination. J.J. Horton (Horton, 2023) demonstrated the ability of LLMs to mimic human preferences arising from different qualitative personas. For example, given the following scenario “A hardware store has been selling snow shovels for $15. The morning after a large snowstorm, the store raises the price to $20.”, the "libertarian" LLM found the increase acceptable while for the "socialist" LLM the increase is unfair. Also, recent work (Argyle et al., 2023; Aher et al., 2023) validates the ability of LLMs to model humans from a variety of subgroups. This prompts us to consider whether LLMs can effectively capture subrational human behaviors without the need for extra effort in reward design (Chan et al., 2021), eventually supporting or replacing traditional agent-based models (LeBaron, 2006; Bouchaud et al., 2018). Instead of modelling an entire system using LLM agents, which is expensive and harder to control, we use LLMs to generate a few synthetic demonstrations for subrational behavior. Such demonstrations can be validated and used within an IL framework to learn agent policies encompassing existing human biases and beliefs. Complementary work in (Kwon et al., 2022) employs LLMs to directly generate a proxy reward function based on natural language prompts. While such a proxy reward can align the training of policies with user-defined objectives, it cannot generate human behavior beyond what is reflected in the reward function. For example, (Reddy et al., 2018) argues that subrational human behavior can arise from incorrect beliefs of environment dynamics rather than the reward function. Also, such a proxy reward does not handle non-exponential discounting models, such as hyperbolic or quasi-hyperbolic discounting. By directly updating the policy network to mirror humanlike behavior generated through LLMs, our methodology addresses these limitations, bypassing the dependence on exponential MDPs. On the other hand, we note that our work requires more careful generation of synthetic demonstrations covering the state space of interest. A lacking of which coverage can be handled using approaches combining RL and IL (Correia & Alexandre, 2023). This new paradigm of generating synthetic human demonstrations using LLMs could benefit the broader research community. LLMs can compensate for the scarcity of expert demonstrations, which are extremely useful to learn behaviors from (Hester et al., 2018) and calibrate human subrationality models proposed in psychological and economic studies (Tversky & Kahneman, 1992; Green et al., 1997; Simon, 1997). Algorithm 1 IL with LLM Demonstrations Input: Demonstrations D = {(si, ai)}ni=1, epochs E Initialize: Policy weights Θ for all k from 1 to E do
Sample mini-batch s ∈ D Human-preference h(s) = KDE({a|(s,a) ∈ D}) Policy preference n(s) = softmax (QΘ(s,a)) Compute loss ||h(s)− n(s)||2 Perform gradient descent update on Θ
end for 3. learning subrational behavior using llms We here discuss our framework to learn subrational policies using LLM demonstrations. We first generate synthetic human demonstration data prompting the LLMs, and then use IL to train agent policies based on such demonstrations. We use Markov Decision Processes (MDPs) as a natural underlying decision model for an agent in temporal decision making tasks. An MDP is a tuple M = (S,A,P,R) comprising: the state space S; the action space A; a transition function P : S × A → ∆(S), where ∆(S) denotes probability distributions over state space S; and a reward function R : S ×A → R. A rational agent follows policy π : S → ∆(A) that maximizes the expected reward:
J(π) = Eat∼π(st),st+1∼P(st,at) [ T−1∑ t=0 γtR(st, at) ]
where γ ∈ [0, 1] represents the discount factor that is used to prioritize immediate rewards over those got later in time. Three challenges of using direct RL to model human behavior are: 1) the design of a reward function R that accurately captures desired human behavior (Ng et al., 2000; Kwon et al., 2022); 2) the large amount of data and training required before RL policies reach reasonable performance (Hester et al., 2018); and 3) the inability to capture time-inconsistent human preferences when using exponential discounting of rewards e.g., preference reversals (Chabris et al., 2010). We propose to mitigate these challenges by using a small set of synthetic human (or expert) demonstrations that are generated using LLMs. Such demonstration data can overcome the need to design an intricate reward function as we can use IL. 3.1. generation of llm demonstrations The synthetic demonstration data comprises state action pairs over numerous episodes D = {(si, ai)}ni=1, and is generated by prompting the LLM using either chain-ofthoughts to create a reasoning path (Wei et al., 2022b), or summarizing the state and possible actions using prompt engineering (Reynolds & McDonell, 2021). We report details of data generation in the experimental section where we use
GPT-4 (OpenAI, 2023a) due to its expressivity. Demonstration data can massively accelerate the RL training process when initialized with a policy learned using supervised learning on the data. Importantly, human agents may have irrational behavior in specific scenarios (i.e., states) which may be difficult to identify and model. LLMs have shown the ability to reproduce such biased behaviors (Aher et al., 2023; Schramowski et al., 2022) providing credible and novel advantages to using such demonstrations to model subrational human agents. Lastly, these demonstrations can be used to fine-tune exponential RL policies to capture timeinconsistent human behavior, which would have otherwise been intractable to model using standard RL. 3.2. imitation learning with llm demonstrations Given demonstration data D, we can use IL to learn a policy πθ for the agent, formally defined as:
argmax θ
E(s,a)∼D[log πθ(a|s)]
Algorithm 1 reports the proposed procedure, where we initialize the agent policy network πΘ, and in particular the value function Q(s, a). Then for each state s ∈ S we extract the distribution of agent actions in the synthetic LLM demonstrations, using a non-parametric Kernel Density Estimation. We then update the policy πΘ by minimizing the difference between its estimated action distribution and the one from synthetic demonstrations. While more sophisticathed algorithms can be employed (e.g., GAIL (Ho & Ermon, 2016)) we adopt a simple algorithm as our goal is to study the application of LLMs in generating synthetic human demonstrations, rather than to propose a novel IL algorithm. Note that pure IL like above requires demonstrations encompassing the entire state space. If not, one could start with learning the policies using an RL training phase followed by IL, similarly to (Hester et al., 2018). 4. experiments In this section, we test the ability of our framework to simulate human behaviors in four economic games where humans demonstrate subrational behavior, and show that we can replicate findings from existing literature. We use GPT4 (OpenAI, 2023a) as the current most expressive model, and a 3-layer neural network for the Q value function. We consider the following games: • The Ultimatum Game (Güth et al., 1982), a game theory experiment where a player decides how to split an amount of money, and another player can accept or reject the split. • The Stanford marshmallow experiment (Mischel & Ebbesen, 1970) where a player is offered a choice between one small but immediate reward, or a bigger reward if they waited for a period of time. • The Double or nothing gamble where a player is offered
a choice to double their money or lose everything, in order to illustrate human decisions under risk as described by prospect theory (Kahneman & Tversky, 1979). • The Procrastination experiment (O’Donoghue & Rabin, 1999) where a student player chooses which day to write a report and miss a good movie given an academic deadline. 4.1. ultimatum game Scenario. The ultimatum game considers two players splitting a given amount of money T . The first player (the proposer) decides to split the money between himself and the responder. The responder (who knows the total sum) decides to either accept or reject the split. If the responder rejects the proposal, both players receive nothing; if the responder accepts, they both get the agreed amount of money. For example, the proposer decides to split T = $10 by giving $2 to the responder and taking $8 for himself; if the responder accepts they get $8 and $2, respectively. Otherwise they both get nothing. Social Preferences and Limited Rationality. If we assume that agents have stable, well-defined, and rational preferences, we realize that the theory for the simple ultimatum game is rather obvious: a rational responder would accept any non-zero proposal. In fact, even if the proposal is unfair, getting something is better than getting nothing. This represents a Nash equilibrium for the ultimatum game. In reality though, human behavior is much different. The work of Nobel laureate Richard Thaler (Thaler, 1988) focuses on some "anomalies" in the ultimatum game, defined as demonstration of limited rationality and social preferences by human participants. They show that a human responder will likely reject unfair proposals, as they prefer to "punish" the proposer, even though they will receive nothing. According to their own social preference, people will prefer "fair" (i.e., 50:50) splits (Sanfey et al., 2003), or in general they often reject offers less than 30% of the initial sum T (Houser & McCabe, 2013; Krawczyk, 2018). Experimental Setting. We consider an initial amount of T = $10 for the game. Therefore, the game has 10 possible actions for the proposer x ∈ {0, . . . , 10}, where x is the amount to offer and (10 − x) is the amount they keep for themselves. Accordingly, each offer is a possible state for the responder who has 10 states and 2 actions a ∈{accept, reject}. In this experimental setup, we focus on modelling the human responder, which we consider the more interesting player in the game. We model the proposer using a Multi-Armed Bandit (Sutton & Barto, 2018); and use an RL policy network for the responder. The two agents are trained together using as reward the amount of money they individually get. We alternately use IL with LLM demonstrations for a human responder or one that cares about a
fair split to obtain the responder policy. Synthetic demonstrations collected using LLMs. We instruct GPT-4 as "Impersonate Jerry who cares a lot about fairness." or "Impersonate a human called Jerry". We call the two experiments "Fair" and "Human", respectively. We then prompt the model as: John receives $T, and he proposes to offer Jerry x and keep T-x for himself. If Jerry accepts they both get the agreed amount, but if Jerry rejects they both receive nothing. Continue the sentence: ‘Jerry decides to’, where T = 10 and x ∈ {0, . . . , T}. We then convert each prompt and answer into a state-action (s, a) pair. We generate 10 synthetic observations for each state x. Results. Figure 1 shows rewards and distribution of money kept by the proposer. Figure 1. (a) shows classic RL training for both agents, where the proposer learns to split the money in an unfair manner, and the responder accepts any splits. In fact, this is the expected outcome from a fully rational responder. In Figure 1. (b) we use LLM demonstrations to train a "human" responder model, which now rejects very low (i.e., less than 20%) splits, and thus the two agents converge to a $8 and $2 split. In Figure 1. (c) we use LLM demonstrations to train a "fair" responder model, which now rejects unfair (i.e., unbalanced) splits, and thus the two agents converge around a 50:50 split. Finally, Figure 1. (d) reports the distributions of money the proposer keeps for the three variants. Comparison to human subject studies. We compare our results with those in economic literature in (Houser &
McCabe, 2013) and (Krawczyk, 2018). These works show that a human responder is less likely to accept offers below $3, out of $10. This is inline with our results confirming that synthetic LLM demonstrations could be effectively used to model human interactions, as is also highlighted in recent work (Aher et al., 2023). Importantly, LLMs enable the simulation of a broader range of scenarios without any additional costs (e.g., $100 instead of $10). 4.2. stanford marshmallow experiment. Scenario. The Stanford Marshmallow Experiment is a classic psychological study that explores delayed gratification in children. In this experiment, a child is placed in a room with a tempting marshmallow or a similar treat. The researcher presents the child with a choice: they can either eat the marshmallow immediately or wait for a predetermined amount of time (usually 15 minutes) without eating it. If the child can resist the temptation and wait until the time elapses, they are rewarded with an additional marshmallow. This experiment is used to investigate the ability of children to delay their desires and make choices based on long-term benefits rather than immediate gratification. Self-Control and the Stanford Marshmallow Experiment. If we assume that children have strong self-control and can easily delay gratification, the theory behind the Stanford Marshmallow Experiment might seem straightforward: a child with impeccable self-discipline would wait patiently for the delayed reward, in this case, an extra marshmallow. In this ideal scenario, the child would understand that waiting for the second marshmallow is a more desirable outcome than consuming the first one immediately, representing a form of self-control equilibrium in this experiment. In reality, however, the behavior of children in the marshmallow experiment is quite different. The work of Walter Mischel (Mischel & Ebbesen, 1970) uncovered what could be considered "anomalies" in human self-control and decision-making. They demonstrated that many children struggle with self-control and are unable to resist the immediate temptation of eating the first marshmallow, even though waiting would lead to a greater reward. In particular, such work revealed that children’s self-control abilities vary, and their choices in the experiment are influenced by factors like age, environment, and individual differences. While incorporating these factors into RL is non-trivial, obtaining human demonstrations for very specific settings can be expensive or impossible, and synthetic demonstrations from LLMs could overcome these challenges. Experimental Setting. We focus on modelling a child aligned with the psychological game in this setup. We consider a single agent with 2 actions a ∈ {accept, wait} for the candy in the current state. We consider one initial
state where the agent can choose to accept the candy now or wait, and two subsequent states derived by each choice. Synthetic demonstrations collected using LLMs. We instruct GPT-4 as "You are Janice a Y years old child.". We prompt the model as: Janice is offered to get one candy now, or to wait for 2 more hours to eventually get two candies. Continue the following sentence ’Janice decides to’, where Y ∈ {2, 5}. Interestingly, we noticed that with ‘15 minutes’ waiting like the original experiment, the model is more inclined to wait (Appendix 15), possibly indicating different human preferences w.r.t. the original paper dated more than 50 years ago. We generate 10 synthetic observations for each age Y . Results. In Figure 2. (a) we show a classic RL approach trained until convergence, where the reward is the amount of candies the agent gets. The agent is completely rational, and it learns to always wait to get two candies. Most important, it is very hard to model different background factors for the agent, like age. For example, can we derive an RL policy for a child of 2 years vs a child of 5 years? A common approach could be myopic RL modelling shown in Figure 2. (b) where we modify the discount factor γ to let the agent being more myopic (i.e,. looking only for immediate rewards). In the setting with γ = 0.3 the agent chooses to take the candy immediately, with a lower reward. Even if we can use myopicity modeling, it is not clear how to relate the value γ with the age of the child. Instead, an LLM could be easily prompted to get precise synthetic human observations. Figure 2. (c) shows that if we directly train an IL policy using
LLM demonstrations, we are able to reproduce behavior of a 2-year old child who takes one candy immediately, or a more rational 5-year old child that waits for more candies. Comparison to human subject studies. We compare our results with the study of waiting behavior of nursery school children by W. Mischel (Mischel & Ebbesen, 1970) comprising 16 boys and 16 girls, with age ranging from 3 to 8 years. In this experiment, they found that most children do not wait, when both rewards are visible to them; while around 75% of children wait if both rewards having been removed from their sight. Our results show a possible reconstruction of this experiment: a 2-year old child does not wait, while a 5-year old child is more rational and waits; with a mix of both actions for a 3-year old child (details in the appendix). These findings are aligned with the aforementioned study which suggests that the capacity to wait for long-term goals is develop markedly at about ages 3-4. Nonetheless, we also note that the original experiments are from 50 years ago, making current human biases and needs very different (e.g., children may have easier access to treats now). 4.3. double or nothing gamble Scenario. We create a simple economic game named “double or nothing" to illustrate human bias in decisionmaking under risk. Assume that two people are gambling, and person A has lost an initial bet of $5 against person B. Before the initial bet is paid, person A (or B) can choose to play a second bet of $5 with a higher probability (> 50%) of losing (or winning). Since the second bet is an unfair game that favors the winner (person B), a rational agent will always deny the second bet as person A or accept the second bet as person B. However, humans in real life are willing to take the risk of the second bet as person A (hoping to offset their loss from the previous bet), or deny the second bet as person B (afraid of losing their current reward). Prospect theory. The expected utility theory is a fundamental model of human decision making under uncertainty, which says that an agent chooses between risky prospects based on expected utility: U = ∑n i=1 pi ·xi, where pi is the probability of an outcome with payoff xi. RL aligns with this hypothesis as the value of a state is determined by the expected utility of next states weighted by their transition probabilities. But, a substantial body of evidence has shown that human decisions systematically violate this theory. To model such deviation, Kahneman and Tversky introduced prospect theory (Kahneman & Tversky, 1979) which has been widely used for explaining experimental findings on human risk taking over several decades (Benartzi & Thaler, 1995; Thaler et al., 1997; Barberis, 2013). The model consists of two key elements: (1) a biased value function v(xi) that is concave for gains, convex for losses, and steeper for
losses than gains, and (2) a nonlinear probability weighting function w(pi) which overweights small probability and underweights large probability. Humans with prospect bias are risk-averse over gains and risk-seeking over losses. Experimental Setting. We create the "double or nothing" gamble environment with unfairness factor of the second bet ϵ ∈ {0, 0.1, 0.2, 0.3, 0.4}. The winner of the first bet has probability {0.5 + ϵ} to win the second bet against the loser. We train both players separately. Given ϵ as the state, the agent takes an action a ∈ {accept, reject} to decide whether or not to add the second bet. A rational RL agent learns to maximize the expected reward from the two bets U = ∑n i=1 pi · xi, whereas the prospect biased RL agent
has reward function U = ∑n
i=1 w(pi) · v(xi) per Eq. 1 and Eq. 2 with parameters estimated from real human data. Synthetic demonstrations collected using LLMs. We prompt GPT-4 as follows: • 1) You won an initial $5 bet against Tom. Before you collect your reward, you can choose to add a second bet of $5 with {0.5 + ϵ} probability to win. If you win the second bet you will double your gain from the initial bet, but if you lose you will gain nothing. Continue the following sentence
‘you decide to’;
• 2) You lost an initial $5 bet against Tom. Before you pay your debt, you can choose to add a second bet of $5 with {0.5− ϵ} probability to win. If you win the second bet you will recover your loss from the initial bet, but if you lose you will double your loss. Continue the following sentence ‘you decide to’. We collect 10 demonstrations for each ϵ ∈ {0, 0.1, 0.2, 0.3, 0.4} and show the probability of accepting the second bet as the winner/loser in Table 1. Results. Table 1 shows the probability of adding the second bet in LLM demonstrations. In general, we observe that the loser (winner) of the first bet is more (less) willing to take the risk of playing an additional bet, unless the unfairness of the second bet is large enough (ϵ ≥ 0.3). We compare an IL policy trained over LLM demonstrations with a rational RL policy and the prospect-biased RL policy in Figure 3. Since the second bet always favors the winner, the rational RL agent learns to maximize their reward by accepting it as winner and rejecting as loser. On the other hand, agents trained using prospect theory and LLM demon-
strations yield similar (lower) rewards which are subrational e.g., when ϵ = 0.1, both are not willing to take the risk as winner, but prefer to take the additional bet as loser. Comparison to human subject studies. We compare behavior generated by the IL policy using LLM demonstrations to the RL policy using the prospect theory model specified by parameters estimated from real humans in the literature (Tversky & Kahneman, 1992). Both models yield similar behavior and rewards, showing risk-averse behavior in gains and risk-seeking behavior in losses. These results highlight the simplicity of our framework in incorporating subrational and biased behavior into agent policies without the need to design intricate or ad-hoc reward functions. 4.4. academic procrastination with deadlines Scenario. A student needs to spend one day to write a course report due within the next H days. They receive a credit for the submitted report on the last day. The theatre is playing movies in increasing order of quality over the next H days. Which day does the student choose to write the report and miss the movie? Let ct represent the cost of missing a movie on day t ∈ {1, 2, · · · , H} so that 0 < c1 < c2 · · · < cH . And, R > 0 is the reward received upon submitting the report on day H . This setup is based on the example on salient costs in (O’Donoghue & Rabin, 1999). Procrastination and Quasi-Hyperbolic Discounting. Studies have shown that a large fraction of students procrastinate on their academic deliverables until the deadline gets very close (Rothblum et al., 1986; Day et al., 2000). This is an example for the lack of self-control observed in humans: they choose to watch a movie today with the expectation that their future self would write the report tomorrow. But when tomorrow arrives, the same decision repeats until the deadline gets very close. Sophisticated humans who are aware about their lack of self-control expect their future selves to procrastinate and complete the report earlier than naive humans who are optimistic about their future selves (Pollak, 1968). Such behavior is modeled by quasi-hyperbolic discounting (Laibson, 1994) where the
value function in a state st is given by
V (st) = E [ R(st) + β
H−t∑ k=1 δkR(st+k)
]
where δ ∈ [0, 1] is the long-term discount factor similar to exponential discount factor γ, and β ∈ [0, 1] captures ‘short-term impatience’ representing the preference over immediate rewards today vs tomorrow (O’Donoghue & Rabin, 1999). With the assumption of exponential discounting (as in standard RL), one arrives at time-consistent preferences. Time-inconsistent preferences are those where the optimal policy from today’s perspective conflicts with the optimal policy from tomorrow’s perspective (Laibson, 1994). While it is unclear how to modify standard RL algorithms (that rely on the Bellman equation) to arrive at this time-inconsistent human behavior modeled by quasi-hyperbolic discounting, we show next that such behavior can be generated through IL using synthetic human demonstrations from LLMs. Experimental Setting. Let the deadline be H = 4 with the theatre playing a mediocre movie on day 1, a good movie on day 2, a great movie on day 3 and an excellent movie on day 4. We collect LLM demonstrations for three types of students as categorized by their Grade Point Average (GPA). As a baseline for comparison, we use a policy learned using RL with costs c1 = 1, c2 = 2, c3 = 4, c4 = 7 and final reward R = 14 with exponential discount factor γ = 1. Synthetic demonstrations collected using LLMs. We instruct GPT-4 as "You impersonate a student with GPA x, who loves watching movies. GPA measures the students commitment towards their academics.". We then prompt the model as: "The theatre has a line up of increasingly better movies as days pass. The student has a deadline to submit a course report within the next four days. The student needs to pick one day to write the report, which means they will miss the movie on that day. Continue the sentence ‘The student writes the report on day ’", where x ∈ {1, 3, 4.5}. We collect 10 demonstrations for each GPA x as shown in Figure 4. (a), where we see that the probability of writing the report on the first day increases as the GPA increases. Results. Figure 4. (b) shows the training rewards for an RL policy using the above cost and reward setting. A histogram of the day of writing the report when using the trained RL policy is plotted in Figure 4.(d). Observe that exponential discounting with γ = 1 says it’s optimal to write the report on day 1 since it picks t that maximizes R−ct. Thus, the RL policy always corresponds to writing the report on day 1. On the other hand, the LLM demonstrations can be used to train a policy by IL, whose rewards are plotted in Figure 4. (c) with the day of writing the report in Figure 4.(d). Note that when LLM demonstrations are used, they give rise to one
policy per student type/GPA. Observe from Figure 4. (c) that the rewards at the end of training are lower for lower GPA values, and improve as GPA increases. Since the cost of writing the report increases with the day of writing it, this says that the learned IL policies give earlier days of writing the report as GPA increases. The same can be observed in Figure 4. (d) where while the RL policy without any LLM demonstrations corresponds to writing the report on the first day, the IL policies with LLM demonstrations correspond to writing it on later days for lower GPAs. Thus, while the RL policy gives time consistent actions, incorporating LLM demonstrations helps us to capture procrastination, while accounting for differences in student attributes such as GPA. Comparison to human subject studies. We compare our results to a study of procrastinating behavior among 379 undergraduate students in an introductory psychology course (Rothblum et al., 1986). The study examines the extent of prevalence of procrastinating behavior, and the relationship between academic procrastination and academic performance. The authors found a positive correlation between self-reported procrastination and delay in taking selfpaced quizzes, and a negative correlation between procrastination and grade point average. Our findings firstly capture procrastination with the help of LLM demonstrations, and secondly are in line with this study since we observe higher delays in writing the report for students with lower GPAs. However, we note that this relationship between GPA and academic delay requires deeper investigation as remarked in (Solomon & Rothblum, 1984) since the observations are based on correlation and not causation. 5. discussion and broader context In this work, we investigate the utility of foundation models to synthesize subrational human decision making behavior, that deviates from perfectly rationality (Simon, 1955). Our work is motivated by the need to capture subrational and biased human behavior across disciplines ranging from economics of households (Frederiks et al., 2015) to financial markets (Liu et al., 2022), alongside the difficulty in obtaining historical data for the same to design calibrated subrational human models (Tversky & Kahneman, 1992; Green et al., 1997). We propose a framework to generate synthetic human demonstration data for a variety of human subgroups using LLMs, that is subsequently used with IL to derive behavioral policies for each human. Such demonstration data has potential to be used to calibrate subrational human models while being cost-effective and efficient to generate as opposed to real human studies that cover small subsets of the human population. This data is also useful when trying to capture human behavior that is hard to describe through the use of a reward function (Kwon et al., 2022), and in cases where exponential discounting does not hold (Chabris et al., 2010). Calibrated human models can improve realism of agent-based models of real economic systems, that can be subsequently used for policy applications (Farmer & Foley, 2009; Dong et al., 2023). We ground our framework by comparing our experimental findings in four economic games with real human studies in the literature, where we are able to reproduce similar findings.