We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice. We consider two versions: sequential, where Bob observes Alice’s cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler [AM95]. We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob’s preferences with increasing precision, thereby securing a disproportionate share of the resource over time. We analyze the limits of how much a player can exploit the other one and show that fair utility profiles are in fact achievable. Specifically, the players can enforce the equitable utility profile of (1/2, 1/2) in the limit on every trajectory of play, by keeping the other player’s utility to approximately 1/2 on average while guaranteeing they themselves get at least approximately 1/2 on average. We show this theorem using a connection with Blackwell approachability [Bla56]. Finally, we analyze a natural dynamic known as fictitious play, where players best respond to the empirical distribution of the other player. We show that fictitious play converges to the equitable utility profile of (1/2, 1/2) at a rate of O(1/ √ T ). 1 introduction Cake cutting is a model of fair division [Ste48], where the cake is a metaphor for a heterogeneous divisible resource such as land, time, memory in shared computing systems, clean water, greenhouse gas emissions, fossil fuels, or other natural deposits [Pro13]. The problem is to divide the resource
∗Alphabetical author ordering. S. Brânzei was supported in part by US National Science Foundation CAREER grant CCF-2238372. This material is based upon work supported by the National Science Foundation under Grant No. DMS-1928930 and by the Alfred P. Sloan Foundation under grant G-2021-16778, while S. Brânzei was in residence at the Simons Laufer Mathematical Sciences Institute (formerly MSRI) in Berkeley, California, during the Fall 2023 semester. This work is partially supported by DARPA QuICC, NSF AF:Small #2218678, NSF AF:Small #2114269 and Army-Research Laboratory (ARL) #W911NF2410052. †Purdue University. E-mail: simina.branzei@gmail.com. ‡University of Maryland. E-mail: hajiaghayi@gmail.com. §Purdue University. E-mail: phill289@purdue.edu. ¶University of Maryland. E-mail: suhoshin@umd.edu. ‖Purdue University. E-mail: wang5675@purdue.edu. ar X
iv :2
40 2. 08 54
7v 2
[ cs
.G T
] 1
8 Fe
b 20
among multiple participants so that everyone believes the allocation is fair. There is an extensive literature on cake cutting in mathematics, political science, economics [RW98, BT96, Mou03] and computer science [BCE+16], with a number of protocols implemented [GP14]. Traditional approaches to cake cutting often consider single instances of division. However, many real-world scenarios require a repeated division of resources. For instance, consider the recurring task of allocating classroom space in educational institutions each quarter or that of repeatedly dividing computational resources (such as CPU and memory) among the members of an organization. These settings reflect the reality of many social and economic interactions, necessitating a model that not only addresses the fairness of a single division, but also the dynamics and strategies that emerge among participants over repeated interactions. Repeated fair division is a classic problem first considered by Aumann and Maschler [AM95], where two players—denoted Alice and Bob—have private valuations over the cake and interact in the following environment. Every day a new cake arrives, which is the same as the ones in previous days. Alice cuts the cake at a point of her choosing, while Bob chooses either the left piece or the right piece, leaving the remainder to Alice. [AM95] considered the simultaneous setting, where both players take their actions at the same time each day, and analyzed the payoffs achievable by Bob when he can have one of two types of valuations. In this paper, we provide the first substantial progress in this classic setting. We further analyze the simultaneous version from [AM95] and also go beyond it, by considering the sequential version where Bob has the advantage of observing Alice’s chosen cut point before making his selection. Tactical considerations remain pivotal in the sequential version, which is none other than the repeated Cut-and-choose protocol with strategic players. A key observation in our study is the strategic vulnerability inherent in repeated Cut-andchoose. At a high level, if Bob consistently chooses his preferred piece, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob’s preferences with increasing precision, thereby securing a disproportionate share of the resource over time. To fight back Alice’s attempt to exploit him, Bob could deceive her by being unpredictable, thus hiding his preferences. While this behavior has the potential to reduce Alice’s share of the cake, it could also come at the price of affecting Bob’s own payoff guarantees in the long term. Our analysis of the repeated cake cutting game formalizes the intuition that Alice can exploit a (nearly) myopic Bob that often chooses his favorite piece. However, the outcome where Alice gets more value is not necessarily fair, as she is happier than Bob. The fairness notion of equitability embodies the idea that players should be equally happy, formally requiring that Alice’s value for her allocation should equal Bob’s value for his allocation. Achieving equitability is particularly important in scenarios with potential for conflict, such as splitting an inheritance. We show that achieving equitable outcomes in the repeated interaction is in fact possible. Specifically, each player has a strategy that ensures that the other player gets no more than approximately 1/2 on average, while they themselves also get approximately 1/2 on average. This approaches the equitable utility profile of (1/2, 1/2) in the limit. We obtain this result by using a connection with Blackwell approachability [Bla56]. Finally, we consider a natural dynamic known as fictitious play [Bro51], where the players best respond to the empirical frequency given by the past actions of the other player. We show that fictitious play converges to the equitable utility profile of (1/2, 1/2) at a rate of O(1/ √ T ). 1.1 model Cake cutting model for two players
We have a cake, represented by the interval [0, 1], and two players N = {A,B}, where A stands for Alice and B for Bob. Each player i has a private value density function vi : [0, 1]→ R+. The value of player i for each interval [c, d] ⊆ [0, 1] is denoted Vi([c, d]) = ∫ d c vi(x) dx. The
valuations are additive: Vi (⋃m j=1Xj ) = ∑m
j=1 Vi(Xj) for all disjoint intervals X1, . . . , Xm ⊆ [0, 1]. By definition, atoms are worth zero and the valuations are normalized so that Vi([0, 1]) = 1 for each player i. We assume the densities are bounded:
• There exist δ,∆ > 0 such that δ ≤ vi(x) ≤ ∆ for all x ∈ [0, 1]. A piece of cake is a finite union of disjoint intervals. A piece is connected if it’s a single interval. An allocation Z = (ZA, ZB) is a partition of the cake among the players such that each player i receives piece Zi, the pieces are disjoint, and ⋃ i∈N Zi = [0, 1]. The valuation (aka utility or payoff) of player i at an allocation Z is Vi(Zi). An allocation Z is equitable if the players are equally happy with their pieces, meaning VA(ZA) = VB(ZB). Let mA be Alice’s midpoint of the cake and mB Bob’s midpoint. Alice’s Stackelberg value, denoted u∗A, is the utility she receives when she cuts the cake at mB and Bob chooses his favorite piece, breaking ties in Alice’s favor. The midpoints and Alice’s Stackelberg value are depicted in Figure 1. repeated cake cutting Each round t = 1, 2, . . . , T , the next steps take place:
• A new cake arrives, which is identical to the ones in previous rounds. • Alice cuts the cake at a point at ∈ [0, 1] of her choice. • Bob chooses either the left piece or the right piece, then Alice takes the remainder. We consider two versions: sequential, where Bob observes Alice’s cut point at before choosing left/right 1, and simultaneous, where he only observes her cut point after making his choice. A pure strategy is a map from the history observed by a player to the next action to play. A mixed strategy is a probability distribution over pure strategies. 1.2 our results Our results will examine how players fare in the repeated game over T rounds. Given a history H, Alice’s Stackelberg regret is
RegA(H) = T∑ t=1 [ u∗A − utA(H) ] , (1)
where u∗A is Alice’s Stackelberg value and u t A(H) is Alice’s utility in round t under history H.
Suppose Alice uses a mixed strategy SA and Bob uses a mixed strategy SB. Then SA ensures Alice’s Stackelberg regret is at most γ against SB if RegA(H) ≤ γ for all T -round histories H that could have arisen under the strategies (SA, SB). Precise definitions for strategies and regret can be found in Section 3. alice exploiting bob We start with the following observation about the sequential setting. If Bob chooses his favorite piece in each round, then Alice can exploit him in the long run by running binary search on his midpoint until identifying it within a small error and then cutting near it for the rest of time. This will lead to Alice getting essentially her Stackelberg value in all but O(log T ) rounds, while Bob will get 1/2 in all but O(log T ) rounds. Proposition 1. If Bob plays myopically in the sequential setting, then Alice has a strategy that ensures her Stackelberg regret is O(log T ). This exploitation phenomenon holds more generally: if Bob’s strategy has bounded regret with respect to the standard of selecting his preferred piece in every round in hindsight, then Alice can almost get her Stackelberg value in each round. Her Stackelberg regret is a function of Bob’s regret guarantee, as quantified in the next theorem. Theorem 1 (Exploiting a nearly myopic Bob). Let α ∈ [0, 1). Suppose Bob plays a strategy that ensures his regret is O(Tα) in the sequential setting. Let Bα denote the set of all such Bob strategies. • If Alice knows α, she has a strategy SA = SA(α) that ensures her Stackelberg regret is O ( T α+1 2 log T ) . Moreover, Alice’s Stackelberg regret is Ω ( T α+1 2 ) for some Bob strategy in Bα. • If Alice does not know α, she has a strategy SA that ensures her Stackelberg regret is O ( T log T ) . This is essentially optimal: if SA guarantees Alice Stackelberg regret O(T β) against all Bob strategies in Bα for some β ∈ [0, 1), then SA has Stackelberg regret Ω(T ) for some Bob strategy in Bβ. 1The sequential version is the repeated Cut-and-choose protocol, where the players may not necessarily be honest. In contrast, in the simultaneous setting, Alice may not approach her Stackelberg value on every trajectory of play. In order to get her Stackelberg value in any given round, Alice needs to cut near Bob’s midpoint and Bob needs to pick the piece he prefers, say R. However, if Bob deterministically commits to picking R, he will be completely exploited by an Alice who cuts at 1, breaking any reasonable regret guarantee he might have. Indeed, any Bob with a deterministic strategy (possibly using different actions over the rounds) has a corresponding Alice who can completely exploit him. Therefore, any Bob strategy with a good regret guarantee would behave randomly, making it impossible for Alice to reliably get her Stackelberg value on every trajectory. For this reason, we focus on the sequential setting when studying how Alice can exploit Bob.2 equitable payoffs. Motivated by Theorem 1, we examine the general limits of how much a player can exploit the other one and whether fair outcomes are achievable, in both the sequential and simultaneous settings. Given a history H, player i is said to get an average payoff of γ if( 1
T ) T∑ t=1 uti(H) = γ . (2)
The left hand side of (2) is not expected utility, but rather the observed total utility averaged over T rounds. We say a utility profile (uA, uB) is equitable if uA = uB. In the single round setting, uA and uB will naturally represent the utilities of the players at an allocation. In the repeated setting, uA and uB will represent the time-average utilities of the players. The next theorems show that the equitable utility profile of (1/2, 1/2) can be approached on every trajectory of play, by having a player keep the other’s utility to approximately 1/2 on average while ensuring they themselves get at least approximately 1/2 on average. Theorem 2 (Alice enforcing equitable payoffs; informal). In both the sequential and simultaneous settings, Alice has a pure strategy SA, such that for every Bob strategy SB:
• on every trajectory of play, Alice’s average payoff is at least 1/2− o(1), while Bob’s average payoff is at most 1/2 + o(1). More precisely,
uA T ≥ 1 2 −Θ ( 1√ T ) and
uB T ≤ 1 2 + Θ
( 1
lnT
) ,
where ui is the cumulative payoff of player i over the time horizon T . A key ingredient in the proof of Theorem 2 is a connection with Blackwell’s approachability theorem [Bla56]. Generally speaking, Blackwell approachability can be used by a player to limit the payoff of the other player in a certain region of the utility profile. However, the main challenge is that there are uncountably many types of Bob and so Alice cannot apply the strategy from Blackwell directly. Instead, Alice’s strategy constructs a countably infinite set of representatives, which allows us to adapt Blackwell’s argument to this setting. We show a symmetric theorem for Bob in the sequential setting, while in the simultaneous setting Bob’s guarantee only holds in expectation.  Theorem 3 (Bob enforcing equitable payoffs; informal). • In the sequential setting: Bob has a pure strategy SB, such that for every Alice strategy SA, on every trajectory of play, Bob’s average payoff is at least 1/2− o(1), while Alice’s average payoff is at most 1/2 + o(1). More precisely,
uB T ≥ 1 2 − 1√ T and
uA T ≤ 1 2 + Θ ( 1√ T ) . • In the simultaneous setting: Bob has a mixed strategy SB, such that for every Alice strategy SA, both players have average payoff 1/2 in expectation. fictitious play Fictitious play is a classic learning rule where at each round, each player best responds to the empirical frequency of play of the other player. Fictitious play was introduced in [Bro51]. Convergence to Nash equilibria has been shown for zero-sum games [Rob51] and special cases of general-sum games [Nac90, MS96b, MS96a]. In the cake cutting model, learning rules such as fictitious play are more meaningful in the simultaneous setting, where there is uncertainty for both players due to the simultaneous actions. The precise definition of the fictitious play dynamic is in Section 6, while an example of trajectories for an instance with random valuations and uniform random tie-breaking can be found in Figure 2. The convergence properties of fictitious play can be characterized as follows. Theorem 4 (Fictitious Play; informal). When both Alice and Bob run fictitious play, the average payoff of each player converges to 1/2 at a rate of O (
1√ T
) . roadmap to the paper. Related work is surveyed in Section 2. Preliminaries, which state the necessary notation, can be found in Section 3. An overview of how Alice can exploit a nearly myopic Bob can be found in Section 4, with formal proofs in Appendix A. An overview of how players can enforce equitable payoffs can be found in Section 5, with formal proofs in Appendix B. Fictitious play can be found in Section 6, with formal proofs in Appendix C. 2 related work We overview related work in several areas, including fair division, repeated games and their connections with learning. Cake cutting and fairness notions. The cake cutting model was introduced in [Ste48] to capture the allocation of a heterogeneous resource among agents with complex preferences. There are n players, each with a private value density over the cake, which is represented as the interval [0, 1]. The goal is to find a “fair” allocation of the cake among the n players, where the fairness notion could include proportionality, envy-freeness 3, and equitability. The related task of necklace splitting [Alo87] requires dividing the cake in t pieces (not necessarily contiguous) such that each player has value 1/t for each of the pieces. Special cases include the consensus halving problem, where t = 2, and finding perfect allocations, where t = n. For surveys, see [RW98, BT96, Mou03, BCE+16, Pro13]. Existence of fair allocations. Proportional allocations with contiguous pieces can be computed efficiently for any number of players [EP84, DS61]. However, the existence of more stringent fairness notions such as envy-freenes is typically ensured by a fixed point theorem. An envy-free allocation with contiguous pieces exists for any instance with n of players via an application of Sperner’s lemma or Brouwer’s fixed point theorem [ES99, Str80], while the existence of perfect partitions is shown via Borsuk-Ulam [Alo87]. Complexity of cake cutting. There is a standard query model for cake cutting, called the Robertson-Webb (RW) query model [WS07], where a mediator (algorithm) helps the players find a fair division by asking them enough queries about their preferences until it has sufficient information to output a fair allocation. Proportional allocations with contiguous pieces can be computed with O(n log n) queries [EP84], with matching lower bounds for finding proportional (not necessarily contiguous) allocations were given in [WS07, EP06]. For the query complexity of exact envy-free cake cutting (possibly with disconnected pieces), a lower bound of Ω(n2) was given by [Pro09] and an upper bound of O ( nn nn nn )
by [AM16]. [ACF+18] designed a simpler algorithm for 4 agents. The problem of computing envy-free allocations with high probability was solved by [Che20]. Equitable, envy-free, or perfect allocations with minimum number of cuts cannot be computed exactly by RW protocols [Str08], thus ε-fairness becomes the goal. The query complexity of finding ε-fair (envy-free, equitable, perfect) allocations was analyzed in [CDP13, PW17, BN22, BN19, HR23]. [DQS12] studied the complexity of cake cutting in the white box model, showing that ε-envy-free cake cutting with contiguous pieces is PPAD-complete when the valuations are unrestricted (i.e. not necessarily induced by value densities, allowing externalities). See [GHS20, FRHHH22, Seg18] for more on the complexity of cake cutting. 3An allocation is envy-free if no player prefers the piece of another player to their own. The complexity of consensus halving was studied, e.g., in [DFH21, GHI+20, FRHSZ20] and the complexity of necklace splitting in [AG20, FRHSZ21]. Incentives in cake cutting. A body of work studied truthful cake cutting in the RW query model [MT10, BM15] and in the direct revelation model [CLPP13, BST23, BLS22, Tao22]. The equilibria of cake cutting protocols with strategic agents were considered in [NY08, BM13], with an algorithmic model of generalized cut-and-choose protocols proposed in [BCKP16]. [GI21] designed branch-choice protocols as a simpler yet expressive alternative. Multiple divisible/indivisible goods and chores. New models. The complexity of finding fair allocations in settings with multiple indivisible was considered in [OPS21, PR20, PR19, MS21, CKMS21, BCF+19] among many others. For more on the fair allocation of indivisible goods and applicable solution concepts, see, e.g., [ABFV22, Pro20, CGM20, CGM+21, PW14]. For the allocation of divisible or indivisible bads, see, e.g., [KMT21, CGMM21]. [GZH+11] studied fairness in settings inspired by cloud computing, where there are multiple divisible goods (e.g. CPU and memory) and the users have to run jobs with different resource requirements. [GZH+11] introduced the dominant resource fairness (DRF) mechanism and showed it has strong fairness and incentive properties. [PPS15] further studied the properties of the DRF mechanism for indivisible goods. [KSG+20] studied the allocation of multiple goods in settings where the players do not know their own resource requirements, with the goal of designing mechanisms that guarantee efficiency, fairness, and strategy-proofness. Cake cutting with separation was studied in [ESS21], fair division of a graph or graphical cake cutting in [BCE+17, BS21, DEG+22], multi-layered cakes in [IM21], cake cutting where some parts are good and others bad in [SH18], and when the whole cake is a bad (e.g. chore) in [FH18, DFHY18, HSY23]. Cake cutting in two dimensions was studied in [SHNHA17] and cake cutting in practice in [KOS22]. The allocation of multiple homogeneous divisible goods was studied in [CGPS22]. Dynamic fair division. Closest to our setting is the analysis in the book of [AM95] (page 243), where two players are dividing a cake with a cherry. Alice (the cutter) has a uniform density and so she does not care for the cherry, while Bob (the chooser) may or may not like the cherry. Alice and Bob declare their actions simultaneously and Alice is only allowed to cut in one of two locations. Additionally, Alice has a prior over the type of Bob she is facing. The question is how the players should behave in the repeated game. [AM95] analyze the set of payoffs approachable for Bob using Blackwell approachability. The difference from our setting is that we allow arbitrary value densities for the players and do not assume priors. Additionally, we also consider the sequential version. Online cake cutting was considered by [Wal11], in the setting where agents can arrive and depart over time and the goal is to still ensure some form of fairness. Dynamic fair division with multiple divisible goods was studied in [KPS14], with agents arriving over time but not departing and the allocation algorithm taking irrevocable decisions (i.e., the resources can never be taken back once given to an agent). [FPV15] considered the allocation of a single unit of a divisible resource when the arrivals and departures of the agents are uncertain. The question is how to maintain fairness and Pareto efficiency in a way that minimizes the disruptions to existing allocations. [BHP22] consider dynamic fair division with partial information, where T goods become available over a sequence of rounds, every good must be allocated immediately and irrevocably before the next one arrives, and the valuations are drawn from an underlying distribution. Learning in repeated Stackelberg games. The Stackelberg game (competition) was first introduced by [Sta34] to understand the first mover advantage of firms when entering a market. The Stackelberg equilibrium concept has received significant interest in economics and computer science, with real world applications such as security games [Tam11, BBHP15], online strategic classification [DRS+18], and online principal agent problems [HMRS23]. Our model can be seen as each player facing an online learning version of a repeated Stackelberg game. [KL03] considered a seller’s problem of designing an efficient repeated posted price mechanism to buy identical goods when it interacts with a sequence of myopic buyers. [GXG+19, BGH+20, ZZJJ23] considered a repeated Stackelberg game to study how the follower or leader can exploit the opponent in a general game with arbitrary payoffs. Their techniques, however, do not apply to our model as they typically consider the setting of one player knowing the entire payoff matrix trying to deceive the other player given various behavioral assumptions. Exploiting no-regret agents. Several works have considered the extent to which one player can exploit the knowledge that the other player has a strategy with sublinear regret. The goal is often to approach the Stackelberg value, the maximum payoff that the exploiter could get by selecting an action first and allowing the opponent to best-respond. In simultaneous games, [DSS19] showed that it is possible for the exploiter to get arbitrarily close to their Stackelberg value, assuming knowledge of the other player’s payoff function. [HLNW22] showed that, for certain types of sequential games, an exploiting leader can approach their Stackelberg value in the limit. Our Theorem 1 is a similar statement in our setting, but we bound the exploited agent’s behavior with an explicit regret guarantee rather than using discounted future payoffs. Moreover, our setting is not captured by the types of games they consider. Fictitious play. Fictitious play was introduced in [Bro51]. Convergence to Nash equilibria has been shown for zero-sum games [Rob51] and special cases of general-sum games [Nac90, MS96b, MS96a]. None of these results directly apply to our setting, but the most relevant is [Ber05], which covered non-degenerate 2 × n games (i.e. where every action has a unique best response). Our “2 × ∞” game is degenerate, as Bob does not have a unique best response to Alice cutting at mB. Few existing works apply fictitious play to settings where the players have continuous action spaces. An example is [PL14], which showed that a variant (stochastic fictitious play) does converge in two-player zero-sum games with continuous action spaces. [Kar59] conjectured that fictitious play converges at a rate of O(T−1/2). [BFH13] found small games where the convergence rate is O(T−1/2), but with very large constants in the O(). [DP14] disproved Karlin’s conjecture, showing that there exist games in which convergence takes place at a rate of Ω(T−1/n) using adversarial tie-breaking rules. [PPSC23] found more examples of games in which fictitious play converges exponentially slowly in the number n of actions that each player has. [Har98] showed that fictitious play converges at a rate of O(T−1) in 2 × 2 zero-sum games. [ALW21] considered settings with diagonal payoff matrices and non-adversarial tie-breaking rules and showed convergence rates of O(T−1/2). The result in [ALW21] does not imply a rate of convergence in our setting because requiring the payoff matrix to be diagonal would correspond to Alice only being allowed to cut at 0 or 1. This assumption is not as natural in our setting. In fact, if Alice can only cut at 0 or 1 the game becomes zero-sum. Furthermore, we allow arbitrary tie-breaking rules. Strategic experimentation In the general strategic experimentation model, there are n players and k arms. In every round, each player pulls an arm, receives the resulting reward, and obtains feedback about the other player. There are two types of feedback models: perfect monitoring, where the feedback consists of both the choice of the other player and their reward; and imperfect monitoring, where the feedback consists of the choice of the other player but not their reward. [BH99] study strategic experimentation with two players, two arms, and perfect monitoring in continuous time, where one of the arms is “safe” and emits steady rewards, while the other arm is “risky” and is governed by a stochastic process. The main effects observed in symmetric equilibria are a free rider effect and an encouragement effect, where a player may explore more in order to encourage further exploration from others. [KRC05] considered the same problem for exponential bandits, and obtained a unique symmetric Markov equilibria followed by various asymmetric ones. We refer to [HS17] for a survey. [Aoy98, Aoy11] study strategic experimentation with imperfect monitoring when there are two players and two arms with discrete priors, showing that the players eventually settle on the same arm in any equilibrium. This is a version of the agreement theorem by [Aum76] for the multi-player multi-armed bandit model, stating that rational players cannot agree to disagree. [RSV07, RSV13] also study the model with imperfect monitoring, but where the decision to switch from the risky arm to the safe one is irreversible. The model with imperfect monitoring has similarities to our setting. Our model also involves two players engaged in a strategic repeated game such that each player observes the action of the other player but not their reward. The main difference is that the payoffs of the players are misaligned, the action spaces are orthogonal, and the payoffs are deterministic. Social learning Initiated by [Ban92, Wel92], a long line of literature studied social learning (or herd behavior) to investigate agents who learn over time in a shared environment. Unlike the strategic experimentation model, the agents do not strategize against each other, but only observe the past actions and possibly rewards therein. The objective is to identify whether the social learning succeeds or fails if the agents receive private signals [SS00], have behavioral biases [BHSS23], or have a certain network structure [BG98]. Incentivized exploration by [KMP14, CH18] considers mechanism design in a similar problem, to induce the society to behave in a desired manner. 3 preliminaries In this section we formally define the notation needed for the proofs. All our notation applies to both the sequential and simultaneous settings, unless otherwise stated. history Recall T is the number of rounds. For each round t ∈ [T ],
• let at ∈ [0, 1] be Alice’s cut at time t and bt ∈ {L,R} be Bob’s choice at time t, where L stands for the left piece [0, at] and R for the right piece [at, 1]. • let At = (a1, . . . , at) be the history of cuts until the end of round t and Bt = (b1, . . . , bt) the history of choices made by Bob until the end of round t.
A history H = (AT , BT ) will denote an entire trajectory of play. strategies Let P be the space of integrable value densities over [0, 1]. A pure strategy for Alice at time t is a function
StA : [0, 1] t−1 × {L,R}t−1 × P × N→ [0, 1],
such that StA(At−1, Bt−1, vA, T ) is the next cut point made by Alice as a function of the history At−1 of Alice’s cuts, the history Bt−1 of Bob’s choices, Alice’s valuation vA, and the horizon T . For Bob, we define pure strategies separately for the sequential and simultaneous settings due to the different feedback that he gets:
• Sequential setting. A pure strategy for Bob at time t is a function
StB : [0, 1] t × {L,R}t−1 × P × N→ {L,R} . That is, Bob observes Alice’s cut point and then responds. • Simultaneous setting. A pure strategy for Bob at time t is a function
StB : [0, 1] t−1 × {L,R}t−1 × P × N→ {L,R} . Thus here Bob chooses L/R before observing Alice’s cut point at time t.
A pure strategy for Alice over the entire time horizon T is denoted SA = (S 1 A, . . . , S T A) and tells Alice what cut to make at each time t. A pure strategy for Bob over the entire time horizon T is denoted SB = (S 1 B, . . . , S T B) and tells Bob whether to play L/R at each time t.
A mixed strategy is a probability distribution over the set of pure strategies. 4 rewards and utilities Suppose Alice has mixed strategy SA and Bob has mixed strategy SB. Let u t A and u t B be the random variables for the utility (payoff) experienced by Alice and Bob, respectively, at round t. The utility of player i ∈ {A,B} is denoted
ui = ui(SA, SB) = T∑ t=1 uti . The utility of player i from round t1 to t2 is ui(t1, t2) = ∑t2
t=t1 uti. The expected utility of player i is E[ui] = ∑T
t=1 E[uti], where the expectation is taken over the randomness of the strategies SA and SB. Given a historyH, let uti(H) be player i’s utility in round t underH and let ui(H) = ∑T t=1 u t i(H)
be player i’s cumulative utility under H. midpoints and stackelberg value Let mA ∈ [0, 1] be Alice’s midpoint of the cake, with VA([0,mA]) = 1/2, and mB ∈ [0, 1] be Bob’s midpoint, with VB([0,mB]) = 1/2. Since the densities are bounded from below, the midpoint of each player is uniquely defined. Alice’s Stackelberg value, denoted u∗A, is the utility Alice gets when she cuts at mB and Bob chooses his favorite piece breaking ties in favor of Alice (i.e. taking the piece she prefers less). 4In fact, this is equivalent to the behavior strategy in which the player assigns a probability distribution given a history, thanks to Kuhn’s theorem [Kuh50, Kuh53]. The original version of Kuhn’s theorem is restricted to games with finite action space, but can be extended to any action space that is isomorphic to unit interval by [Aum61, DST16], which contains our setting. 4 alice exploiting bob In this section we give an overview of Theorem 1, which considers the sequential setting and quantifies the extent to which Alice can exploit a Bob that has sub-linear regret with respect to the benchmark of choosing the best piece in each round. The formal proof of Theorem 1 can be found in Appendix A. The proof of Proposition 1 is included in Appendix A as well. We start by defining the notion of Stackelberg regret [DRS+18, HLNW22]. Definition 1 (Stackelberg regret). Given a history H, Alice’s Stackelberg regret is
RegA(H) = T∑ t=1 [ u∗A − utA(H) ] , (3)
recalling that u∗A is Alice’s Stackelberg value and u t A(H) is Alice’s utility in round t under the history H.
For Bob, we consider the basic notion of static regret, where Bob compares his payoff to what would have happened if Alice’s actions remained the same but he chose the best piece in each round. Definition 2 (Regret). Given a history H, Bob’s regret is
RegB(H) = T∑ t=1 [ max { VB([0, at]), VB([at, 1]) } − utB(H) ] ,
recalling that utB(H) is Bob’s utility in round t under the history H.
Regret guarantees. Suppose Alice uses a mixed strategy SA and Bob uses a mixed strategy SB. We say that
• Alice’s strategy SA ensures Alice’s Stackelberg regret is at most γ against SB if RegA(H) ≤ γ for all T -round histories H that could have arisen under the strategy pair (SA, SB). • Bob’s strategy SB ensures Bob’s regret is at most γ if RegB(H) ≤ γ for all T -round histories H that could have arisen under the strategy pair (SA, SB). More broadly, a Bob strategy SB has regret γ if
• RegB(H) ≤ γ for all T -round historiesH that could have arisen under strategy pairs (SA, SB), for all Alice strategies SA. Next we provide a proof sketch for Theorem 1, which is divided in the next two propositions, corresponding to the cases where Alice knows and does not know α. Proposition 2. Let α ∈ [0, 1). Suppose Bob plays a strategy that ensures his regret is O(Tα) and let Bα denote the set of all such Bob strategies. Assume Alice knows α. Then she has a strategy SA = SA(α) that ensures her Stackelberg regret is O ( T α+1 2 log T ) . This is essentially optimal:
Alice’s Stackelberg regret is Ω ( T α+1 2 ) for some Bob strategy in Bα. Proof sketch. We sketch both the upper and lower bounds. Sketch for the upper bound. Let SB denote Bob’s strategy, which guarantees his regret is O(Tα). Suppose Alice knows α. Then Alice initializes an interval I = [0, 1] and uses the next strategy. Iteratively, for i = 0, 1, . . . ,:
1. Alice discretizes the interval I = [u,w] in a constant number of sub-intervals (set to 6) of equal value to her, by cutting at points ai,j for j ∈ [5] such that u < ai,1 < ai,2 < . . . < ai,5 < w. Denote ai,0 = u and ai,6 = w.
The full proof explains why the index j from step 3 is unique and why it is in fact necessary to include a slightly larger interval than [ai,j , ai,j+1] in the recursion step, due to Bob potentially having lied if his midpoint was very close to a boundary of [ai,j , ai,j+1] but on the other side. Sketch for the lower bound. The lower bound of Ω ( T α+1 2 ) relies on the observation that
rounds where Alice cuts near mB and Bob picks his less-preferred piece cost Bob very little but cost Alice a lot. More precisely, suppose mA < mB and Alice cuts at mB − ε. Then compared to his regret bound, Bob loses Θ(ε) if he picks the wrong piece. On the other hand, Alice loses Θ(mB −mA) = Θ(1) compared to her Stackelberg value. Bob can use this asymmetry by acting as if his midpoint were Θ ( T α−1 2 ) closer to mA than it
really is. Lying Θ ( T α+1 2 ) times costs Bob only Θ(Tα) regret, but costs Alice Θ ( T α+1 2 ) regret. To
avoid accumulating more regret than this, Bob can afterwards revert to picking his truly preferred piece; the damage to Alice’s payoff has already been done. Proposition 3. Let α ∈ [0, 1). Suppose Bob plays a strategy that ensures his regret is O(Tα). Let Bα denote the set of all such Bob strategies. If Alice does not know α, she has a strategy SA that ensures her Stackelberg regret is O ( T
log T
) . This is essentially optimal: if SA guarantees Alice Stackelberg regret O(T β) against all Bob strategies in Bα for some β ∈ [0, 1), then SA has Stackelberg regret Ω(T ) for some Bob strategy in Bβ. Proof sketch. Alice’s strategy that achieves O(T/ log T ) regret follows the same template as her strategy from Proposition 2. The only difference is that she sets η differently (and much larger) to cover any possible regret bound Bob could have. The main idea of the lower bound is that, if Alice does not know the value of α in Bob’s regret bound, she cannot know when she has true information about Bob’s preferences. We exploit this by having a Bob with O(T β) regret behave exactly like one with O(Tα) regret but a different midpoint. Then Bob can hide his deception from an Alice with O(T β) regret because he can tolerate more regret than Alice. Theorem 1 is implied by Proposition 2 and Proposition 3. We briefly remark that the players’ value densities must be bounded for Theorem 1 to hold; see Remark 1 in Appendix A.2 for a counterexample with unbounded value densities. 5 equitable payoffs In this section, we give an overview of the proofs for Theorem 2, which shows how Alice can enforce equitable payoffs, and Theorem 3, which shows how Bob can enforce equitable payoffs. The formal proofs can be found in Appendix B. 5.1 alice enforcing equitable payoffs We include a formal statement of Theorem 2 next. It contains the precise constants and shows that, in fact, the guarantee that Alice can enforce holds at every point in time t, as opposed to just at the end. Restatement of Theorem 2 (Alice enforcing equitable payoffs; formal). In both the sequential and simultaneous settings, Alice has a pure strategy SA, such that for every Bob strategy SB:
• on every trajectory of play, Alice’s average payoff is at least 1/2− o(1), while Bob’s average payoff is at most 1/2 + o(1). More precisely, for all t ∈ {3, . . . , T}:
uB(1, t) t ≤ 1 2 + 5∆ + 11 ln(2t/5) and
uA(1, t) t ≥ 1 2 − 4√ t− 1 ,
recalling that ∆ is the upper bound on the players’ value densities. Moreover, even if Bob’s value density is unbounded, his average payoff will still converge to 1/2. Proof sketch. Alice’s strategy is based on Blackwell approachability [Bla56]. The challenge is that Alice has to be prepared for an uncountably infinite variety of Bob’s valuation functions, while the original construction in [Bla56] works when the number of player types is finite. Another difference is that Alice’s action space is also infinite, which turns out to be necessary. We get around the infinite-Bob issue in two steps. First, Alice’s strategy defines a countably infinite set V as a stand-in for the full variety of Bobs. We design V to include arbitrarily good approximations to any valuation function. Second, we replace Blackwell’s original finite-dimensional space with a countably-infinite-dimensional one, where the elements of V are the axes. We define an inner product on this space and use it to adopt Blackwell’s argument. Briefly, Alice’s strategy tracks the average payoff to each type of Bob in V and defines S to be the region of the space where all of them have payoffs at most 1/2. In each round, she constructs a cut point which moves the Bobs’ average payoff closer to S, and in the limit traps them in S.
Under this strategy, Alice’s payoff guarantee is mostly a byproduct of Bob’s. If Bob and Alice have the same value density, then uA+uB = 1, so bounding Bob’s payoff to 1/2 also bounds Alice’s to 1/2. We achieve the substantially better bound on Alice’s payoff by explicitly including her value density vA in the set V of Bobs, thus eliminating any approximation error. 5.2 bob enforcing equitable payoffs We include a formal statement of Theorem 3 next, with the precise constants. Restatement of Theorem 3 (Bob enforcing equitable payoffs; formal). • In the sequential setting: Bob has a pure strategy SB, such that for every Alice strategy SA, on every trajectory of play, Bob’s average payoff is at least 1/2− o(1), while Alice’s average payoff is at most 1/2 + o(1). More precisely,
uB T ≥ 1 2 − 1√ T and
uA T ≤ 1 2 +
( ∆
2δ + 2 ) 1√ T ,
recalling that δ and ∆ are, respectively, the lower and upper bounds on the players’ value densities. • In the simultaneous setting: Bob has a mixed strategy SB, such that for every Alice strategy SA, both players have average payoff 1/2 in expectation. Proof sketch. We cover the simultaneous setting first because it informs the sequential setting. Simultaneous setting. Bob’s algorithm is extremely simple: in each round, randomly select L or R with equal probability. The expected payoffs to each player follow immediately. Sequential setting. This strategy can be thought of as a derandomized version of the simultaneoussetting strategy. The simplest way to derandomize it would be to strictly alternate between L and R, but if Bob runs that strategy Alice can easily exploit it. Instead, Bob mentally partitions the cake into √ T intervals I1, . . . , I√T of equal value to him. He then treats each interval Ii as a separate cake, alternating between L and R for the rounds Alice cuts in Ii. Alice can still exploit this strategy on a single interval Ii, but doing so can only give her an average payoff of 1/2 +O(VA(Ii)) = 1/2 +O(1/ √ T ). The proof shows that this bound applies for any Alice strategy. 6 fictitious play In this section we include a proof sketch of Theorem 4, which analyzes the fictitious play dynamic. The formal proof can be found in Appendix C.
To define fictitious play, we introduce the empirical frequency and empirical distribution of play:
• The empirical frequency of Alice’s play up to (but not including) time t is:
ϕtA(x) = t−1∑ τ=1 1{aτ=x} ∀x ∈ [0, 1] . • The empirical frequency of Bob’s play up to (but not including) time t is:
ϕtB(x) = t−1∑ τ=1 1{bτ=x} ∀x ∈ {L,R} . The empirical distribution of player i’s play up to (but not including) time t is: pti(x) = ϕti(x) (t−1) , where x ∈ [0, 1] for Alice and x ∈ {L,R} for Bob. Definition 3. (Fictitious play) In round t = 1, each player simultaneously selects an arbitrary action. In every round t = 2, . . . , T , each player simultaneously best responds to the empirical distribution of the other player up to time t. If there are multiple best responses, the player chooses one arbitrarily. Our main result in this section is proving that the average payoffs under fictitious play converge to (1/2, 1/2) and quantifying the rate of convergence. The precise statement is included next. Restatement of Theorem 4. When both Alice and Bob run fictitious play, regardless of tiebreaking rules, their average payoff will converge to 1/2 at a rate of O(1/ √ T ). Formally:∣∣∣∣uAT − 12 ∣∣∣∣ ≤ 2√10√T and ∣∣∣∣uBT − 12
∣∣∣∣ ≤ √10√T ∀T ≥ 5 . Proof sketch. To analyze the fictitious play dynamic, we define for each t = 0, . . . , T :
• αt = rt− ℓt, where rt is the number of times Bob picked R up to round t and ℓt is the number of times he picked L
• βt = ∑t
τ=1
( 2VB([0, aτ ])− 1 ) . The quantities αt and βt control what happens under fictitious play: Alice’s decision in round t+1 is based on αt and Bob’s decision in round t+1 is based on βt. These decisions in turn affect
αt+1 and βt+1, forming a dynamical system. In general, these dynamics result in a counterclockwise spiral through α-β space. Figure 6 illustrates the sequences αt and βt for the instance in Figure 2. We define ρt = |αt| + |βt| and formalize this spiral, by showing that the sequence {ρ}Tt=0 is non-decreasing and analyzing the change in (αt, βt) from round to round. Figure 6 illustrates the parameter ρt over time. Figure 7 illustrates the spiral (associated with the same trajectory as in Figure 2 and 6), where the spiral is visualized as a scatter plot of the sequence (αt, βt)t≥1. We first use these dynamics to bound Bob’s payoff. Bob’s payoff can be almost directly read off due to changes in βt closely matching changes in Bob’s payoff. Bob’s total payoff to round t turns out to be of the order t/2± ρt, so bounding the rate at which the spiral expands also bounds Bob’s payoff. We then use the dynamics to bound the total payoff to Alice and Bob. Alice can only cut in the interior of the cake when αt = 0, which happens less and less often as the spiral expands. The players’ total payoff when Alice cuts at one end of the cake is 1, so across T rounds we show the sum of cumulative payoffs of the players is of the order T ±Θ( √ T ). Combining the bound on the total payoff with the bound on Bob’s payoff gives a bound for Alice’s payoff.