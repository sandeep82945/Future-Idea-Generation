Fraudulent or illegal activities are ubiquitous across many applications and involve users bypassing the rule of law, often with the strategic aim of obtaining some benefit that would otherwise be unattainable within the bounds of lawful conduct. However, user fraud is detrimental, as it may compromise safety or impose disproportionate negative externalities on particular population groups. To mitigate the potential harms of users engaging in fraudulent activities, we study the problem of policing such fraud as a security game between an administrator and users. In this game, an administrator deploys R security resources (e.g., police officers) across L locations and levies fines against users engaging in fraudulent or illegal activities at those locations. For this security game, we study both welfare and revenue maximization administrator objectives. In both settings, we show that the problem of computing the optimal administrator strategy is NP-hard and develop natural greedy algorithm variants for the respective settings that achieve at least half the welfare or revenue as the welfare-maximizing or revenue-maximizing solutions, respectively. We also establish a resource augmentation guarantee that our proposed greedy algorithms with just one additional resource, i.e., R + 1 resources, achieve at least the same welfare (revenue) as the welfare-maximizing (revenue-maximizing) outcome with R resources that is NP-hard to compute. Finally, since the welfare and revenue-maximizing solutions can differ significantly, we present a framework inspired by contract theory, wherein a revenue-maximizing administrator is compensated through contracts for the level of welfare it contributes to the system. Beyond extending our theoretical results in the welfare and revenue maximization settings to studying equilibrium strategies in the contract game, we also present numerical experiments highlighting the effectiveness of using contracts in bridging the gap between the revenue and welfare-maximizing administrator outcomes. 1 introduction Fraudulent or illegal activities that involve users bypassing the rule of law are ubiquitous across applications and arise as users seek to strategically obtain some benefit that would otherwise not be attainable within the bounds of lawful conduct. For instance, in transportation networks, users often drive above the speed limit to reduce their travel times, and, in school choice contexts, parents often misreport their home addresses to admit their children to better public schools [1]. Similar issues of users engaging in fraud arise in other domains, including labor markets [2], strategic classification [3, 4], non-market allocation mechanisms [5], and resource allocation [6]. However, fraudulent or illegal activities can be detrimental, as even a few users engaging in such activities can compromise safety, result in disproportionate negative externalities to particular groups of the population, or hamper efficiency. In transportation networks, users driving above the speed limit can compromise road safety, and, in school choice settings, sophisticated gaming by some parents, who typically belong to higherincome strata, often adversely affects users not engaging in such practices [1]. Moreover, in healthcare contexts, the manipulation of patient’s priority by transplant providers in organ transplant waiting lists often results in significant reductions in organ donations [7, 8]. We present further detailed examples of users engaging in fraudulent activities and the associated harms to other groups of the population in Section 1.1. The prevalence of fraud and its associated harms across applications poses critical security challenges and requires policing to deter users from engaging in such activities. Central to the problem of policing
ar X
iv :2
40 2. 11 20
9v 2
[ cs
.G T
] 2
1 Fe
b 20
fraudulent or illegal activities is a resource allocation task, which involves allocating a limited set of security resources (e.g., police officers) to mitigate the level of fraud in the system. However, the challenge in policing fraudulent activities is that only limited security resources are available, i.e., providing complete coverage to prevent fraud is not possible. This resource limitation raises a fundamental question of how to best allocate the available resources to protect against fraudulent or illegal activities at various susceptible nodes or locations in a system. To answer this question, we study the problem of policing fraudulent activities as a security game between an administrator and fraudulent users at different nodes or locations in a system. In our security game, the administrator can allocate a budget of security resources across various locations and levy fines against users engaging in fraud. For our studied security game, we show that simple algorithms with a low computational overhead can achieve a good performance relative to the administrator’s optimal strategies, which are NP-hard to compute. To analyze the associated equilibria and the corresponding optimal administrator strategies, we leverage and combine techniques from optimization theory, linear programming, and approximation algorithms. Our modeling assumptions and, in particular, considering fines as a mechanism to deter fraudulent users are motivated by several real-world applications, e.g., users typically pay fines for road traffic or speeding violations. Moreover, in line with prior work on security games [9], we adopt a game-theoretic framework to study the problem of deploying security resources to mitigate fraudulent and illegal activities. We do so as a game-theoretic framework enables us to incorporate the preferences of both the administrator and users and predictions for how a fraudulent user will respond to a resource allocation policy and the corresponding fines set by the administrator. 1.1 examples This section presents detailed examples of real-world settings where users engage in fraudulent or illegal activities. Our examples help further contextualize our studied problem setting and provide a grounding for the model we develop in Section 3. While we present two examples, our developed framework more generally applies to a broad range of settings where users engage in fraud. Intermediate Public Transport (IPT): IPT comprises informal modes of transport that are pivotal in serving the last-mile connectivity needs in many developing nation cities, particularly when the formal public transportation system has inadequate capacity to serve the travel demands of users [10, 11]. IPT services typically entail mini-buses [12], share-taxis [13, 14], or auto-rickshaws [15]. While IPT services offer affordable on-demand last-mile connectivity, commuters often face long wait times during rush hours [16], which frequently results in disorderly queues and, in particular, illicit behavior by some commuters who jump the queue to reduce their wait times [17, 10]. For instance, some male passengers who have recently arrived to avail of an IPT service, e.g., at a mini-bus stand, might bypass the queue and board a moving vehicle as it arrives to pick up passengers. Such alighting of moving vehicles is more challenging for women or less-abled travelers, who typically bear the burden of disproportionately high wait times when using IPT systems. Healthcare: In healthcare, medication over-prescription is a global problem and is particularly prevalent in places where a large portion of a healthcare provider’s income relies on drug sales [18]. These perverse financial incentives for health care providers not only fleece patients as they purchase more medications but also could be potentially detrimental to their health. Analogous to the over-prescription of medications, hospitals often keep patients longer than is required, which enables the hospital to earn more money from the patient occupying a bed [19]. Both of these examples highlight the need for policing to deter defaulting users from engaging in such fraudulent activities at susceptible nodes or locations in a system, e.g., hospitals in a healthcare context or mini-bus stands where users queue to avail a mini-bus in a transportation context. 1.2 our contributions Motivated by the prevalence and detrimental effects of fraudulent and illegal activities across applications, we study a security game between an administrator and users engaging in fraud at susceptible nodes or locations in a system. In this security game, the administrator has R security resources that it can allocate across these locations and levy fines against defaulting users to deter them from engaging in fraud. For
this security game, we study the resulting equilibria and the optimal administrator strategies under both the welfare maximization and revenue maximization objectives, which we introduce in Section 3, under two informational assumptions: (i) deterministic, where the administrator knows each location’s type, i.e., the administrator has complete knowledge of the total number of fraudulent users, the benefit that fraudulent users derive when engaging in fraud, and the value of preventing users from engaging in fraud at each location, and (ii) probabilistic, where the administrator only has distributional information on each location’s type. We first study the deterministic setting in Section 4, where we develop a natural greedy procedure to compute the administrator’s revenue-maximizing strategy and show that the problem of computing the administrator’s welfare-maximizing strategy is NP-hard. To that end, in the welfare-maximization setting, we develop a variant of a greedy algorithm, different from that in the revenue-maximization setting, that achieves at least half the welfare compared to that of the welfare-maximizing solution. Moreover, we establish a resource augmentation guarantee that our proposed greedy algorithm with just one additional resource, i.e., R+1 resources, achieves at least the same welfare as the welfare-maximizing solution with R resources that is NP-hard to compute. To establish the hardness, approximation, and resource augmentation guarantees, we develop and leverage properties of the optimal solution to the administrator’s welfare-maximization problem and geometric insights based on the structure of the administrator’s welfare function. Next, in Section 5, we study the probabilistic setting, where we show that, unlike in the deterministic case, computing the expected revenue maximizing strategy of the administrator is NP-hard. Yet, we develop variants of greedy algorithms that achieve similar approximation and resource augmentation guarantees to those in the deterministic setting for the expected revenue and welfare maximization administrator objectives. The crux of establishing our algorithmic guarantees involves constructing a monotone concave upper approximation (MCUA) of the expected revenue and welfare functions of the administrator (see Section 5.3 for more details on the MCUA) and analyzing the optimal solutions of the linear programs that optimize the corresponding MCUAs. Our results shed light on the benefits of using simple algorithms, e.g., variants of the greedy algorithm, and highlight the value of recruiting one additional security resource rather than expending computational effort in solving for the optimal administrator strategies that are NP-hard to compute. Finally, since the revenue and welfare maximizing outcomes can differ significantly and, in particular, a revenue-maximization administrator objective can substantially hamper welfare, in Section 6, we extend our security game framework to incorporate contracts. In this contract game, a welfare-maximizing principal offers contracts to a revenue-maximizing administrator to compensate it for the welfare it contributes to the system. For this contract game, we show that our developed algorithmic approaches and theoretical guarantees for the earlier studied welfare and revenue maximization administrator objectives naturally carry forward in studying optimal administrator strategies. Moreover, we introduce a dense-sampling approach to compute a near-optimal solution to the principal’s problem of selecting an optimal contract to maximize its payoff. Finally, we present numerical experiments based on a case study of queue jumping in IPT services, which highlight the effectiveness of contracts in bridging the gap between welfare and revenue-maximizing outcomes. In the appendix, we provide proofs and extensions of theoretical results omitted from the main text, present numerical implementation details along with additional numerical results, and highlight extensions of the model presented in this work, which opens directions for future work. 2 related literature Game theory has served as a foundational paradigm in studying multi-agent systems wherein agents pursue their selfish interests [20]. Among the many successful applications of game theory, it has, in particular, gained traction in security applications, where the problem of protecting essential security resources is formulated as a game, wherein the objective of the security agency is to compute an optimal strategy to deploy its resources to prevent security breaches, given that the adversary will optimize its utility on observing this strategy. Security games have found many applications, including protecting security checkpoints at airports [21], protecting shipping ports [22], fare inspections in transit systems [23], and more recently, in green security contexts [24, 25, 26, 27]. For a more detailed review of the state-of-the-art on security games, see [9, 28]. While our work contributes to the security games literature and, more generally, to the literature on
using game theoretic approaches to mitigate fraud, our modeling framework, game structure, and solution methodology differs from prior works on security games in several ways. First, unlike classical security games, where adversaries typically have an allocation task of determining a utility-maximizing set of locations to target to perform security breaches, in our setting, the role of users (i.e., the adversaries) is to decide whether or not to commit fraud at their respective locations. Next, while we also consider mixed-strategy (or randomized) resource allocation policies of the administrator as in prior work on security games, we depart from much of the previous security games literature as we explicitly model fines that administrators levy on defaulting users. Consequently, given the differences in our modeling assumptions and game structure, rather than developing large-scale mixed-integer linear programs to compute equilibria as in past works on security games, we leverage the structure of the payoffs of the administrator and users induced by the fines in our setting to uncover novel geometric and structural insights and develop algorithms based on linear programming upper bounds with constant factor approximation guarantees. Methodologically, our work aligns with the literature on approximation algorithms for NP-hard problems [29, 30, 31], as we also develop polynomial time algorithms that achieve a constant factor approximation to the optimal solutions that are NP-hard to compute. Moreover, our work contributes to the literature on beyond worst-case algorithm design by developing resource augmentation guarantees [32], wherein an algorithm’s performance is compared to the optimal solution handicapped with fewer resources. In particular, as has been demonstrated in several contexts [33, 34, 35], we obtain Bulow-Klemperer [36] style guarantees for our problem setting as we establish that simple greedy-like algorithms with one additional resource achieve the same or better performance in terms of maximizing revenue or welfare as the optimal solutions (which are NP-hard to compute) with no extra resources. Overall, our obtained guarantees for the developed greedy algorithms contribute to the broader literature on simplicity versus optimality in algorithm design [37, 38]. Our work is also related to contract theory [39, 40], which typically considers a principal-agent problem where a principal delegates a task to an agent who takes a (possibly) costly action, unobservable to the principal, that triggers a distribution over rewards [41, 42, 43, 44, 45, 46]. While we also study an optimal contract problem, unlike classical principal-agent models, we use contracts as a mechanism to bridge the gap between the revenue and welfare maximizing outcomes. 3 model and preliminaries This section presents a model of our security game and introduces the strategies and payoffs of the administrator and users. For additional discussions of some of our modeling assumptions beyond those presented in this section, we refer to Appendix A. 3.1 parameters of security game We consider a security game where an administrator seeks to allocate a budget of R security resources (e.g., police officers) across a set of L locations susceptible to fraud and levies a fine of k against users found engaging in fraud. In this security game, each location l ∈ L corresponds to a type, specified by a triple Θl = (Λl, dl, vl), where Λl > 0 denotes the number of users engaging in fraud, dl > 0 corresponds to the benefit received by users who engage in fraud, and vl > 0 represents the value or welfare to the administrator for allocating a security resource to mitigate fraud at location l. We note that the administrator’s welfare vl can serve as a proxy to capture many possible administrator objectives, e.g., the total level of fraud at a given location that an administrator can mitigate or the extent of the negative externality imposed by fraudulent users on those not engaging in fraud. To make our model more concrete, we now elucidate an example of a location’s type Θl in the context of queue jumping in intermediate public transport services. In this context, Λl corresponds to the number of users engaging in queue jumping at a mini-bus or share-taxi stand corresponding to a location l, dl represents the monetary equivalent of the wait time that users save when engaging in queue jumping, and vl can, for instance, represent the total fraud that the presence of a security resource at location l can prevent, given by vl = Λldl, i.e., the total benefits that fraudulent users accrue, which is equal to the monetary equivalent of the additional wait time faced by non-defaulting users. As another example, a welfare function vl = xΛldl or vl = Λl(dl)
x for some x ≥ 1 can serve as a proxy to capture the fact that an administrator may place a higher value in reducing additional wait time of non-defaulting users. While many other formulations for the administrator’s welfare vl can be used depending on the administrator’s goals in a specific context, for
generality, we subsume these different objectives into the term vl to represent the value that the administrator places on allocating a security resource to mitigate fraud at location l.
Furthermore, for ease of exposition, in this section, we introduce the model in the deterministic setting when the type at each location is known to the administrator, a setting we study in Section 4. We then introduce and investigate the probabilistic setting wherein the administrator only knows the probability distribution from which each location’s type is drawn in Section 5. 3.2 strategies of administrator and users As in prior security games literature [9], we model our problem as a Stackelberg game, where an administrator (the leader) selects a strategy to allocate its available security resources to which the users (the followers) respond by deciding whether to engage in fraud. Here, we present the strategies of the administrator and users in our studied security game. Administrator Strategy: We denote σ = (σl)l∈L as the resource allocation (mixed)-strategy of the administrator, where σl ∈ [0, 1] denotes the probability with which a security resource is allocated to location l. The mixed strategy of the administrator satisfies the administrator’s resource budget, i.e.,∑
l∈L σl ≤ R. For brevity of notation, we define the feasible set of the administrator’s mixed-strategy vector as ΩR = {σ = (σl)l∈L : σl ∈ [0, 1] for all l ∈ L and ∑ l∈L σl ≤ R}, where the subscript R represents the number of security resources available to the administrator. User Strategy: In response to the administrator’s strategy σ, users at each location l decide whether to engage in fraud at that location. To model the strategy of users, we let yl(σ) ∈ [0, 1] denote the probability with which users at location l will engage in fraud, where the outcome yl(σ) = 1 (yl(σ) = 0) corresponds to a setting where users at location l do (do not) engage in fraud. 3.3 user and administrator objectives In this work, we assume users to be utility maximizers, as is standard in the security games literature [23], and study the administrator’s resource allocation strategies under two objectives: (i) revenue maximization and (ii) welfare maximization. A revenue maximization objective aligns with the model of a selfish administrator, which is a standard assumption in prior work on security games [23] and closely resembles practice. For instance, in road traffic scenarios, police often place speed traps where users are likely to violate the speed limit even though other locations may be more accident-prone [47]. Moreover, a welfare maximization objective is a natural choice for an administrator seeking to, for instance, ensure it targets the locations most impacted by fraud. We first elucidate the utility maximization problem of users, who at each location l are assumed to best respond to the administrator’s strategy σ. In particular, if the administrator allocates security resources to location l with probability σl, users at location l choose whether to engage in fraud based on whether the gains from committing fraud, given by (1−σl)dl, outweigh the risk of potential losses through fines, given by σlk. Then, the utility maximization problem of the users, given an administrator strategy σ, at a location l is given by:
max yl∈[0,1]
Ul(σ, yl) = yl[(1− σl)dl − σlk]. (1)
The above problem represents the additional gains to users at location l when engaging in fraud with probability yl, where, without loss of generality, we normalize the utility of not engaging in fraud (which happens with probability 1− yl) to zero. Next, we elucidate the administrator’s welfare and revenue maximization problems. Revenue Maximization: To elucidate the administrator’s revenue maximization problem, note that the revenue accrued under an administrator strategy σ at each location l is given by σlyl(σ)kΛl, where yl(σ) represents the best-response of users at location l, given by the solution of Problem (1). In other words, if yl(σ) = 0, i.e., users do not engage in fraud, the administrator collects no revenue from location l, while if yl(σ) > 0, then the administrator levies a fine k on the Λl fraudulent users at location l, resulting in a revenue of kΛl from those users. Then, the administrator’s revenue maximization problem is given by the following bi-level program:
max σ∈ΩR
yl(σ)∈[0,1],∀l∈L
QR(σ) = ∑ l∈L σlyl(σ)kΛl, (2a)
s.t. yl(σ) ∈ argmax y∈[0,1] Ul(σ, y), for all l ∈ L, (2b)
where in the upper level problem the administrator deploys a strategy σ to maximize its revenue to which users best-respond by maximizing their utilities in the lower-level problem. Here, we express the administrator revenue QR(σ) only as a function of σ and not the user strategy vector y = (yl)l∈L for notational simplicity, as the best-response function yl(σ) of users at each location l can itself be expressed as a function of the administrator’s strategy σ, as given by Equation (2b). In the remainder of this work, we will clarify the formulation of yl(σ) based on context. Welfare Maximization: To elucidate the administrator’s welfare maximization problem, we first note that the welfare under an administrator strategy σ at each location l is given by σlvl + (1 − σl)(1 − yl(σ))vl = vl− (1−σl)yl(σ). In other words, the administrator accrues a value vl if it allocates a resource to location l, as the presence of a security resource can prevent fraud at that location, but only accrues vl with probability 1 − yl(σ), i.e., the probability that users at location l do not engage in fraud given strategy σ, if it does not allocate a resource to location l. Then, defining the level of welfare not accrued by the administrator under a strategy σ with R resources as W−R (σ) = ∑ l∈L(1− σl)yl(σ)vl, the administrator’s welfare is given
by WR(σ) = ∑ l∈L vl −W − R (σ). Finally, since the term ∑ l∈L vl is a constant independent of σ, the welfaremaximizing strategy is equivalent to one that minimizes W−R (σ) and can be computed using the following bi-level program:
min σ∈ΩR
yl(σ)∈[0,1],∀l∈L
W−R (σ) = ∑ l∈L (1− σl)yl(σ)vl, (3a)
s.t. yl(σ) ∈ argmax y∈[0,1] Ul(σ, y), for all l ∈ L, (3b)
where the administrator employs a strategy that minimizes the objective W−R (σ) in the upper level problem to which users best-respond by maximizing their utilities in the lower-level problem. Having defined the administrator’s and users’ objectives, our goal is to find a tuple of strategies (σ∗, (yl(σ ∗))l∈L)
that solve the above-defined bi-level programs. Note that such a tuple of strategies constitutes an equilibrium of our security game corresponding to the respective administrator objectives. Moreover, we note that as the structure of the equilibrium best-response function yl(σ) takes on a simple form (see Section 4), analyzing equilibria in our security game reduces to studying the optimal administrator strategies, which will be the main focus for the remainder of this work. 4 revenue and welfare maximization in deterministic setting We begin with the study of our security game and the corresponding resource allocation strategies of the administrator under both revenue (Section 4.1) and welfare (Section 4.2) maximization objectives in the deterministic setting when the administrator knows each location’s type. 4.1 revenue maximization This section studies the administrator’s revenue maximization problem and analyzes a greedy algorithm, shown in Algorithm 1, which allocates resources to locations in the descending order of their Λl values, where the total spending at each location l is no more than a threshold of dldl+k . We show that Algorithm 1 achieves a revenue-maximizing outcome for the administrator. Theorem 1 (Optimality of Greedy Algorithm for Revenue Maximization Setting). Suppose that the administrator knows the type Θl at each location l ∈ L. Then, the allocation strategy corresponding to Algorithm 1 achieves a revenue-maximizing outcome, i.e., it solves Problem (2a)-(2b). Theorem 1’s proof relies on the fact that, in the revenue maximization setting, given an administrator strategy σ, the best-response function yl(σ) of users, given by the solution of Problem (1), at each location
Algorithm 1: Greedy Algorithm for Administrator’s Revenue Maximization Objective
Input : Total Resource capacity R, Locations types Θl = (Λl, dl, vl) for all locations l Order locations in descending order of Λl ; for l = 1, 2, ..., |L| do
σl ← min{R, dldl+k } ; Allocate the minimum of the remaining resources and dl dl+k to location l ;
R← R− σl; Update amount of remaining resources ; end
l is given by a threshold policy:
yl(σ) =
{ 0, if σl >
dl dl+k ,
1, otherwise. (4)
Notice that when σl = dl dl+k , any yl(σ) ∈ [0, 1] is a best-response for users at location l, i.e., users at location l are indifferent between engaging and not engaging in fraud. However, at the threshold σl = dl
dl+k ,
the administrator’s revenue is maximized when yl(σ) = 1 with any yl(σ) < 1 resulting in a strictly lower administrator revenue. Thus, in the revenue maximization setting, our security game has an equilibrium and, consequently, yl(σ) corresponds to a solution of the lower-level problem of the bi-level Program (2a)-(2b) if and only if yl(σ) = 1 when σl =
dl dl+k . We also note that selecting yl(σ) = 1 when σl = dl dl+k aligns with the
notion of strong Stackelberg equilibria [48], where the ties of the followers (users) are broken to optimize the leader’s (administrator’s) payoff. Equation (4) implies that if the probability of allocating resources to a location exceeds a threshold, users at that location will stop engaging in fraud as the risk of bearing the fines outweighs the gains from fraud. Given users’ best-response function in Equation (4), the revenue at each location as a function of the amount of resources allocated to that location is depicted in Figure 1 (left). We now use users’ best response function in Equation (4) to complete the proof of Theorem 1. Proof (Sketch) of Theorem 1. Leveraging Equation (4), Theorem 1’s proof relies on establishing that the bilevel Program (2a)-(2b) can be reduced to solving a linear program. The key observation to this reduction relies on the fact that it suffices to consider administrator strategies satisfying σl ∈ [0, dldl+k ] for all locations l, as allocating more than dldl+k resources to any location will result in reduced revenues (see Figure 1), as yl(σ) = 0 when σl > dl
dl+k . We then leverage the structure of the linear program to show that Algorithm 1
that allocates resources to locations in the descending order of the Λl values achieves an optimal solution to this linear program. For a complete proof of Theorem 1, see Appendix B.1. Theorem 1 is in contrast to general hardness results for solving bi-level programs, as it establishes that a simple greedy algorithm (Algorithm 1) computes
the administrator’s revenue-maximizing strategy, i.e., solves Problem (2a)-(2b), in polynomial time. In particular, Algorithm 1’s complexity is O(|L| log(|L|)), since the complexity of sorting locations is O(|L| log(|L|)) while that of iterating through the locations in linear in |L|. 4.2 welfare maximization This section studies the administrator’s welfare maximization problem in the deterministic setting when the administrator knows each location’s type Θl. To this end, we first present several properties of the administrator’s welfare-maximizing strategy in Section 4.2.1. Then, we establish that computing the welfare-maximizing strategy is NP-hard (Section 4.2.2) and present a greedy algorithm with its associated approximation and resource augmentation guarantees (Section 4.2.3). 4.2.1 properties of welfare maximizing strategy In this section, we present properties of the structure of the optimal solution of the administrator’s welfaremaximization problem, which play a pivotal role in establishing the hardness of solving Problem (3a)-(3b) (see Section 4.2.2) and analyzing the performance of the greedy algorithm to approximate the administrator’s welfare-maximizing strategy (see Section 4.2.3). To elucidate these properties, we first note in the welfaremaximization setting that the best-response function of users, given by the solution of Problem (1), at each location l given an administrator strategy σ is given by the following threshold policy:
yl(σ) = { 0, if σl ≥ dldl+k , 1, otherwise. (5)
As in the revenue maximization setting, we recall that when σl = dl dl+k , any yl(σ) ∈ [0, 1] is a best-response for users at location l. However, at the threshold σl = dl
dl+k , the administrator’s welfare is maximized
and equal to vl when yl(σ) = 0 with any yl(σ) > 0 resulting in a strictly lower administrator welfare of vl − (1 − σl)yl(σ)vl at that location. Thus, in the welfare maximization setting, our security game has an equilibrium and, consequently, yl(σ) corresponds to a solution of the lower-level problem of the bi-level Program (3a)-(3b) if and only if yl(σ) = 0 when σl =
dl dl+k . We note here that unlike the best response of
users in the revenue-maximization setting, where yl(σ) = 1 when σl = dl
dl+k , in the welfare maximization
setting, we take yl(σ) = 0 when σl = dl
dl+k . Such a difference in the outcomes is attributable to the nature
of the revenue and welfare functions at each location l, as depicted in Figure 1, where the welfare increases to vl once the probability of allocating resources to location l exceeds the threshold
dl dl+k
while the revenue drops to zero. Given the best-response function of users at each location l, we now characterize several properties of the administrator’s welfare-maximizing strategy. Proposition 1 (Properties of Welfare-Maximizing Strategy). Suppose that users at each location bestrespond using Equation (5). Then, there exists a solution σ̃∗ of Problem (3a)-(3b) satisfying:
1. σ̃∗l ∈ [ 0, dldl+k ] for all locations l ∈ L.
2. There exists a set L1 with σ̃ ∗ l = dl dl+k for all l ∈ L1, a set L2 with σ̃∗l = 0 for all l ∈ L2, and at most one location l′ with σ̃∗l′ ∈ ( 0, dl′dl′+k ) , where L1, L2, and {l′} are disjoint and L1 ∪ L2 ∪ {l′} = L.
Moreover, the administrator’s optimal welfare under the strategy σ̃∗ is: WR(σ̃ ∗) = ∑ l∈L1 vl + σ̃ ∗ l′vl′ . The proof of the first claim in the proposition statement follows as spending more than dldl+k at any location does not increase the welfare at that location (see right of Figure 1). The proof of the second claim follows from the linearity of the welfare function in the region [ 0, dldl+k ) for all locations l. Together, both these claims in Proposition 1 establish that, without loss of generality, for the administrator’s welfare maximization problem, it suffices to restrict attention to administrator strategies where the total allocation of security resources σl at any location l does not exceed
dl dl+k and where there is at most one location l′ such that σl′ ∈ ( 0, dl′dl′+k ) . Consequently, the optimal welfare of the administrator takes a relatively simple form, as in the statement of the proposition. We refer to Appendix B.2 for a complete proof of Proposition 1. 4.2.2 np-hardness of welfare maximization This section establishes that the problem of computing the administrator’s welfare-maximizing strategy is NP-hard. Theorem 2 (NP-Hardness of Welfare Maximization). The problem of computing the administrator’s welfaremaximizing strategy, i.e., solving Problem (3a)-(3b), is NP-hard. Proof (Sketch) of Theorem 2. We prove this result through a reduction from an instance of the partition problem, which consists of a sequence of numbers a1, . . . , an with ∑ l∈[n] al = A and involves the task of
deciding whether there is some subset S1 of numbers such that ∑
l∈S1 al = A 2 . We now construct an instance of the welfare maximization problem (WMP) with n+ 1 locations, where the first n locations correspond to each number of the partition instance, where we define vl = al and let dl
dl+k = alA for all locations l ∈ [n]. We also consider a location n + 1 with vn+1 = maxl∈[n] vl + ϵ and
dn+1 dn+1+k = 12 + δ, where ϵ, δ > 0 are small constants. Finally, we let the number of resources R = 1 2 . We then show that a sequence of numbers correspond to a “Yes” instance of partition if and only if the optimal total welfare of the above defined WMP instance is at least A2 . To prove the forward direction of this claim, for any “Yes” instance of partition with a set S such that∑ l∈S al = A 2 , we construct an allocation strategy σ such that σl = al A for all l ∈ S and σl = 0 for all l ∈ [n+ 1]\S. We then verify that σ is feasible and achieves WR(σ) ≥ A2 . To establish the reverse direction, we leverage Proposition 1 to show that the only way for both the upper bound resource constraint of A2 and the lower bound welfare constraint of A 2 to be satisfied is if the location l′ defined in the statement of Proposition 1 is such that l′ = ∅. For a complete proof of Theorem 2, see Appendix B.3. Theorem 2 establishes that Problem (3a)-(3b) cannot be solved in polynomial time unless P = NP . This result contrasts the polynomial time algorithm we developed in the revenue-maximization setting (see Theorem 1). We note that the NP-hardness of the welfare maximization setting stems from the fact that, unlike the revenue function, the administrator’s welfare function is discontinuous at σl =
dl dl+k (see Figure 1). 4.2.3 greedy algorithm for welfare maximization Given the impossibility of developing a polynomial time algorithm for the administrator’s welfare-maximization problem unless P = NP (see Theorem 2), this section presents a computationally efficient algorithm to compute an administrator strategy with strong approximation guarantees to the optimal solution of Problem (3a)-(3b). In particular, we develop a variant of a greedy algorithm, described in Algorithm 2, to compute an allocation of resources that achieves at least half the welfare as the welfare-maximizing allocation. Moreover, we show that running the greedy algorithm with one additional resource, i.e., R + 1 resources, results in an outcome with at least the same welfare as the welfare maximizing outcome. We begin by first presenting our algorithmic approach, which proceeds as follows. First, we order the
locations in descending order of their bang-per-buck ratios, given by vldl dl+k = vl(dl+k)dl , and find the solution σ̃ corresponding to the greedy algorithm that allocates at most dldl+k resources to each location l in descending order of their bang-per-buck ratios. We define the quantity vldl
dl+k
as the bang-per-buck ratio as the total
welfare received on allocating a fraction dldl+k of resources to location l is given by vl (see right of Figure 1). Next, we compute an allocation σ′ corresponding to spending all the available resources at a single location that yields the highest welfare for the administrator. Finally, we return the resource allocation strategy between the two computed strategies σ̃ and σ′ that achieves a higher administrator welfare. This procedure is formally presented in Algorithm 2. A few comments about Algorithm 2 are in order. First, as with Algorithm 1, Algorithm 2 is computationally efficient with a running time of O(|L| log(|L|)), which corresponds to the complexity of sorting the locations in descending order of vl(dl+k)dl , and the remaining steps of Algorithm 2 can be performed in linear time in the number of locations. Next, Algorithm 2, which orders locations based on their bang-per-buck ratios, resembles analogous algorithms from the literature on the knapsack problem, which is the problem of finding the value-maximizing subset of items of given sizes that fits within the capacity of a knapsack. Algorithm 2: Greedy Algorithm for Administrator’s Welfare Maximization Objective
Input : Total Resource capacity R, Location Types Θl = (Λl, dl, vl) for all locations l Step 1: Find Greedy Solution σ̃: Order locations in descending order of vl(dl+k)
dl ;
for l = 1, 2, ..., |L| do σ̃l ← min{R, dldl+k } ; Allocate the minimum of the remaining resources and dl dl+k to location l ;
R← R− σ̃l; Update amount of remaining resources ; end Step 2: Find Solution σ′ that Maximizes Welfare from Spending on Single Location σl ← argmaxσ∈ΩR:σl′=0,∀l′ ̸=l WR(σ), ∀l ; Compute allocation σ l maximizing welfare from only spending on l ; σ′ ← argmaxl∈L WR(σl) ; Step 3: Return Solution with Higher Welfare: σ∗A ← argmax{WR(σ̃),WR(σ ′)} ;
Despite the connection between Algorithm 2 and the knapsack literature, our problem setting differs from several well-studied variants of the knapsack problem. Unlike the 0-1 knapsack problem, wherein the decision space is binary, in our problem setting, the administrator’s decision space of mixed strategies is continuous. Moreover, unlike the fractional knapsack setting, which has a continuous decision space and where a greedy algorithm that allocates resources in descending order of the bang-per-buck ratios is optimal, in our setting, computing the administrator’s welfare maximizing strategy is NP-hard (Theorem 2). Consequently, given the similarities and differences between the welfare maximization setting and the knapsack literature, we can interpret our studied welfare maximization problem as a novel variant of the knapsack problem. In particular, we can consider locations as items, whose value (or welfare) function increases linearly as a higher fraction of it is packed in the knapsack (i.e., as the probability of allocating resources to a location is increased) up to some threshold. At this location-specific threshold, dldl+k , which we interpret as the size of the item, the value function of the item has a jump discontinuity and equals the item’s value vl, as depicted on the right of Figure 1. Note in the case when the fine k = 0, our studied welfare function has no jump discontinuity at dldl+k (see Figure 1); thus, when k = 0, our welfare maximization problem reduces to a fractional knapsack problem that is solvable in polynomial time. Hence, the presence of fines in our security game introduces discontinuities in the administrator’s welfare function, which corresponds to the source of the NP-hardness of the welfare maximization Problem (3a)-(3b) (Theorem 2). We now present the approximation guarantees of Algorithm 2 to the optimal welfare corresponding to the solution of Problem (3a)-(3b). Our first result establishes that Algorithm 2 achieves at least half the welfare as that corresponding to the welfare-maximizing allocation. Theorem 3 (1/2 Approximation of Greedy Algorithm for Welfare Maximization). Denote σ∗A as the solution corresponding to Algorithm 2 and let σ∗ be the welfare-maximizing allocation that solves Problem (3a)-(3b). Then, σ∗A achieves at least half the welfare as σ ∗, i.e., WR(σ ∗ A) ≥ 12WR(σ ∗). For a detailed proof sketch and proof of Theorem 3, see Appendix B.4. The key insight in developing Algorithm 2 and establishing Theorem 3 despite the discontinuity in the welfare function (see Figure 1) is in recognizing that the administrator’s welfare-maximization objective can be upper bounded by the objective of a linear program that resembles a fractional knapsack optimization. Further, we note that the obtained approximation ratio of 12 of Algorithm 2 aligns with the approximation guarantee of an analogous greedy algorithm for the NP-hard 0-1 knapsack problem. Yet, unlike the 0-1 knapsack problem for which there exists a polynomial-time approximation scheme (PTAS) using dynamic programming, extending this idea to the setting considered in this work is challenging due to the administrator’s continuous action space. We defer settling the question of whether a PTAS exists for the welfare maximization problem as a direction for future research. Next, we show that if the administrator had an extra resource, i.e., R + 1 resources, then Algorithm 2 achieves a higher welfare than that of the welfare-maximizing outcome with R resources. Theorem 4 (Resource Augmentation Guarantee for Welfare Maximization). Denote σ∗A as the solution corresponding to Algorithm 2 with R + 1 resources and let σ∗ be the welfare-maximizing allocation that solves Problem (3a)-(3b) with R resources. Then, the total welfare under the allocation σ∗A is at least that corresponding to the allocation σ∗, i.e., WR+1(σ ∗ A) ≥ WR(σ∗). The proof of this result relies on much of the machinery developed in proving Theorem 3, and we present its proof in Appendix B.5. We note that the administrator only requires R + maxl∈L
dl dl+k resources to
obtain the guarantee in Theorem 4; however, we present the result with R+1 resources for ease of exposition. Theorem 4 highlights the benefit to administrators for recruiting one additional security resource (e.g., police officer) and applying a simple algorithm, i.e., Algorithm 2, rather than investing computational effort to solve the NP-hard welfare-maximization problem. 5 revenue maximization in probabilistic setting In this section, we study the administrator’s revenue maximization objective in the probabilistic setting when the administrator only has distributional information on each location’s type. To this end, we first introduce the notation for the probabilistic setting and the administrator’s expected revenue maximization problem in Section 5.1. We then show that computing the administrator’s expected revenue maximizing strategy is NP-hard in Section 5.2 and present a variant of a greedy algorithm and its associated approximation ratio and resource augmentation guarantees in Section 5.3. While we focus on the administrator’s revenue maximization objective in this section, our developed algorithmic ideas and corresponding guarantees naturally generalize to the administrator’s welfare maximization objective in the probabilistic setting, which we elucidate in Appendix C. 5.1 model for probabilistic setting and expected revenue maximization objective To model the probabilistic setting, we assume that the type Θl at each location l is drawn independently from some discrete probability distribution with finite support (Θil)i∈I = (Λ i l, d i l, v i l)i∈I , where the support, defined by I, satisfies |I| ∈ N. We define the probability that a location has a type Θil as qil for all i ∈ I. In other words, P(Θl = Θil) = qil for all i ∈ I, where qil ≥ 0 and ∑ i∈I q i l = 1 for all locations l ∈ L. We also assume, without loss of generality, that each location’s types are ordered such that d1l
d1l +k ≤ d
2 l
d2l +k ≤ . . . ≤ d
|I| l
d |I| l +k
. In this setting, in line with Bayesian Stackelberg games [21], we assume that while users at each location know the realization of the type i ∈ I at that location, the administrator only knows the distribution of each location’s type. Consequently, denoting the best response of users corresponding to type i at location l as yil(σ), given an administrator strategy σ, we formulate the following expected revenue maximization problem (ERMP) of the administrator:
max σ∈ΩR
yil (σ)∈[0,1],∀l∈L,i∈I
QR(σ) = ∑ i∈I qil ∑ l∈L σly i l(σ)kΛ i l, (6a)
s.t. yil(σ) ∈ argmax y∈[0,1] U il (σ, y) = y[(1− σl)dil − σlk], for all l ∈ L, i ∈ I, (6b)
where in upper-level problem, the administrator uses a strategy σ that maximizes its expected revenue to which the users at each location best respond by maximizing their utilities in the lower-level problem based on the realized type at that location. As in the deterministic setting, we note that the best response of users is given by the threshold function
yil(σ) =
{ 0, if σl >
dil dil+k ,
1, otherwise. (7)
We reiterate, as with the best response of users in the deterministic setting in Equation (4), that when σl = dil
dil+k , any yil(σ) ∈ [0, 1] is a best-response for users at a location l with type i. Yet, we let yil(σ) = 1 when
σl = dil
dil+k as it corresponds to the highest expected revenue outcome for the administrator (see Section 4.1
for a further discussion). Finally, due to the best-response function of users given by Equation (7), the resulting expected revenue at each location l as a function of the amount of resources allocated is both non-monotone and discontinuous at the resource amounts dil
dil+k for all i, as is depicted in Figure 2 (left). 5.2 np-hardness of probabilistic setting In the probabilistic setting, when the administrator only knows the distribution of location types, we show that computing the administrator’s expected revenue maximizing strategy is NP-hard. Theorem 5 (NP-Hardness of Expected Revenue Maximization). The problem of computing the administrator’s expected revenue maximizing strategy, i.e., solving Problem (6a)-(6b), is NP-hard. Proof (Sketch). We prove this result through a reduction from partition. In particular, given a partition instance with a sequence of numbers a1, . . . , an, we now construct an ERMP instance with two types, i.e., where |I| = 2, and n locations, where each number al corresponds to a location. In this setting, for ease of exposition, we drop the fine k from Objective (6a) as it is a uniform constant that applies to all locations l and types i. Then, we define: (i) d1l
d1l +k = 12 al A , (ii) d2l d2l +k = alA , (iii) q 1 l Λ 1 l = A
( 2 maxl′∈[n] al′ al − 1 ) , and (iv)
q2l Λ 2 l = A
( 1 + 2maxl′∈[n] al′
al
) for all l ∈ [n]. Moreover, we let the number of resources R = 34 . Then, we
claim that we have a “Yes” instance of partition if and only if the optimal expected revenue for this ERMP instance is at least A2 + 2n×maxl∈[n] al. To prove the forward direction of this claim, for any “Yes” instance of partition with a set S such that∑ l∈S al = A 2 , we construct a resource allocation strategy σ such that σl = al A for all l ∈ S and σl = 1 2 al A for all l ∈ [n]\S. We then verify that σ is feasible and achieves QR(σ) ≥ A2 + 2n×maxl∈[n] al. To prove the reverse direction, we first show that when R = 34 there exists an optimal administrator strategy σ̃∗ that satisfies σ̃∗l ≥ d1l
d1l +k for all locations l. Leveraging this property and another structural
property of the expected revenue-maximizing strategy of the administrator (see Appendix B.6) akin to that established in Proposition 1, we show that the administrator’s optimal strategy can only satisfy the resource constraint and achieve an expected revenue that is at least A2 +2n×maxl∈[n] al, if there is some set S
′ ⊆ [n], satisfying ∑ l∈S′ al = A 2 . For a complete proof of Theorem 5, see Appendix B.6. Theorem 5 establishes that Problem (6a)-(6b) cannot be solved in polynomial time unless P = NP . This result contrasts the polynomial time algorithm we developed in the deterministic revenue maximization setting (see Theorem 1), as, while, for any location l, the administrator’s revenue function in the deterministic setting is continuous and monotone in the range
σl ∈ [ 0, dldl+k ] , the expected revenue function is discontinuous and non-monotone in the range σl ∈ [ 0, d |I| l
d |I| l +k ] (see Figure 2), even when the number of types |I| = 2. We note that the fundamental challenge in developing both our hardness results, i.e., for deterministic welfare maximization (Theorem 2) and expected revenue maximization (Theorem 5), lies in constructing the right instances or gadgets to achieve our desired reductions, which we develop leveraging the structural properties of the respective problem settings. For example, in the welfare maximization setting, our reduction crucially relies on Proposition 1, and, hence, the discontinuity of the welfare function at σl =
dl dl+k
. Analogously, in the expected revenue maximization setting, our reduction leverages the non-monotonicity of the expected revenue function. 5.3 greedy algorithm for expected revenue maximization Given the impossibility of developing a polynomial time algorithm for the administrator’s expected revenue maximization problem unless P = NP (see Theorem 5), this section develops a computationally efficient algorithm to compute an administrator strategy and presents its associated approximation ratio and resource augmentation guarantees, akin to that for the welfare maximization problem in the deterministic setting (see Section 4.2.3), to the solution of Problem (6a)-(6b). To motivate our algorithmic approach, we first note that the difficulty in solving Problem (6a)-(6b) is attributable to the non-monotonicity and discontinuity of the expected revenue function at the resource amounts dil
dil+k for all i due to the best-response Problem (7) of users, an example of which is depicted in
Figure 2 for a setting with five types (i.e., |I| = 5). Given this difficulty of directly optimizing the expected revenue function, we define its upper bound, which we term the monotone concave upper approximation (MCUA), that is tractable to compute. To develop this approximation, we first define a piece-wise linear
upper bound of the expected revenue function that connects the origin (0, 0) to its points of discontinuity, as depicted by the green curve in Figure 2 (center). While this upper bound is continuous, it may be non-monotone and non-concave. Thus, we finally construct the MCUA of the expected revenue function by concavifying this upper bound and only retaining the portions of this concave upper bound where the expected revenue is (strictly) increasing in the resources allocated, as depicted by the orange line in Figure 2 (right). Since the MCUA of the expected revenue function for each location l is piece-wise linear (e.g., see right of Figure 2), we characterize it via a set S corresponding to the set of all piece-wise linear segments of this MCUA across all locations. We associate each segment s ∈ S with a triple (ls, cs, xs), where ls represents the location corresponding to segment s, cs corresponds to its slope, and xs represents its horizontal width, i.e., resource requirement, as depicted on the right in Figure 2. Having introduced the notion of the MCUA of the expected revenue function, we now elucidate our algorithmic approach, presented in Algorithm 3, which involves two key steps. First, rather than directly optimizing the administrator’s expected revenue, an NP-hard problem (see Theorem 5), we optimize its corresponding MCUA using a greedy-like procedure. In particular, we order the segments of the MCUA of the expected revenue function in the set S in the descending order of the slopes cs and allocate at most xs to each segment in the descending order of the slopes of the MCUA of the expected revenue function. Our greedy procedure results in an allocation σ̃ and terminates when a segment’s resource requirement xs exceeds the available resources. 1 Next, we compute an allocation σ′ corresponding to spending all the available resources at a single location that yields the highest expected revenue for the administrator. Finally, we return the resource allocation strategy between the two computed strategies σ̃ and σ′ that achieves a higher expected revenue. While we focus on discrete distributions, we note that Algorithm 3 along with the approach of generating a piece-wise linear MCUA of the expected revenue function can also be applied to continuous probability distributions (see Appendix G). We now present the main results of this section, which establish the approximation guarantees of Algorithm 3 to the optimal solution of Problem (6a)-(6b). Our first result establishes that Algorithm 3 achieves at least half the expected revenue as that corresponding to the solution of Problem (6a)-(6b). Theorem 6 (1/2 Approximation for Expected Revenue Maximization). Denote σ∗A as the allocation corresponding to Algorithm 3 and let σ∗ be the solution of Problem (6a)-(6b) with R ≥ 1 resources. Then, σ∗A achieves at least half the expected revenue as σ∗, i.e., QR(σ ∗ A) ≥ 12QR(σ ∗). The crux of establishing Theorem 6 involves showing that optimizing the MCUA of the expected revenue function can be reduced to solving a linear program and that an analogous greedy-like process to that in Step 1 of Algorithm 3 achieves the optimal solution of this linear program. We note that this linear program is different from the fractional knapsack linear program to prove Theorem 3 in the welfare maximization setting, as, unlike the welfare function in the deterministic setting, the MCUA of the expected revenue function at each location is piece-wise linear with (potentially) multiple segments with differing slopes. For a proof of Theorem 6, see Appendix B.7. Next, we show that if the administrator had an extra resource, i.e., R + 1 resources, then Algorithm 3 achieves at least the same revenue as the expected revenue maximizing outcome with R resources. 1We terminate our greedy procedure at the point in the algorithm when a segment’s resource requirement xs exceeds the available resources, as the expected revenue function is non-monotone in the resources allocated, and the MCUA, by construction, is only guaranteed to coincide with the expected revenue function at each location l at the points where the resources allocated equal dil
di l +k
for some values of i (e.g., see Figure 2). Algorithm 3: Greedy Algorithm for Administrator’s Expected Revenue Maximization Objective
Input : Total Resource capacity R, Location Types Θil = (Λ i l , d i l , v i l ) for all locations l and types i Output: Resource Allocation Strategy σ∗A Step 1: Greedy Allocation Based on Slopes of MCUA of Expected Revenue Function: Generate MCUA of the expected revenue function for each location l ; S̃ ← Ordered list of segments s across all locations of this MCUA in descending order of slopes cs ; Initialize allocation strategy σ̃ ← 0 ; for segment s ∈ S̃ do
if xs ≤ R then σ̃ls ← σ̃ls + xs ; Allocate xs to location ls ; R← R− xs; Update amount of remaining resources ; else break ; Only allocate resources if xs ≤ R end
end Step 2: Find Solution σ′ that Maximizes expected revenue from spending on Single Location: σl ← argmaxσ∈ΩR:σl′=0,∀l′ ̸=l QR(σ) for all locations l ; σ′ ← argmaxl∈L QR(σl) ; Step 3: Return Solution with a Higher Expected Revenue: σ∗A ← argmax{QR(σ̃), QR(σ ′)} ;
Theorem 7 (Resource Augmentation Guarantee for Expected Revenue Maximization). Denote σ∗A as the solution corresponding to Algorithm 3 with R + 1 resources and let σ∗ be the expected revenue maximizing allocation that solves Problem (6a)-(6b) with R resources. Then, the total revenue under the allocation σ∗A is at least that corresponding to σ∗, i.e., QR+1(σ ∗ A) ≥ QR(σ∗). The proof of Theorem 7, as with that of Theorem 6, relies on the fact that a greedy-like process akin to step one of Algorithm 3 optimizes the MCUA of the expected revenue function. For a complete proof of Theorem 7, see Appendix B.8. Akin to Theorem 4, we note that the administrator only requires R+maxi∈I maxl∈L dil
dil+k
resources to obtain the guarantee in Theorem 7; however, we present the result with R + 1 resources for ease of exposition. Theorem 7 highlights the benefit to administrators for recruiting one additional security resource (e.g., police officer) and applying a simple algorithm, i.e., Algorithm 3, that relies on computing a tractable MCUA of the expected revenue function rather than investing computational effort to solve the NP-hard Problem (6a)-(6b). 6 contracts to bridge revenue and welfare maximization Thus far, we have studied our security game under the revenue and welfare maximization administrator objectives. While a welfare maximization objective captures an idealized representation of administrator behavior, in practice, an administrator often seeks to maximize revenues, which usually compromises the system’s welfare (see Appendix F for experiments depicting the contrast in the revenue and welfare maximization outcomes). For instance, in road traffic scenarios, police often place speed traps to collect fines and increase revenues at locations where users are likely to violate the speed limit even though other locations may be more accident-prone [47]. To address this concern of the gulf between the revenue and welfare maximization outcomes, in this section, we extend our security game framework to incorporate contracts, wherein a revenue-maximizing administrator is compensated for the welfare it contributes to the system. Section 6.1 introduces the contract framework and studies equilibria in the associated contract game. Then, Section 6.2 presents numerical experiments highlighting the efficacy of contracts in bridging the gap between the welfare and revenue maximization outcomes. 6.1 contract framework We consider a contract game between three players: (i) a welfare-maximizing principal, (ii) a revenuemaximizing administrator, and (iii) fraudulent users at different locations in a system.2 In our contract game, a welfare-maximizing principal offers a contract, specified by a parameter α ∈ [0, 1], to a revenuemaximizing administrator, where the contract level determines the payment made by the principal to the
2To distinguish between a principal and an administrator, we can, for instance, interpret the principal as the government maximizing welfare and the administrator as a police department within the government maximizing its own revenues. administrator as a proportion of the total welfare it contributes to the system. In particular, the principal selects the contract level α ∈ [0, 1] to optimize its payoff, given by the difference between the welfare accrued and the total payments it makes to the administrator, to which the administrator best responds by choosing a revenue-maximizing strategy σ(α), which can be computed via the following bi-level program:
max σ∈ΩR
yl(σ)∈[0,1],∀l∈L
QR(σ) + αWR(σ), (8a)
s.t. yl(σ) ∈ argmax y∈[0,1] Ul(σ, y), for all l ∈ L. (8b)
In the upper level problem, the administrator selects a strategy σ(α) to maximize its total revenue to which users best respond by maximizing utilities in the lower-level problem. Note here that, in our contract game, since the principal gives α fraction of the total welfare accrued to the administrator, the administrator’s objective of maximizing revenues corresponds to Equation (8a), which represents the sum of the fines collected by the administrator from allocating its security resources and the payment it receives from the principal for its contribution to the system’s welfare. The corresponding payoff of the principal, given a contract level α and the administrator strategy σ(α), as given by the solution of Problem (8a)-(8b), is thus given by (1− α)WR(σ(α)). An equilibrium of our contract game is specified by a triple (α∗,σ(α∗), (yl(σ(α ∗)))l∈L), where (σ(α ∗), (yl(σ(α ∗)))l∈L)
represent the solutions of the bi-level Program (8a)-(8b) given parameter α∗, which the principal selects to maximizes its payoff (1−α)WR(σ(α)). In the following, we first note that our earlier developed algorithmic approaches and theoretical guarantees in the revenue and welfare maximization settings naturally apply in studying the administrator and user strategies, i.e., the solution of Problem (8a)-(8b), given any contract α ∈ [0, 1]. Then, we present a dense-sampling approach to compute a near-optimal solution to the principal’s problem of selecting a contract α ∈ [0, 1] that maximizes its payoff (1− α)WR(σ(α)). Administrator and User Strategies: In studying the equilibrium strategies of the administrator and users, we first note that solving Problem (8a)-(8b) is, in general, NP-hard, which follows from an analogous reduction to that in the proof of Theorem 2 (see Appendix D). Thus, we develop an algorithm, which we henceforth refer to as Contract-Greedy, akin to the greedy algorithms developed in the earlier studied revenue and welfare maximization settings, to compute an administrator strategy in our contract game for any contract α ∈ [0, 1]. For instance, in the deterministic setting when the administrator knows each location’s type, Contract-Greedy (see Algorithm 6 in Appendix D) is akin to Algorithm 2 in the welfare maximization setting other than in the process of sorting locations, as the administrator in our contract game, maximizes a linear combination of the revenue and welfare objectives. Moreover, since the administrator maximizes a linear combination of the revenue and welfare objectives, our earlier developed approximation ratio and resource augmentation guarantees also extend to this contract game. Thus, as many of the ideas developed in earlier sections apply in studying optimal administrator strategies in our contract game, for brevity, we defer the details of the algorithm, Contract-Greedy, and its guarantees to Appendix D.
Principal’s Strategy: Thus far, we have studied the strategies of the administrator and users given a contract α. We now consider the principal’s problem of selecting a contract α ∈ [0, 1] to maximize its payoff (1 − α)WR(σ(α)). To this end, since obtaining an exact functional form of the welfare WR(σ(α)) for all contracts α ∈ [0, 1] is challenging, we present a dense-sampling method to compute a near-optimal solution to the principal’s payoff maximization problem. In particular, consider the solutions σ(α) of the bi-level Program (8a)-(8b) for α taken from a finite set As = {0, s, 2s, . . . , 1} for some step-size s ∈ (0, 1). We then evaluate the total welfare of each of the solutions σ(α) for α ∈ As and return the value α∗s from this discrete set that results in the highest payoff to the principal, i.e., (1 − α∗s)WR(σ(α∗s)) ≥ (1 − α)WR(σ(α)) for all α ∈ As.3
We now show that applying the above dense-sampling procedure approximately maximizes the principal’s payoff across all contract parameters α ∈ [0, 1], when the administrator strategy, given any parameter α, corresponds to the optimal solution of the bi-level Program (8a)-(8b). 3Methods beyond dense sampling, e.g., gradient-based methods [49], can also be used; however, we use dense sampling, as it is computationally tractable when optimizing over a single variable and achieves the guarantee in Theorem 8. Theorem 8 (Near-Optimality of Dense Sampling). Let α∗ ∈ [0, 1] be the principal’s payoff maximizing contract and α∗s ∈ As be the contract computed through dense-sampling. Further, given any α, let σ(α) be the solution of Problem (8a)-(8b). Then, for a step-size s ≤ ϵ∑
l vl , the loss in the principal’s payoff through
dense sampling is bounded by ϵ, i.e., (1− α∗)WR(σ(α∗)) ≤ (1− α∗s)WR(σ(α∗s)) + ϵ. The challenge in establishing Theorem 8 is that the welfare WR(σ(α)) is, in general, not continuous in α (e.g., the welfare function is discontinuous at σl =
dl dl+k for any location l, as depicted in Figure 1). Thus, the
key idea in proving this result involves showing that the welfare WR(σ(α)) is monotonically (non)-decreasing in α. For a complete proof of Theorem 8, see Appendix E.
The near-optimality of dense sampling, more generally, applies beyond administrator strategies corresponding to the solution of Problem (8a)-(8b) and, in particular, holds for any strategy σ̃(α) such that the welfare WR(σ̃(α)) is monotonically non-decreasing in α (see Appendix E.1). For instance, this monotonicity condition is satisfied by the solution computed using Contract-Greedy under a correlation assumption on the valuations vl and the welfare bang-per-buck ratios vl(dl+k)
dl (see Appendix E.1). Moreover, we ob-
serve from our experiments in Section 6.2 that the welfare corresponding to the strategies computed using Contract-Greedy is non-decreasing in α. 6.2 numerical experiments This section studies our contract game through numerical experiments based on a case study of queue jumping in IPT services (see Section 1.1) in Mumbai, India. In the following, we present model parameters for our experiment and results demonstrating the variation in the welfare accrued by a revenue-maximizing administrator for different model parameters as the contract level α is varied. Model Parameters: We consider a problem instance with L = 448 locations, representing the locations to hail the IPT service in Mumbai [50]. We assume that each location l has one type, i.e., |I| = 1, where the number of fraudulent users Λl are exponentially distributed with rate 80, i.e., Λl ∼ Exp(80) for all l, and the benefits dl from engaging in fraud are exponentially distributed with rate 20, i.e., dl ∼ Exp(20) for all l. Moreover, we vary the number of resources R ∈ {1, 2, . . . , 30}, the fine k ∈ {50, 100, . . . , 500}, and consider a welfare function given by vl = Λl(dl)
x (see Section 3.1) for x lying in the range {1, 1.25, 1.5, 1.75, 2} for all locations l. For a detailed overview of the motivations behind the choice and calibration of our model parameters, we refer to Appendix F.
Results: Figure 3 depicts the variation in the welfare achieved by the allocation corresponding to Contract-Greedy as the contract α is varied as a fraction of the welfare achieved using Algorithm 2 in the welfare maximization setting for different model parameters. In the left of Figure 3, we fix the fine to k = 500 and the number of resources R = 10 and vary the exponent x of the welfare function vl = Λl(dl)
x. Analogously, in the center of Figure 3, we fix the welfare function vl = Λl(dl)
1.25 and the fine k = 500, with each curve corresponding to a different number of resources R. Finally, the right of Figure 3 depicts curves for different fines, where the welfare function vl = Λl(dl)
1.25 and the number of resources R = 15. For the results in Figure 3, we consider the contract α to be chosen from a discrete set between zero and one with 0.05 increments, i.e., the step-size s = 0.05. From Figure 3, we first observe that regardless of the model parameters, the welfare corresponding to an administrator strategy computed using Contract-Greedy is monotonically non-decreasing in the contract α. Such a monotonic relation aligns with the proof of Theorem 8 and is natural as a higher contract α implies that the administrator is compensated more for the welfare it contributes. Further, we note from Figure 3 that the revenue-maximizing solution, corresponding to α = 0, achieves only a small fraction of the welfare achieved using Algorithm 2 in the welfare maximization setting, suggesting that the administrator’s revenue and welfare maximization goals can often be at odds. However, for most tested parameters, an appropriately chosen contract α can recover a majority of the welfare achieved by Algorithm 2. For instance, when vl = Λl(dl)
1.25, a contract of 0.5 maximizes the principal’s payoff and achieves about 86% of Algorithm 2’s welfare. Next, we note from Figure 3 (left) that as we increase the exponent x of the welfare function vl = Λl(dl) x, the fraction of the welfare achieved by the allocation computed using Contract-Greedy to that achieved by Algorithm 2 increases for each contract α. Such a relation naturally follows as the welfare term in the administrator’s Objective (8a) increasingly dominates the revenues from the collected fines at each location with an increase in the exponent of the welfare function. Consequently, from the left of Figure 3, our
results, for the studied welfare functions with an exponent x > 1, demonstrate that using even small values of the contract α can recover most of the system welfare, thus bridging the gap between the welfare and revenue-maximizing outcomes. Further, the fraction of the welfare achieved by the strategy computed using Contract-Greedy to that achieved by Algorithm 2 in the welfare maximization setting for any contract α (i) remains nearly constant regardless of the number of resources (center of Figure 3) and (ii) increases for lower fines (right of Figure 3). Such a result holds as while varying the number of resources R does not influence any property of the locations, e.g., a location’s type Θl = (Λl, dl, vl) is independent of R, changing fines impacts the threshold dl dl+k at each location l at which the revenue and welfare functions have a jump discontinuity (see Figure 1). Consequently, while our greedy-like algorithms are not influenced by a change in the number of resources other than that the algorithms either terminate sooner or later depending on the number of resources, a change in the fine influences our algorithms’ outcomes as the locations are sorted in a (potentially) different order for each fine k.
Moreover, Figure 3 (right) implies that at lower fines, the allocation computed using Contract-Greedy recovers a higher proportion of the welfare compared to that achieved using Algorithm 2 in the welfare maximization setting. Such a result holds, as at lower fines, an administrator maximizing Objective (8a) is more likely to optimize welfare as the compensation it receives from the principal outweighs its low fine collections. Thus, Figure 3 (right) highlights that setting high fines can be detrimental to the system’s welfare in the presence of a revenue-maximizing administrator. Such a result thus highlights the value of setting low to moderate fines, as often happens in practice, in deterring an administrator from solely maximizing revenues through the collected fines and instead incorporating welfare maximization in its objective even at low contract levels α. Overall, our results present several sensitivity relations that elucidate the impact of the welfare function vl, number of resources R, and fine k on the welfare achieved in the system using Contract-Greedy for different contract parameters α. Moreover, our results highlight the effectiveness of contracts in bridging the gap between the welfare and revenue maximization administrator objectives. For a further discussion and analysis of the results in Figure 3, we refer to Appendix F.3. 7 conclusion and future work In this work, we studied the problem of policing and monitoring fraudulent activities at potentially susceptible nodes or locations in a system as a security game between an administrator and fraudulent users. Motivated by several real-world settings where fraudulent and illegal activities by certain groups of users are prevalent (see Section 1.1), we introduced a model of a security game wherein the administrator can deploy a budget of security resources across locations and levy fines against users found engaging in illegal activities. We studied our security game under both welfare and revenue maximization administrator objectives. In both settings, we showed that, in general, the problem of computing the optimal resource allocation strategy of the administrator is NP-hard, and we developed greedy-like algorithms for both administrator objectives with associated approximation ratio and resource augmentation guarantees. Finally, given that the revenue and welfare-maximizing outcomes of the administrator can differ significantly from each other, we presented a framework inspired by contract theory that helps ensure that a revenue-maximizing administrator incorporates welfare maximization as part of its objective when allocating security resources. Beyond extending our theoretical results in the welfare and revenue maximization settings to the contract framework, we also
presented numerical experiments based on a real-world application case of intermediate public transport services in Mumbai, India. Our numerical results validated our theoretical insights and highlighted the effectiveness of using contracts in bridging the gap between the revenue and welfare-maximizing outcomes of the administrator.