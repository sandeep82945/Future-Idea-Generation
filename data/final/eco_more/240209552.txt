There is increasing interest in using LLMs as decision-making “agents.” Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions—and more broadly, determining whether an LLM agent is reliable enough to be trusted—requires a methodology for assessing such an agent’s economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained “elements” that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a “rationality report card”. Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models’ ability to exhibit rational behavior. 1 introduction Recently, much research has worked to leverage Large Language Models (LLMs) to create decisionmaking engines, configuring them either to act directly as economic agents [Cai et al., 2023, Horton, 2023, Wang et al., 2023a] or to serve as key elements of broader systems that do so [Zhuge et al., 2023, Wang et al., 2023b, Shen et al., 2023]. LLM-based agents are already showing strength in planning (e.g., for personal finance [Reworkd, 2023]), solving complex problems [e.g., medical diagnostics; McDuff et al., 2023], leveraging tools [e.g., Schick et al., 2023] and playing games [e.g., chess; Nakajima, 2023]. Better decision-making capabilities will be critical for advancing the use of Reinforcement Learning from AI Feedback (RLAIF) to fine-tune chatbots, such as constitution-based approaches [Bai et al., 2022, Hong et al., 2023]. LLMs may soon take the place of humans in some social science experiments [Horton, 2023, Aher et al., 2023, Park et al., 2023]. Eventually, this research agenda offers the promise of realizing the longstanding AI dream of personalized economic agents. Preprint. Under review. ar X
iv :2
40 2. 09 55
2v 1
[ cs
.C L
How can we best configure LLMs to maximize their performance on decision-making tasks (e.g., via prompting to perform chain-of-thought reasoning; fine-tuning; or more complex architectures that make repeated calls to a model)? After we have done so, how well do LLM-based agents perform? Recent research has begun to develop testing regimes that can address these questions in various restricted domains. These include various narrowly defined tasks [Liu et al., 2023, Qin et al., 2023, Zhou et al., 2023, Yao et al., 2023]; limited economic settings [Shah et al., 2022, Chen et al., 2021, Sinha and Khandait, 2020, Araci, 2019, Akata et al., 2023]; and open-world video games [Wang et al., 2023a, Zhu et al., 2023]. Going beyond such problem-specific approaches to assessing decision making more broadly requires a holistic approach to describing good decision making and explaining how it can be decomposed into different, individually testable components. One approach is to divide decision making into distinct, ad hoc tasks, emphasizing those that have been clasically studied in NLP [Liang et al., 2022, Gehrmann et al., 2021]. We advocate a different approach: enumerating first principles that describe how agents should make decisions, and then evaluating an agent’s degree of adherence with these principles. Answering the normative question of how decision-makers should act has been the focus of more than a century of research in economics, cognitive psychology, computer science, and philosophy. The resulting literature provides a mature mathematical foundation for so-called economic rationality. The grounding principle is that agents should (implicitly or explicitly) quantify their preferences according to a utility function and make decisions to maximize their own expected utility. The literature further characterizes elements of economic settings that fundamentally impact rational behavior: e.g., stochasticity is different from determinism; multi-agent environments are different from single-agent environments; reasoning about how best to make decisions for groups of agents is different from reasoning about how to act as an individual. In some cases, the theory is prescriptive: e.g., it’s better to maximize utility than to accept lower-utility alternatives. In other cases, things become more complicated: e.g., in multi-agent environments, determining the best choice depends on beliefs about how other agents will act. In still others, impossibility results rule out all desirable options, e.g., when deciding how to aggregate multiple agents’ preferences. Finally, in some cases human decision-makers exhibit cognitive biases that deviate from rational behavior, even when the theory makes a firmly prescriptive recommendation. How can we hope to assess rationality when the landscape is so complex? Our approach is to identify tests for which the “rational” answer is well defined. In cases where the prescriptive recommendation is clear, assessment is unproblematic. More ambiguous settings can be tested by explicitly asking for a desired behavior (e.g., eliciting a Nash equilibrium strategy). Such settings also often admit prescriptive special cases: e.g., even in multi-agent environments, it is never a good idea to play a dominated strategy. Axiomatic theories naturally give rise to meaningful tests, which can be applied from the simplest settings (e.g., the von Neumann–Morgenstern axioms for utility maximization) to the most complex (Arrow’s axioms for social choice functions). Famous human subject experiments that illustrate cognitive biases also naturally give rise to tests, which human decision-makers often fail. In the end, we obtain a Rationality Report Card (RRC). We leave it to the end user to determine the scoring rubric: e.g., should the agent receive good grades for doing well only in a subset of simple settings; for being as rational as possible across the board; or for behaving as humanly as possible, including replicating biases? More specifically, our work begins by identifying a rich and hierarchical taxonomy of 64 “elements of rationality” for which some notion of a “right answer” is well defined (Section 2). We define each element, giving an example of each in an appendix. We then move on (in Section 3) to describe how we used this taxonomy to derive a fine-grained benchmark distribution that serves as the basis for rationality report cards. Our benchmark allows each element of rationality to be instantiated in multiple “grade levels” of difficulty and in multiple domains (e.g., asking questions about finance vs. medicine). For 49 elements, we have written LLM prompts to synthetically generate 24,500 multiple-choice questions and manually validated 2,450 generations in total. We also discuss how RRCs can be graded. We built a web interface for generating benchmark questions, validating them, and visualizing experimental results; it allows elements to be filtered by position in the taxonomy, by logical dependence (e.g., the “maximize utility” element depends on the “transitivity” element), by domain, and by grade level. To demonstrate the utility of our system, we generated full RRCs for 14 language models, ranging from Llama 7B to GPT-4 Turbo, evaluated on 40,000 test questions. This experimental setup is described in Section 4. We spent $4,800 making calls to OpenAI’s API and devoted 13,240 GPU hours of compute to evaluating open models. Section 5 describes our results;
here are some highlights. Across our benchmark, we found that model size correlated heavily with performance: models smaller than 40 billion parameters were not reliably able to outperform random guessing. Model performance consistently decreased with grade level. GPT-4 Turbo was consistently the best model across all of our metrics and elements; its performance was excellent up to grade 5 (Level-k Reasoning), decent up to grade 7 (Avoidance of the Endowment Effect), and fell to random guessing from grade 9 (Best Response) and above. Self-explanation and few-shot prompting were consistently able to help. Self-explanation generally enhanced performance, albeit offering the most gains on lower-grade-level questions. Few-shot prompting enhanced model performance when we offered up to three examples, but decreased performance beyond that point. We release all model outputs to support evaluation research and contributions, and provide a public website with all results, underlying model predictions details, alongside an extensible codebase to support the community in taking RRCs further. 2 taxonomizing the elements of economic rationality Building a structured assessment of LLMs’ economic rationality requires first deciding on a way of structuring the space of economic behavior. Fundamentally, economics is concerned with decision making, and an economically rational agent is one that makes good decisions given its own interests and its knowledge about the environment in which it acts. Different economic environments can give rise to very different decision-making problems. We thus divide our space into increasingly rich settings. We begin with DECISIONS IN SINGLE-AGENT ENVIRONMENTS, exploring preference formation and decision making when an agent has a set of alternative choices, each of which leads either to a single, deterministic outcome or to a draw from a probability distribution over outcomes. DECISIONS IN MULTI-AGENT ENVIRONMENTS enriches this setting, requiring the agent to make decisions when the outcomes depend on interactions with other economic agents with their own preferences and beliefs. DECISIONS ON BEHALF OF OTHER AGENTS asks the agent to aggregate the preferences of other agents to achieve good outcomes for all. Lastly, FOUNDATIONS are core mathematical and cognitive skills that underlie economic reasoning: arithmetic, optimization, probability, logic, and theory of mind. (We will hereafter list FOUNDATIONS first, since it is the simplest of all.) Each setting is partitioned into multiple distinct modules, and each module consists of multiple elements of rationality: measurable capabilities that an economically rational agent is able to exhibit. Economics is a vast and rich discipline, and so choices about how to divide topics and derive elements could have been made in many ways. We derive elements in our taxonomy from a key design decision: to ask only questions that have a correct answer. This led us to avoid a wide range of topics that do not easily lend themselves to such questions. We thus aimed not to characterize every single behavior that a rational agent would exhibit, but rather to identify a useful set of tests on which scoring highly constitutes strong evidence of economic rationality. In cases where we fall short even of this goal, we intend to continuously update both our taxonomy and the benchmark we derive from it. Even given our restriction to representative, testable elements of rationality rather than all of decision making, it is an underspecified task to determine what we should demand of an economically rational agent. In some cases, the ability to make decisions for groups of other agents will be important; in others it will be unnecessary. Some agents need to reason about strategic counterparties; other agents do not. Some agents should make linear trade-offs between utility and money; others should be risk averse. Some agents should behave as perfect utility maximizers in every situation; others should reproduce human strategic behavior as accurately as possible even when this means exhibiting a cognitive bias. Our response to this issue is twofold. First, notwithstanding the arguments just made, we ground each element of rationality in terms of a canonical “right answer.” We always align these canonical answers with the von Neumann–Morgenstern (vNM) utility axioms, which imply freedom from cognitive biases, hence e.g. time consistency and reference independence. In some cases the vNM axioms are not sufficiently constraining; thus, e.g., we align our canonical answers with a linear utility for money (and thus risk neutrality). Second, we allow the user to define a “rationality report card” in whatever way they choose, stipulating the polarity with which an agent should be scored on questions (e.g., perhaps it should deviate from canonical answers on risk neutrality questions to exhibit risk aversion; perhaps it should deviate from canonical answers on cognitive bias questions to better model human behavior) and potentially skipping over certain elements or even entire settings. We set up our taxonomy and benchmark in this way not because we believe that our canonical answer is the right one for every circumstance, but instead because we found it simplest to present both elements of rationality and experimental results in terms of an easily understood reference point. 2.1 setting 1: foundations The economic model of rational decision making is highly mathematical. An agent therefore needs to be fluent in a variety of mathematical basics to be able to compute the value of outcomes, reason about their likelihoods, and choose the best one. In multiagent settings it is also necessary to reason about other agents’ beliefs. This setting lays out these core skills, dividing them into five modules: Arithmetic; Optimization; Probability; Logic; and Theory of Mind. A key difference between this setting and all of the others that we propose is that most of its elements have already been the subject of rich study by the NLP community. We nevertheless include these elements here both to standardize them within our framework, given their importance to economic rationality, and to integrate foundations within our dependency graph (discussed further in Section 3.2.3). 2.1.1 module 1.1: arithmetic Economic reasoning is fundamentally quantitative, so arithmetic operations are a bedrock foundation for much of what is to come. Element 1.1.a (Addition and Subtraction) The ability to add or subtract. Element 1.1.b (Multiplication and Division) The ability to multiply or divide. 2.1.2 module 1.2: optimization Much economic reasoning depends on the primitive operation of identifying the best choice among a set of alternatives, sometimes given constraints. Element 1.2.a (Optimize over a Discrete Set) The ability to identify the biggest or smallest among a set of explicitly given alternatives. Element 1.2.b (Optimize a Continuous Function) The ability to identify a maximum or minimum value given a specific continuous relationship between independent and dependent variables. Element 1.2.c (Constrained Optimization) The ability to find the maximum or minimum of a function subject to constraints. 2.1.3 module 1.3: probability Reasoning under uncertainty is a critical framework for rational decision making. Element 1.3.a (Compute Probabilities of Outcomes) The ability to compute probabilities of individual outcomes given a natural language description of a probability distribution. Element 1.3.b (Complement Rule) The ability to compute the complement probability of an event (i.e., the probability that it does not occur). Element 1.3.c (Bayes’ Rule) The ability to update probabilistic beliefs according to Bayes’ Rule: Let A and B be events and P (B) ̸= 0, then P (A|B) = P (B|A)P (A)/P (B). 2.1.4 module 1.4: logic Logical reasoning forms a basis for much rational reasoning, and so constitutes another category of mathematical foundations. Element 1.4.a (Categorical Syllogism) The ability to deduce if the conclusion logically follows from two assertions (e.g., “A is in C and B is in A, is B in C?”). Element 1.4.b (Conditional Syllogism) The ability to deduce if the conclusion logically follows from two conditional statements (e.g., “If A then B and if B then C, if A then C?”). Element 1.4.c (Logical Equivalence of Contrapositive) The ability to deduce that logical statements and their contrapositives are logically equivalent (e.g., “If A, then B” is equivalent to “if not B, then not A”). 2.1.5 module 1.5: theory of mind Theory of mind is the understanding that others have beliefs, desires, intentions, and perspectives that are different from one’s own. This is crucial for predicting and interpreting the actions of others, especially in competitive contexts or when there is incomplete information about others’ actions or intentions. Element 1.5.a (First-Order False Belief) The ability to identify the beliefs that an agent has that are different from the actual truth or the agent’s own belief. Element 1.5.b (Second-Order False Belief) The ability to identify the beliefs that an agent has about what another agent believes that are different from the actual truth or the agent’s own belief. 2.2 setting 2: decisions in single-agent environments We now turn to explicitly assessing economic rationality. Throughout this paper we leverage the von Neumann–Morgenstern expected utility model [vNM; von Neumann and Morgenstern, 1944], which provides a comprehensive framework establishing ideal norms for how a decision-maker should act [Harsanyi, 1955]. This normative aspect is critical for us, as it allows us to identify testable elements of rationality. The dominance of the vNM approach in economic analysis can be attributed to two key characteristics. First, it makes predictions based on a sparse description of the choice problem: the only components that need to be specified are the agent’s objectives and constraints. Second, it applies to an extremely wide range of choices, extending beyond traditional economic matters like consumption and savings to personal decisions regarding education, career, and healthcare, and business decisions about production levels, technological investments, workforce management, and market entry and exit strategies. There exist various scenarios in which the vNM model’s qualitative predictions are robustly violated in human subject studies. While individual human decision-makers are not typically able to articulate general decision rules that explain their own behavior, a descriptive literature in behavioral economics has attempted to identify such rules as a way of capturing consistent ways in which human choice behavior deviates from the rational ideal (notably, c.f. Savage [1954], Kahneman and Tversky [1979]; for a recent survey, see Erev et al. [2017]). These are of particular interest both because they are likely to be exhibited by humans and may also be exhibited by LLMs trained on examples of human reasoning. We follow [Kochenderfer, 2015] in organizing the modules in this setting by the normative axioms in deterministic and stochastic environments as well as deviations from these axioms drawn from the descriptive literature. 2.2.1 module 2.1: axioms of utility in deterministic environments The vNM utility theory rests on a set of axioms, which are easy to interpret as elements of rationality. We begin with the simplest description of these axioms, in which the agent confronts choices in deterministic environments. Element 2.1.a (Completeness) The ability to determine a preference between two options A and B. E.g., prefer A over B, B over A, or indifference. Element 2.1.b (Transitivity) The ability to be consistent in preferences over options. E.g., if A is preferred over B, and B over C, then A should be preferred over C.
Element 2.1.c (Independence) The ability to remain consistent in preferences between pairs of options regardless of the presence of other alternatives. E.g., if A is preferred to B, introducing a third option C should not change this preference. 2.2.2 module 2.2: avoidance of cognitive biases in deterministic environments A wide range of cognitive biases have been identified by the descriptive economic literature. We identify their opposites as elements of rationality. Element 2.2.a (Avoidance of Sunk Cost Fallacy [Parayre, 1995]) The ability to walk away from an investment at any point where its future costs exceed its expected future benefits, disregarding prior investments. Element 2.2.b (Avoidance of Endowment Effect [Morewedge and Giblin, 2015]) The agent’s maximum willingness to pay to acquire an object should be the same as the price they are willing to accept to sell that same object when they own it. Element 2.2.c (Avoidance of Time Inconsistency [Loewenstein and Prelec, 1992]) The ability to be consistent in preferences across time; e.g., not preferring immediate rewards to larger future rewards when waiting would lead to greater overall utility. 2.2.3 module 2.3: axioms of utility in stochastic environments We now elaborate our economic environment to include a stochastic relationship between an agent’s choices and the resulting economic outcomes. Here, vNM adapts the utility theory axioms to guide rational decision making by defining “lotteries”: probabilistic combinations of outcomes. Element 2.3.a (Completeness over Lotteries) The ability to determine a preference between two lotteries A and B. E.g., prefer A over B, B over A, or indifference. Element 2.3.b (Transitivity over Lotteries) The ability to select among lotteries in a consistent manner. E.g., if A is preferred over B, and B over C, then A should be preferred over C.
Element 2.3.c (Independence over Lotteries) The ability to remain consistent in preferences between pairs of lotteries regardless of the presence of other alternatives. E.g., if A is preferred to B, introducing a third lottery C should not change this preference. 2.2.4 module 2.4: risk neutral expected utility computations This module includes elements evaluating adherence to a linear utility function when computing expected utilities, delving into the behavioral patterns exhibited by individuals and institutions in their approach to risk. Element 2.4.a (Compute Expected Utility) The ability to correctly compute the sum of the products of each outcome’s utility and its probability. Element 2.4.b (Maximize Expected Utility) The ability to select the prospect with the highest expected utility. Element 2.4.c (Avoidance of Risk-Averse Behavior) The ability to make decisions based on an objective evaluation of all potential outcomes without over-valuing more certain payoffs. Element 2.4.d (Avoidance of Risk-Seeking Behavior) The ability to make decisions based on an objective evaluation of all potential outcomes without over-valuing rare high-reward outcomes. Element 2.4.e (Avoidance of Loss Averse Behavior [Kahneman and Tversky, 1984]) The ability to make decisions based on an objective evaluation of all potential outcomes without over-valuing potential losses. 2.2.5 module 2.5: avoidance of cognitive biases in stochastic settings Here, we include elements testing the agent’s ability to avoid making contradictory or inconsistent behaviors, emphasizing how framing effects play in shaping risk-taking attitudes. As we already did in MODULE 2.5: AVOIDANCE OF COGNITIVE BIASES IN DETERMINISTIC ENVIRONMENTS, we state the opposite of each such behavior as an element of rationality. Element 2.5.a (Avoidance of Gambler’s Fallacy) The ability to avoid the incorrect belief that an outcome’s probability (when drawn independently) in the future is reduced if it has occurred atypically often in the past. Element 2.5.b (Avoidance of the Certainty Effect [Kahneman and Tversky, 1984]) The ability to be consistent across preferences towards risk when the payoffs are positive. Element 2.5.c (Avoidance of the Reflection Effect [Kahneman and Tversky, 1984]) The ability to be consistent across preferences towards risk when the payoffs are negative. Element 2.5.d (Avoidance of Ambiguity Aversion [Ellsberg, 1961]) The ability to be consistent across preferences towards known and unknown risks (ambiguity) under differing framing. 2.3 setting 3: decisions in multi-agent environments Economic reasoning changes when the environment contains other agents, falling under the umbrella of game theory [c.f., Fudenberg and Tirole, 1991]. The crucial difference is that other agents cannot simply be modeled as behaving randomly: they act to maximize their own utilities in response to their own beliefs, which include beliefs about the agent’s behavior. Decision making in multi-agent environments thus builds on the elements of rationality already defined, but adds new ingredients. To capture these dynamics, we subdivide the analysis into different representations of strategic interaction as is common in many game theory textbooks [Osborne et al., 2004, Fudenberg and Tirole, 1991, Shoham and Leyton-Brown, 2008]. These representations help in understanding strategic interactions under different conditions in multi-agent decision making scenarios. 2.3.1 module 3.1: normal form games Traditionally in game theory textbooks, a game is described by a matrix which shows the agents, strategies, and payoffs. This form is most commonly used for games where decisions are made simultaneously but can represent any game-theoretic interaction between agents. In this module, we consider games in which agents interact only once selecting strategies without knowledge of the other agents’ choices. Recognizing that LLMs can struggle with tabular data, we begin by assessing the ability to interpret games in both natural language (seen in the left in Figure 2) and with a payoff matrix (see the middle and right of Figure 2). As we see, as games increase in complexity, it becomes more reasonable to describe the game using a payoff matrix. Element 3.1.a (Interpret Games) The ability to select the correct payoff given a set of actions in strategic form games: a matrix of payoffs for a single agent indexed by combinations of strategies by the agents and in bimatrix form games: the matrix includes sets of payoffs, one for each agent. Element 3.1.b (Best Response) The ability to compute and select the strategy with the highest payoff given an opponent’s action. Element 3.1.c (Dominant Strategies) The ability to select strategies that provide a greater payoff than any other strategy, no matter what the other agents do. I.e., strategies that are a best response to all possible strategies. Element 3.1.d (Avoidance of Dominated Strategies) The ability to avoid strategies that are never best responses. Element 3.1.e (Iterated Removal of Dominated Strategies) The ability to systematically eliminate dominated strategies. This process is applied iteratively: after removing all dominated strategies for one agent, the analysis is reapplied to the remaining strategies, including reconsidering what might now be a dominated strategy for other agents in light of the changes. Element 3.1.f (Pure Nash Equilibrium [Nash Jr, 1950]) The ability to play a best response strategy when given knowledge that another agent is also best responding (i.e., is rational). A pure Nash equilibrium occurs when each agent is best responding to the strategies of others wherein no player can benefit by unilaterally changing their strategy. 2.3.2 module 3.2: extensive form games As mentioned, games permit multiple descriptions and extensive form games are represented as trees, showcasing the sequential aspect of decision making. In this module, we consider games where agents can either pick actions sequentially in a round-robin fashion (e.g., tic-tac-toe) or simultaneously over multiple rounds (e.g., best two-out-of-three rock-paper-scissors). The definition of best response, dominated strategies, and Nash equilibria in extensive form games are exactly as they are for normal form games. Indeed, every extensive form game can be converted to an equivalent strategic form or bimatrix form game. However, Nash equilibrium is often too weak a notion for extensive form games. In this module, we consider a refinement on Nash equilibrium known as a subgame perfect Nash equilibrium. The analysis used to find a subgame perfect Nash equilibrium is known as backward induction. Element 3.2.a (Backward Induction) The ability to determine the best action given the subsequent optimal actions working backwards from the end of the game. Element 3.2.b (Subgame-Perfect Nash Equilibrium) The ability to compute and select strategies in a Nash equilibrium not just for the game as a whole but also for every point in the game where the agent takes an action, regardless of the previous moves. 2.3.3 module 3.3: imperfect information in extensive form games In many situations agents must act with partial or no knowledge of the actions of others, or even limited memory of their own past actions. This is often represented as agents being unable to
distinguish nodes in their own action set across the tree. In this module, we consider a refinement on subgame perfect equilibrium: the sequential equilibrium. Element 3.3.a (Sequential Equilibrium [Kreps and Wilson, 1982]) The ability to compute and select a strategy that exists in a sequential equilibrium. 2.3.4 module 3.4: infinitely repeated games We have seen in the previous modules that long-term interactions are fundamentally different from one-shot interactions especially in the presence of uncertainty. Infinitely repeated games also model a long-term relationship in which the agents do not know when they will stop repeating the game: there is no pre-ordained number of repetitions. Therefore, we need new tools as agents can no longer use backwards induction to find equilibrium solutions. Element 3.4.a (Feasibility in Infinitely Repeated Games) The ability to identify if a payoff is feasible in a Nash equilibrium of an infinitely repeated game. Element 3.4.b (Enforceability in Infinitely Repeated Games) The ability to identify if a payoff is enforceable in a Nash equilibrium of an infinitely repeated game. Another important consideration in infinitely repeated games is how to model utilities. We consider the discounted utility model. Element 3.4.c (Trigger Strategies) The ability to compute and select the correct trigger strategy. E.g., a grim trigger strategy, a tit-for-tat strategy, a tit-for-two-tat strategy, etc. 2.3.5 module 3.5: bayesian games So far, the number of agents, the actions available to each agent, and the payoffs have all been assumed to be common knowledge among the agents. Note that this is true even of imperfectinformation games; the actual moves of agents are not common knowledge, but the game itself is. However, Bayesian games allow us to represent agents’ uncertainties about the very game being played. This lack of information fundamentally changes how strategies are formed. We consider solution concepts in both normal form and extensive form games. Element 3.5.a (Bayes–Nash Equilibrium) The ability to compute and select best responses with respect to beliefs about the other agents’ strategies, and can update these beliefs based on observed strategies. Element 3.5.b (Subgame–Perfect Bayes–Nash Equilibrium) The ability to compute and select a strategy that satisfies the following:
1. (Bayes–Nash Equilibrium) The strategy maximizes their expected utility, given their beliefs about the other agents’ types and strategies, and given the strategies of the other agents. 2. (Subgame Perfection) The strategy constitutes a Bayes–Nash Equilibrium not just for the whole game, but for every subgame of the game. This means that even when considering any smaller portion of the game in isolation, the strategies still form a Bayes–Nash Equilibrium. 2.4 setting 4: decisions on behalf of other agents In this final setting, we consider an agent who must make a decision on behalf of other agents. For clarity, we call this agent the decision-maker. In some cases, the decision-maker may be tasked with aggregating the preferences of a group of agents into some global, “social” preference; in others, it may make a choice from some arbitrary decision set. In particular, the decision-maker may be tasked with maximizing social good or with maximizing its own utility. A key modeling issue is whether the decision-maker is aware of the other agents’ true preferences or whether it must ask them to (potentially dishonestly) report them. We divide modules on this axis following other texts in this space [Shoham and Leyton-Brown, 2008] denoting the former scenario as social choice and the latter as mechanism design. 2.4.1 module 4.1: axioms of social choice In this module, we delve into the foundational principles of constructing fair and effective decisionmaking processes within a group. We call a function mapping a collection of individual preference profiles into a single aggregate preference profile a social welfare function. We begin by exploring the axioms that underpin these processes when the decision-maker knows all agents’ preferences. Element 4.1.a (Pareto Efficiency) The ability to select a social welfare function that prefers A to B if all agents prefer alternative A to alternative B. Element 4.1.b (Monotonicity in Social Welfare Functions) The ability to select a social welfare function wherein given a profile of individual preferences the society prefers alternative A to alternative B and a similar profile of individual preferences in which the only change is raise in A’s rank in some individual ranking(s), A is still preferred over B. Element 4.1.c (Transitivity in Social Welfare Functions) The ability to select a social welfare function that defines a transitive output (i.e., well defined ranking over alternatives). Element 4.1.d (Non-Dictatorial Social Welfare Function) The ability to select a social welfare function where there is not a particular individual d, such that the social ranking coincides with d’s ranking any individual preferences profile. 2.4.2 module 4.2: social choice Shifting from the theoretical axioms to applications, we explore basic voting schemes and fair division algorithms. Element 4.2.a (Plurality Vote) The ability to select the alternative which is the most preferred one by the largest number of agents (or rank according to the number of individual preferences an alternative is ranked first). Element 4.2.b (Borda Count) The ability to compute and select the Borda count winner: Borda count is a scheme which, given m alternatives, assigns score m − i to the alternative which is ranked in the i’th place by an agent (e.g. the most preferred alternative gets score m− 1, and the least preferred gets score 0); now select an alternative (or rank) according to the sum of scores the individual rankings provide to each alternative. Element 4.2.c (Copeland’s Method) The ability to compute and select the winner derived by Copeland’s method: Each candidate is compared with every other candidate in a series of one-on-one contests. A candidate receives one point for each victory and half a point for each tie. The candidate with the highest total score is the winner. Element 4.2.d (Fair Division Algorithms) The ability to select the correct fair division algorithm given the context (e.g., divider-chooser, last diminisher) 2.4.3 module 4.3: desirable properties in mechanism design This module adds the wrinkle that agents must report their preferences to the decision-maker and may lie when doing so. The decision-maker’s objective becomes designing the rules of the game, known as a mechanism, in order to incentivize agents to act in a specific way. Unfortunately, in general designing mechanisms to induce agents to report truthfully (i.e., incentive-compatibility) is impossible without additional ingredients. We begin by considering different implementations of incentive-compatible mechanisms. Element 4.3.a (Dominant Strategy Incentive Compatibility) The ability to select a mechanism wherein a strategy in a dominant strategy equilibrium is to report preferences truthfully. Element 4.3.b (Bayesian Incentive Compatibility) The ability to select a mechanism wherein a strategy in a Bayes–Nash equilibrium is to report preferences truthfully. When designing incentive compatible mechanisms, a common additional ingredient is to allow the mechanism to charge or reward agents with an arbitrary monetary amount. Element 4.3.c (Individual Rationality) The ability to select a mechanism wherein it is in the best interest of the agents to participate in the mechanism. Element 4.3.d (Budget Balanced) The ability to select a mechanism wherein the mechanism rewards and charges the same amount of money to and from the agents. 2.4.4 module 4.4: mechanism design We now consider the implementation of specific mechanisms. Element 4.4.a (Top Trading Cycles) The ability to compute and run the top trading cycles algorithm in finding a stable allocation. A common class of mechanisms are auctions. Depending on the properties of the bidders and the nature of the items to be auctioned, various auction structures may be either more efficient or more profitable to the seller than others. We consider three major (one-sided) auction types:
• English Auction, also known as an open-outcry or ascending-bid auction, this auction starts with the auctioneer opening the bidding at some reserve price (which may be zero) and raises the price until no one is willing to increase the bid any further. At which point, the final bidder receives the item and pays her bid price. • First-Price Auction: Each bidder submits a bid discretely and hands it to the auctioneer, who then announces a winner. The winner pays her bid. • Second-Price Auction, also often called a Vickrey auction, here bidders submit bids discretely and the highest bidder wins the item, but now the price the winning bidder pays is the secondhighest bidders bid. Element 4.4.b (Optimal Auction for Bidders with Differing Risk Attitudes) The ability to select the correct revenue maximizing auction when bidders are not risk-neutral. The agent should select the second-price or English auction when bidders are risk-seeking and compute the winning bidder given bids; select the first-price auction when bidders are risk-averse and compute the winning bidder given bids. Element 4.4.c (Optimal Auction for Bidders with Affiliated Values) The ability to select the correct revenue maximizing auction when each bidder’s value has an additional common-value component (e.g., the bidder’s private, noisy signal about the good’s resale value). The agent should select the English auction over a second-price auction, which in turn should be selected over a first-price auction. 3 rationality report cards We are now ready to turn our abstract taxonomy into practical tests assessing LLM performance. We followed the standard practice of encoding our benchmark in Multi-Choice Question Answer (MCQA) format [e.g., Hendrycks et al., 2021, Suzgun et al., 2022, Zellers et al., 2019]. More specifically, each question in a test is a description of a decision-making scenario and a set of candidate choices, exactly one of which is correct. All of the generated questions are organized hierarchically in a web application according to our taxonomy. The remainder of this section describes the methodology we employed in generating and validating these questions and different ways users can leverage them to construct Rationality Report Cards (RRCs). 3.1 generating and validating questions It would be impractical to hand-construct enough questions to assess an agent’s behavior with statistical significance. Instead, we leverage a state-of-the-art LLM to generate a diverse and substantial set of questions, based on a hand-constructed inputs. More specifically, we go from an element to a concrete question as follows. First, we write detailed text describing what makes a good question, such as that each outcome should have an associated probability or that each action pair should have payoffs. These instructions also describe formatting issues, such as how numerical values should
be represented. We also provide a gold-standard example for each question. Together, we call these two text strings a template. Along with the template we append a static system prompt (illustrated in Figure 3) and then repeatedly give the resulting prompt to GPT-4 Turbo to generate many questions from the template. We create several templates for each element, which differ in two key ways. First, we vary the subject matter of the question, which we call the domain, in order to enable assessments of LLM robustness across topics. This approach not only tests an LLM’s capability within each domain but also enables assessment of an LLM’s proficiency, or lack thereof, in specific potential areas of application. Some of our templates concern financial decision making, others focus on medicine and health care, still others ask about technology and innovation. Second, most of our templates also vary in their difficulty levels. We assign every template a grade level ranging from 1–13 to help the user understand its relative difficulty. For example, questions about arithmetic could vary from Grade 1 to Grade 2 depending on the number of digits, whereas questions about Nash equilibrium could vary between Grade 8 and Grade 11. Our specific choices of grade levels are obviously somewhat arbitrary, but we aimed as much as possible to maintain a similar difficulty level across templates in the same grade. Overall, templates at lower grade levels involve basic understanding or application of principles while templates at higher grade levels challenge models with complex problem-solving, critical thinking, and synthesis of concepts. We refer to a set of questions for a given element restricted to a particular domain and grade level as a test. For each test, we implemented 5 templates, totaling between 10–40 templates per element. Figure 4 provides two example questions testing the ability to Maximize Expected Utility that vary in both domains and grade levels. The user can explore our full set of templates through our web application, selecting elements and viewing questions across both domains and grade levels. While GPT-4 Turbo was very good at generating questions, it was not perfect. We thus performed a validation step. First, we programmatically removed questions that were formatted incorrectly. For each element, domain, and grade level triple we then randomly spot-checked 100 samples (i.e., 10% of all generated questions) from what remained. We developed our web application to facilitate such validation, displaying the information needed to ensure that a given generation not only adheres to the intended style and complexity but also properly captures the corresponding element of rationality. We illustrate this interface in the appendix. In total, 98.54% of all spot-checked samples were deemed valid by 2 validators, with the lowest validation rate being 97% (Avoidance of the Endowment Effect). 3.2 rationality report card Users turn a model’s answers to questions across a range of templates into an overall assessment via a Rationality Report Card (RRC). This tool functions much like an academic report card, providing a structured yet customizable evaluation of an LLM’s performance. Figure 5 gives an example of such a subset based on a grade range. We also include various default subsets for different settings (e.g., single-agent decision making; multi-agent decision making), different models (e.g., matching
human performance on cognitive biases), and different use cases (e.g., single-agent decision making in medical domains). These default subsets are accessible via our web application. 3.2.1 scoring an llm’s performance There are multiple ways in which users may want to evaluate an agent’s economic rationality. We thus offer two different families of scoring metrics. 3.2.2 accuracy Exact-match accuracy. This is the fraction of questions answered correctly. Normalized accuracy. Exact-match accuracy scores can be hard to interpret since tests differ in their number of multiple choice options, meaning that the exact-match accuracy of random guessing varies. We can compensate for this by reporting the gap between the model’s exact-match accuracy and random guessing. We compute normalized accuracy for a given element by subtracting the accuracy achieved by random guessing from exact-match accuracy and then dividing by the accuracy of random guessing. Observe that the normalized accuracy of a model falls between -1 and 1. Calibration. It is often important that an LLM be able to express the uncertainty of its recommendation. To quantify such uncertainty, we follow Liang et al. [2022] and compute the expected calibration error [ECE; Naeini et al., 2015, Guo et al., 2017]. ECE measures how closely the probability an LLM assigns to its top answer matches the actual probability of the correct answer, which in our case is 1. It is defined as ∑N i bi||(pi − ci)||, where pi is the exact-match accuracy in bin i, ci is the average probability assigned to top answers in bin i, and bi is the fraction of data points in bin i. We use 10 bins uniformly spaced over the interval [0, 1]. 3.2.3 robustness We can also assess how robustly a model performs, both across domains and across simpler elements that are conceptual subproblems of a given element. Domain robustness. One way to assess robustness is to measure, for each element, a model’s worst-case performance across all domains. We compute the domain robustness on each element by taking the minimum exact-match accuracy over all domains. Dependency robustness. There is a hierarchical structure to our elements of rationality: reasoning about more advanced elements (e.g., the ability to play a best response) depends conceptually on simper elements (e.g., the ability to maximize utility), which of course can depend in turn on yet simpler ones (e.g., the abilities to maximize a function and compute an expectation). We express all such conceptual relationships between elements in a dependency graph. Figure 6 depicts a concrete example of a dependency subgraph for the element Iterated Removal of Dominated Strategies instantiated at grade level 7, where there are two agents each with two actions. The full graph which covers all elements in our taxonomy is accessible in our webapp. It is quite possible for an LLM to be proficient at advanced tasks without proficiency at more basic tasks that make them up. But such behavior is probably not desirable; it offers evidence that if the advanced task were discussed in different terms (e.g., in ways that invoke the conceptually simpler subtasks) model performance could fall. Conversely, if a model fails at an advanced task, it can be informative to trace performance backwards in the dependency graph to understand the model’s performance on the task’s building blocks. We call the quantification of this idea dependency robustness. For an element s we define it as ∑ x∈X |random_gap(s) − random_gap(x)|, where X = {x|random_gap(x) < random_gap(s)}x∈Gs and Gs is the dependency subgraph for some element s. 4 applying our benchmark: setup Table 1 lists the 14 different LLMs we evaluated, varying dramatically in sizes. We ran GPT 3.5 Turbo and 4 Turbo using OpenAI’s API [OpenAI, 2020]. We obtained 12 open-source models from the HuggingFace Hub [Wolf et al., 2019] and ran them on an A100 GPU on Compute Canada. We decoded from all LLMs by sampling with temperature 0. As is standard in model evaluation work [c.f., Liang et al., 2022] we treat LLMs as black boxes that take in input strings and output completion strings along with log probabilities, when available. That means we do not assume access to the internal activations nor a model’s training data. We can nevertheless employ various widely used techniques to tailor an LLM to a desired question. Two common alternatives are self-explanation and prompting. Self-Explanation. Much work has shown that question answering performance can be improved by asking a model to explain its reasoning [Wei et al., 2022, Yoran et al., 2023, Huang et al., 2023]. We take two approaches to implementing this idea, which we dub separate and together. In separate, we call the model twice, first providing the question text and candidate options and asking the model to explain its reasoning, and then providing only the candidate options and asking the model to select the correct answer. In together, we only call the model once, giving the model the question text and candidate options, asking it both to explain its reasoning and to select the correct answer. For each approach, we test the effect that it has on model performance measured both on the accuracy and the confidence (i.e., log probabilities) a model places on its answer. Few-Shot Prompting. Model performance can also be improved by prepending a set of examples to the prompt. For each question, we select n ∈ {0, 1, 2, 4, 5} examples (within the corresponding model normalized exact-match domain and grade level) to test the effect of prompting on a model’s performance. Similarly, we measure the effect by computing the difference on accuracy and confidence for each element. 5 applying our benchmark: results This section assesses models’ relative performance, their robustness, and the extent to which they were improved by adaptation strategies. We report results using multiple different RRCs to highlight different aspects of model performance and to give examples of how RRCs can be used. Our system’s web interface (see Appendix B) can be used to drill more deeply into model performance, the underlying model outputs, and the precise inputs and prompts that generated those outputs. As space permits, we also showcase some of those features here. Whole-Benchmark RRC. Our first RRC aggregates performance across our whole benchmark, using both normalized accuracy and exact-match accuracy. Table 2 shows the results, sorting models in descending order by exact-match accuracy. Performance closely followed models’ numbers of parameters regardless of which measure we used. For models that consistently performed better than random guessing (i.e., those with positive normalized accuracy), normalized accuracy and exact-match accuracy orderings were very similar. The same pattern held in our other experiments; thus, for space reasons, we hereafter focus mainly on normalized accuracy. We also made a head-to-head comparison between each pair of models. GPT-4 Turbo was the most accurate model by a large margin, winning in nearly all matchups. However, it still left a lot to be desired, closing only a third of the gap between random guessing and perfect answers. Of the remaining models, GPT-3.5 Turbo was the second most accurate, followed by Llama-2 70b. Performance again correlated strongly with model size. We did observe that Llama 13b performed better than Llama-2 13b in both accuracy measures but that their ECEs were similar. Grade-Specific RRCs. The previous analysis shows that overall performance was relatively weak for even the best LLMs and terrible for smaller models. However, problems in our benchmark vary tremendously in difficulty. We thus constructed separate RRCs for each grade level, which we expected would impact model performance. The results are illustrated in Figure 7. The red line indicates the performance level of random guessing. Among models exceeding that threshold, performance degraded quite consistently as grade level increased. GPT-4 Turbo closed three quarters of the gap between guessing and perfect answers on Grade 0 (FOUNDATIONS) questions, which are the easiest and have also received the most previous study in the NLP community. Performance fell fast, with only about half the gap closed in Grades 3–4 and performance roughly the same as random guessing from Grade 9 onwards. Many tests in this grade require reasoning about strategic and bimatrix representation of games. For instance, on Best Response (an element testing the ability to select the action with the highest payoff given a fixed opponent action) both GPT-4 Turbo
(−0.121) and GPT-3.5 Turbo (−0.214) performed worse than random guessing; on Iterated Removal of Dominated Strategies (an element testing the ability to iteratively remove both the agent’s and their opponent’s actions that are never best responses) both models performed slightly better: GPT-4 Turbo achieved 0.067 on normalized accuracy and −0.073 for GPT-3.5 Turbo. The two outlying points for GPT-4 Turbo in Grades 3 and 6 are due to strong performance on Avoidance of the Gambler’s Fallacy (an element testing understanding of independent probability draws) and Level-k Reasoning (an element that tests the ability to reason about others’ actions in canonical game theoretic scenarios), respectively. GPT-3.5 Turbo—the second-best model—performed consistently worse, never closing more than about half the gap and falling to random guessing from Grade 7 onwards. Cognitive Bias RRC. It is interesting to ask whether models deviate from economically rational behavior in the same ways as humans. To find out, we constructed an RRC focused only on elements measuring such tendencies: those from deterministic environments and stochastic environments. Focusing on the GPT models, we observed relatively more rational (vs. human-like) performance in deterministic environments (GPT-4 Turbo: 0.575; GPT-3.5 Turbo: 0.307) than in stochastic environments (GPT-4 Turbo: 0.377; GPT-3.5 Turbo: 0.173). In the latter case, both GPT models were susceptible to framing effects. Performance on the Avoidance of the Certainty Effect (an element testing for consistency in risk preferences when the payoffs are positive) was far worse than on the Avoidance of the Reflection Effect (an element testing for consistency in risk preferences when the payoffs are negative). Domain Robustness RRC. Figure 7 shows one point for each element–model pair representing worst-case vs. average normalized accuracy across different domains. It shows high variation in performance even when models achieved high average accuracy. We saw the largest variation in performance in questions testing for Avoidance of Sunk Cost Fallacy, which tests whether an LLM exhibits a human cognitive bias; in particular, GPT-3.5 Turbo exhibited the most such bias in a domain concerned with investing in medical projects. We also saw large performance variation in GPT-4 Turbo when testing for Maximize Expected Utility (an element testing the ability to select the option with the highest expected utility computed with respect to a linear utility function), where the worst performance was in a domain contrasting different medical treatment options. We did not observe that GPT-4 Turbo had any consistent preference towards risk-seeking or risk-averse behavior; rather its performance simply became noisier in the medical treatment domain. We conjecture that this could be due to model alignment (RLHF) having treated medical domains as sensitive. Dependency Robustness RRC. Performance in higher-order elements was almost always worse than in their prerequisites. A notable exception was Pure Nash Equilibrium and its immediate ancestor Iterated Removal of Dominated Strategies (IRDS): across the board, LLMs performed worse on IRDS. This is an especially difficult task for LLMs as it requires iteratively simplifying the bimatrix game representation. This demonstrates a weakness in our test for Pure Nash Equilibrium (and
highlights the value of dependency robustness analysis); evidently, that test does not adequately represent dominance-solvable games. Self-Explanation. All models showed overall performance improvement when asked to explain their reasoning (in both separate and together versions). However, performance gains were not uniform across modules, with the biggest gains coming in low-grade-level questions and cognitive biases in stochastic environments. GPT-4 Turbo’s performance increased dramatically on Avoidance of the Certainty Effect (0.079 → 0.508) and Avoidance of the Reflection Effect (0.390 → 0.865) under this adaptation. Performance on the Avoidance of the Reflection Effect continued to dominate Avoidance of the Certainty Effect. Few-Shot Prompting. We investigated how the number of examples provided in model context influenced performance, varying the maximum number of examples across n ∈ {0, 1, 2, 4, 5}, and rounding down when necessary to fit into the context window. We observed that all models clearly improved from n = 0 to n = 3, including many which had exhibited lower-than-random-guessing accuracy in the zero-shot setting. GPT-4 Turbo’s performance plateaued at 3 examples; at 4 examples and above, we observed deteriorating performance for all models. 6 discussion and conclusions Our work presents a novel benchmark for assessing LLM’s ability to exhibit economically rational behavior, which requires a complex mix of logic, probability, optimization, reasoning about other humans, economic principles, and contextual judgment. Our benchmark can be used to highlight both the strengths and limitations of existing models and adaptation strategies, helping users to determine both where models can be relied upon and where they need more work. In the latter case, our benchmark can be directly useful, offering opportunities for fine-tuning, curating new datasets, and aiding in the development of specialized architectures. The results could impact a wide variety of economic tasks, such as market analysis, policy simulation, and understanding consumer behavior. We also foresee continued progress towards LLMs that mimic human reasoning, whether rational or not. Once they reach a sufficiently high quality threshold, LLMs will also be able to act as proxies in economic studies, facilitating cheaper, bigger, better controlled, and more replicable experiments. 