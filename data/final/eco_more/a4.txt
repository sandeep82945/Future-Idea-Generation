Modern software development involves collaborative efforts and reuse of existing code, which reduces the cost of developing new software. However, reusing code from existing packages exposes coders to vulnerabilities in these dependencies. We study the formation of dependency networks among software packages and libraries, guided by a structural model of network formation with observable and unobservable heterogeneity. We estimate costs, benefits, and link externalities of the network of 696,790 directed dependencies between 35,473 repositories of the Rust programming language using a novel scalable algorithm. We find evidence of a positive externality exerted on other coders when coders create dependencies. Furthermore, we show that coders are likely to link to more popular packages of the same software type but less popular packages of other types. We adopt models for the spread of infectious diseases to measure a package’s systemicness as the number of downstream packages a vulnerability would affect. Systemicness is highly skewed with the most systemic repository affecting almost 90% of all repositories only two steps away. *This version: February 22, 2024. Contact: cfritz@psu.edu, co-pierre.georg@uct.ac.za, angelo.mele@ jhu.edu, and michael.schweinberger@psu.edu. We wish to thank participants at the Network Science in Economics 2023 at Virginia Tech, 2023 BSE Summer Forum Networks Workshop, Southern Economic Association Annual Meeting 2023 as well as seminar participants at Georgetown University and UPenn for helpful comments and suggestions. The authors acknowledge support from the German Research Foundation award DFG FR 4768/1-1 (CF), Ripple’s University Blockchain Research Initiative (CPG), the U.S. Department of Defense award ARO W911NF-21-1-0335 (MS, CF), and the U.S. National Science Foundation awards NSF DMS-1513644 and NSF DMS-1812119 (MS). Mele acknowledges support from an IDIES Seed Grant from the Institute for Data Intensive Engineering and Science at Johns Hopkins University and partial support from NSF grant SES 1951005. 1 introduction Modern software development is a collaborative effort that results in sophisticated software packages that makes extensive use of already existing software packages and libraries (Schueller et al., 2022). This results in a complex network of dependencies among software packages, best described as dependency graphs.1 While this results in significant efficiency gains for software developers (Lewis et al., 1991; Barros-Justo et al., 2018), it also increases the risk that a vulnerability in one software package renders a large number of other software packages also vulnerable. In line with this view, the number of vulnerabilities listed in the Common Vulnerabilities and Exposure Database has increased almost twelve-fold between 2001 and 2019.2 A recent estimate by Krasner (2018) put the cost of losses from software failures at over USD 1 Trillion, up from USD 59 Billion in 2002, estimated by the National Institute of Standards and Technology (2002).3
Because vulnerabilities can spread from one software package to all packages depending on it, coders can create an externality when deciding to re-use code from existing packages rather
1Dependency graphs have been studied for many different programming languages using information from package managers. For an empirical comparison of the most common ones, see Decan et al. (2019). 2The Common Vulnerabilities and Exposure database is a public reference for known information-security vulnerabilities which feeds into the U.S. National Vulnerability Database. See https://cve.mitre.org/. The reported number of incidents has increased from 3,591 for the three years from 1999 to 2001 to 43,444 for the threeyear period from 2017 to 2019. 3Industry group Cybersecurity Ventures estimates that the damages incurred by all forms of cyber crime, including the cost of recovery and remediation, totalled USD 6 Trillion in 2021, and could reach USD 10.5 Trillion annually by 2025 (Source). than implementing the required functionality themselves. Whether this is the case and how large the externality is, is ultimately an empirical question. We model coders’ decisions to create a dependency on another software package as an equilibrium game of network formation with observed and unobserved heterogeneity. The equilibrium characterization provides a likelihood of observing a particular architecture of the software dependency network in the long run. Using data on open source software projects of the Rust programming language, we obtain all 696,790 dependencies among 35,473 repositories of software projects contained in Rust’s software package manager Cargo. We find evidence that a maintainer’s decision to allow a dependency exerts a positive externality on other maintainers: Highly interdependent software packages are likely to become even more interdependent. This means that it is particularly important to ensure that such packages are free of vulnerabilities that potentially can affect a large number of other software packages. Open source software is an ideal laboratory to study software dependencies. Since the actual source code of a software package can be inspected and modified by others, code reuse is not only possible, it is encouraged. Modern software packages are not developed in isolation, but within an ecosystem of interdependent packages. A package manager keeps track of code dependencies and ensures that these are automatically resolved so that when a user installs one package, all of the package’s dependencies are also installed and automatically satisfied. There are many programming languages which provide software using package managers. We restrict our analysis to repositories of software packages written in the programming language Rust and managed by the Cargo package manager.4 Rust is a modern programming language that allows particularly safe software development without compromising performance.5 We use data from https://libraries.io, which, for each software repository includes a full list of all dependencies. We model the development of software as a process where coders reuse existing code, creating a network of dependencies among software packages—organized in repositories, which can
4Cargo is Rust’s official package manager. See here for the official documentation. 5Rust has been voted the most loved programming language by developers on the popular coding knowledge
exchange website http://stackoverflow.com. include several closely related software packages—in the process. This is typical for today’s prevalent object-oriented software development paradigm. Our model describes a system of N individual software repositories. Each repository is managed by a group of coders who decide which dependencies to form. Coders obtain a net benefit of linking to another package which depends on how active this dependency is maintained, how mature, popular, and large it is. We also allow a coders’ utility to be affected by a local, i.e. type-specific, externality: since dependent packages have dependencies themselves, they are susceptible to vulnerabilities imported from other packages. So we assume that coders care about the direct dependencies of the packages they link to. This specification excludes externalities that are more than two links away as in the network formation models of DePaula et al. (2018), Mele (2017) and Mele and Zhu (forthcoming). The network of dependencies forms over time and in each period a randomly selected package needs an update, so the coders decide whether to form a link or update the software in-house. Before updating the link, the package receives a random match quality shock. We characterize the equilibrium distribution over networks as a mixture of exponential random graphs (Schweinberger and Handcock, 2015; Mele, 2022), which can be decomposed into within- and between-type contribution to the likelihood. Estimation of this model is complicated because the likelihood depends on a normalizing constant that is infeasible to compute in large networks. Moreover, the model’s unobserved heterogeneity has to be integrated out in the likelihood, thus further complicating the computations. To alleviate these problems, we resort to an approximate two-step approach. In the first step, we estimate the discrete types of the nodes through approximations of the likelihood via a variational mean-field algorithm for stochastic blockmodels (Vu et al., 2013). In the second step, we estimate the structural payoff parameters using a fast Maximum Pseudo-Likelihood Estimator (MPLE), conditioning on the estimated types (Babkin et al., 2020; Dahbura et al., 2021).6
Our main result is that coders exert a positive externality on other coders when creating a dependency on another package. Other coders are then also more likely to create a dependency with that package. As a result, packages that are already highly interdependent tend to become
6These methods are implemented in a highly scalable open source R package bigergm, available on Github at:
https://github.com/sansan-inc/bigergm. See the Appendix for details on the implementation. even more interdependent. This increases the risk that a vulnerability in a single package has large adverse consequences for the entire ecosystem. One example of how a vulnerability in one package impacted a significant part of critical web infrastructure is Heartbleed, the infamous bug in the widely used SSL/TLS cryptography library, resulting from an improper input variable validation (Durumeric et al., 2014). When Heartbleed was disclosed, it rendered up to 55% of secure webservers vulnerable to data theft, including of the Canadian Revenue Agency, and Community Health Systems, a large U.S. hospital chain. Our model provides a further argument for ensuring the security of highly interdependent software packages. Not only can a vulnerability in such a package affect a large fraction of the entire ecosystem, because of the externality we identify, it is also likely that this fraction increases as time goes on. Another important question in the study of network formation processes is whether linked packages are similar or dissimilar with respect to their covariates. We find evidence of similarity among the same type of software packages in terms of their Size, Popularity, and Maturityas well as between different-type software packages in terms of their Size. We further find evidence of dis-similarity between different-type software packages in terms of their Popularity, Num.Contributors (as an alternative popularity measure), and Num.Forks. In other words, mature, large and popular software packages of one type are likely to depend on similar packages of the same type, but on less popular and mature software packages of another type. This is intuitive, because mature, popular, and large packages are likely to have a large user base with high expectations of the software and coders try to satisfy this demand by providing sophisticated functionality with the help of mature, popular, and large software packages of the same type. These packages are often built using a lot of low-level functionality from large but fairly generic libraries, which is why large popular and mature software packages of one type depend on larger, but less popular and less mature software packages of another type. One example of this is the inclusion of a payment gateway in an e-commerce application. The e-commerce application itself can be sophisticated and complex, like ebay is in the provision of their auction mechanism. For such an application it is particularly valuable to provide users with the ability to use a variety of different payment methods, including credit card, paypal, or buy now pay later solutions like Klarna. Our result is also in line with the recent trend in software de-
velopment away from large monolithic applications and towards interconnected microservices (Traore et al., 2022). Lastly, we adapt the susceptible-infected-recovered (SIR) model from epidemiology to examine the spread of vulnerabilities in the Rust Cargo dependency graph. We measure a repository’s kstep systemicness as the number of downstream packages that are potentially rendered vulnerable by a vulnerability in the upstream repository. We find that systemicness is highly skewed across repositories: While the average vulnerable repository affects 52 repositories two steps away, the most systemic repository affects 26,631 repositories. This heterogeneity underscores the existence of key nodes within the software dependency graph whose compromise could lead to widespread vulnerability exposure. We then study the efficacy of targeted interventions to mitigate vulnerability spread, drawing parallels with vaccination strategies in public health. By focusing on securing the most critical nodes—determined by a combination of betweenness centrality and expected fatality—we assess the potential to significantly reduce the risk of vulnerability contagion. Our findings suggest that protecting the top ten most critical repositories can reduce the extent of vulnerability contagion by nearly 40%. Our paper relates to several strands of literature in both economics and computer science. First, our paper contributes to a growing literature on open source software, which has been an interest of economic research.7 In an early contribution, Lerner and Tirole (2002) argue that coders spend time developing open source software—for which they are typically not compensated— as a signal of their skills for future employers. Likewise, one reason why companies contribute to open source software is to be able to sell complementary services. Open source projects can be large and complex, as Zheng et al. (2008) point out. They study dependencies in the Gentoo Linux distribution, and show that the resulting network is sparse, has a large clustering coefficient, and a fat tail. They argue that existing models of network growth do not capture the Gentoo dependency graph well and propose a preferential attachment model as alternative.8
7For an overview of the broad literature in the emerging field of digital economics, see Goldfarb and Tucker (2019). 8In earlier work, LaBelle and Wallingford (2004) study the dependency graph of the Debian Linux distribution and show that it shares features with small-world and scale-free networks. However, LaBelle and Wallingford (2004) do not strictly check how closely the dependency graph conforms with either network growth model. The package manager model is nowadays adopted by most programming languages which makes it feasible to use dependency graphs to study a wide variety of settings. Kikas et al. (2017), for example, study the structure and evolution of the dependency graph of JavaScript, Ruby, and also Rust. The authors emphasize that dependency graphs of all three programming languages have become increasingly vulnerable to the removal of a single software package. An active literature studies the network structure of dependency graphs (Decan et al., 2019) to assess the vulnerability of a software ecosystem (see, for example, Zimmermann et al. (2019)). These papers show the breadth of literature studying open source ecosystems and dependency graphs. However, the literature considers the network either as stochastic or even as static and given. By contrast, we model the formation of dependencies as coders’ strategic choice in the presence of various and competing mechanisms that either increase or reduce utility.9
The theoretical literature on strategic network formation has pointed out the role of externalities in shaping the equilibrium networks (Jackson, 2008; Jackson and Wolinsky, 1996). Estimating strategic models of network formation is a challenging econometric task, because the presence of externalities implies strong correlations among links and multiple equilibria (Mele, 2017; Snijders, 2002; DePaula et al., 2018; Chandrasekhar, 2016; DePaula, 2017; Boucher and Mourifie, 2017; Graham, 2017, 2020). In this paper, we model network formation as a sequential process and focus on the long-run stationary equilibrium of the model (Mele, 2017, 2022). Because the sequential network formation works as an equilibrium selection mechanism, we are able to alleviate the problems arising from multiple equilibria. Adding unobserved heterogeneity further complicates identification, estimation and inference (Schweinberger and Handcock, 2015; Graham, 2017; Mele, 2022). Other works have considered conditionally independent links without externalities (Graham, 2017, 2020; DePaula, 2017; Chandrasekhar, 2016), providing a framework for estimation and identification in random and fixed effects approaches. On the other hand, because link externalities are an important feature in this contexts, we move away from conditionally independent links, and model nodes’ unob-
9Blume et al. (2013) study how possibly contagious links affect network formation in a general setting. While we do not study the consequences of the externality we identify for contagion, this is a most worthwhile avenue for future research. served heterogeneity as discrete types, whose realization is independent of observable characteristics and network, in a random effect approach. We can thus adapt methods of community discovery for random graphs to estimate the types, following approximations of the likelihood suggested in Babkin et al. (2020) and improved in Dahbura et al. (2021) to accomodate for observed covariates. Our two-steps method scales well to large networks, thus improving the computational challenges arising in estimation of these complex models (Boucher and Mourifie, 2017; Vu et al., 2013; Bonhomme et al., 2019). Our model is able to identify externalities as well as homophily (Currarini et al., 2010; Jackson, 2008; Chandrasekhar, 2016), the tendency of individuals to form links to similar individuals. On the other hand, our model can also detect heterophily (or competition) among maintainers. We also allow the homophily to vary by unobservables, while in most models the homophily is estimated only for observable characteristics (Currarini et al., 2010; Schweinberger and Handcock, 2015; DePaula et al., 2018; Chandrasekhar, 2016; Graham, 2020). 2 a network description of code  2.1 nomenclature and definitions The goal of computer programs is to implement algorithms on a computer. An algorithm is a terminating sequence of operations which takes an input and computes an output using memory to record interim results.10 We use the term broadly to include algorithms that rely heavily on user inputs and are highly interactive (e.g. websites, spreadsheet and text processing software, servers). To implement an algorithm, programming languages need to provide a means of reading input and writing output, have a list of instructions that can be executed by a computer, and provide a means of storing values in memory. More formally:
Definition 1. Code is a sequence of operations and arguments that implement an algorithm. A
10Memory to record interim instructions is sometimes called a “scratch pad”, in line with early definitions of
algorithms which pre-date computers. See, for example, Knuth (1997) (Ch.1). computer program is code that can be executed by a computer system. In order to execute a program, a computer provides resources—processing power and memory— and resorts to a compiler or interpreter, which in themselves are computer programs.11
Software developers, which we refer to as coders, use programming languages to implement algorithms. Early programs were written in programming languages like FORTRAN and later in C, both of which adhere to the procedural programming paradigm. In this paradigm, code is developed via procedures that can communicate with each other via calls and returns. Definition 2. A procedure is a sequence of programming instructions, which make use of the resources provided by the computer system, to perform a specific task. Procedures are an integral tool of almost all programming languages because they allow a logical separation of tasks and abstraction from low-level instructions, which greatly reduces the complexity of writing code. We use the term procedure broadly and explicitly allow procedures to return values. The first large software systems like the UNIX, Linux, and Windows operating systems or the Apache web server have been developed using procedural programming. Today, however, the dominant modern software development paradigm is Object-oriented programming (OOP).12 Under this paradigm, code is developed primarily in classes. Classes contain data in the form of variables and data structures as well as code in the form of procedures (which are often called methods in the OOP paradigm). Classes can communicate with one another via procedures using calls and returns. 11The term "compiler" was coined by Hopper (1952) for her arithmetic language version 0 (A-0) system developed for the UNIVAC I computer. The A-0 system translated a program into machine code which are instructions that the computer can execute natively. Early compilers were usually written in machine code or assembly. Interpreters do not translate program code into machine code, but rather parse it and execute the instructions in the program code directly. Early interpreters were developed roughly at the same time as early compilers, but the first widespread interpreter was developed in 1958 by Steve Russell for the programming language LISP (see McCarthy (1996). 12For a principal discussion of object-oriented programming and some differences to procedural programming, see Abelson et al. (1996). Kay (1993) provides an excellent historical account of the development of early objectoriented programming. Definition 3. A class is a code template defining variables, procedures, and data structures. An object is an instance of a class that exists in the memory of a computer system. Access to a computer’s memory is managed in most programming languages through the use of variables, which are memory locations that can hold a value. A variable has a scope which describes where in a program text a variable can be used.13 The extent of a variable defines when a variable has a meaningful value during a program’s run-time. Depending on the programming language, variables also have a type, which means that only certain kinds of values can be stored in a variable.14
Similar to variables, data structures are memory locations that hold a collection of data, stored in a way that implements logical relationships among the data. For example, an array is a data structure whose elements can be identified by an index. Different programming languages permit different operations on data structures, for example the appending to and deleting from an array. Classes can interact in two ways. First, in the traditional monolithic software architecture, widely used for enterprise software like the one developed by SAP, for operating systems like Microsoft’s Windows, and even in earlier versions of e-commerce platforms like Amazon, individual components cannot be executed independently. In contrast, many modern software projects use a microservices software architecture, which is a collection of cohesive, independent processes, interacting via messages. Both architectures result in software where individual pieces of code depend on other pieces, either within a single application or across various microservices. These dependencies form a network which we formalize in the next section. 13There are three different types of variables in object-oriented code. First, a member variable can take a different value for each object of a class. Second, a class variable has the same value for all objects of a class. And third, a global variable has the same value for all objects (i.e. irrespective of the object’s class) in the program. 14For example, C is a statically typed programming language, i.e. the C compiler checks during the compilation of the source code that variables are only passed values for storage that the variable type permits. Similarly, Rust is a statically typed programming language that, unlike C, is also object oriented. Python, on the other hand, is dynamically typed, and checks the validity of value assignments to variables only during run-time. 2.2 dependency graphs Most open source software is organized by package managers like Cargo (for the Rust programming language) that standardize and automate software distribution. To make the installation of complex software projects easier for users, package managers also keep track of a package’s dependencies and allow users to install these alongside the package they are installing. The existence of package managers and the level of automation they provide is necessary because modern software is frequently updated and different versions of the same package are not always compatible. Packages are logically grouped into repositories, controlled and managed by a so-called maintainer.15
Our unit of analysis is the network of dependencies among repositories. Specifically, we describe a software system consisting of N = {1, . . . , N } repositories with N = |N | ≥ 3, each of which is managed by a different maintainer who decides whether to implement code herself or to re-use existing code from other repositories. This (re-)use of existing code gives rise to linkages between repositories. The resulting network G = (N ,E ) with E ⊆N ×N is called the dependency graph of the software system. Denote gi j = 1 if (i , j ) ⊆ E is an existing link from repo i to repo j , indicating that i depends on j . Denote as g = {gi j } the resulting N ×N adjacency matrix of the repository dependency graph G . We exclude self-loops by defining gi i := 0, because, technically, a package cannot depend on itself. There are two reasons for constructing the dependency graph on the repository rather than the package level. First, information about the popularity of software packages is only created on the repository level, not the package level.16 And second, repositories provide a logical grouping of related software packages. Alternatively, we could construct the dependency graph on the level of individual software packages. However, the breadth of code and functionality between packages can be much smaller than the breadth of code and functionality within a package. Yet another alternative is to study call graphs arising from procedures within the same software
15Maintainers can also be organizations. Some repositories are also controlled by a group of maintainers, but for
our purposes, this is immaterial. 16For example, users can “star” a repository on GitHub if they find it particularly useful. package (see, for example, Grove et al. (1997)). But these are state-dependent, i.e., dependencies arise during runtime and depending on how the package is executed and interacted with. Conveniently, the website libraries.io provides information collected from various open source software package mangers, including Cargo.17 The data provided includes information on projects—essentially the same as a Cargo package—as well as repositories and for each repository a list of all dependencies, defined on the project level. Each project belongs to exactly one repository, which allows us to construct dependencies among repositories from the provided data. On the repository level, this data includes information about the Size of the repository in kB, its Popularity, measured as the number of stars it has on the website hosting the repository, and as an alternative measure of popularity the number of contributors, i.e. the number of individual coders who have added code to a repository, raised or answered an issue, or reviewed code submitted by others.18
Table 1 gives a high-level overview of the repo-based dependency graph constructed in this way. While there are 91 weakly connected components, the largest weakly connected component covers almost all (over 99%) nodes and edges. This means that we will still cover all relevant dynamics even if we focus on the largest weakly connected component of the dependency graph only. 17They obtain this data by scraping publicly available repositories hosted on GitHub, GitLab, or Bitbucket. 18We restrict ourselves to repositories that have a size larger than zero and that have no more than 50,0000 stars where the latter restriction is included to eliminate 154 repositories where the number of stars seems to have been artificially and unreasonably increased using e.g. a script. Furthermore, there is a lot of heterogeneity in how interconnected repositories are, as Table 2 shows. The standard deviation of the out-degree, i.e. how many dependencies a given repository uses, is almost twice as large as the mean, meaning that there is substantial heterogeneity, with the median being that a repository uses 5 dependencies, while the maximum is 316 dependencies. This heterogeneity is even more stark for the in-degree, i.e. how often a given repository is a dependency for another repository. The standard deviation is more than ten times as large as the mean and the median is 0, while the maximum is 14,585. In Table 3 we show the 10 most depended-on packages (i.e. with the highest in-degree). For each of them it is quite intuitive why the repository is used so often. The most-depended on repository, for example, is libc, which allows Rust code to interoperate with C code, which is particularly important for high-performance and security relevant computations. The secondmost depended repository is rand, which is used to generate random numbers, while log is used for run-time logging, a crucial task during code development. The largest weakly connected component of the repo-based dependency graph is shown in Figure 1, where we apply the community detection algorithm of Blondel et al. (2008) to color-code packages in the same community. The algorithm identifies 32 distinct communities where community members are relatively highly connected to other community members and less connected to nodes outside the community. 2.3 covariate data In addition to the software dependency data, we also obtain additional data from the from libraries.io website for each package (e.g. https://libraries.io/cargo/libc), which we show for the ten most-depended on repositories in Table 3. We use four types of covari-
ates, summarized in a vector of observable attributes xi . First, we use a repository’s Size of all code in the repository, measured in Kilobytes. Second, Popularity is measured as the number of stars–an expression of how many people like a repository or find it useful–a repository received on the website hosting it, e.g. github. And, third, we use an alternative popularity measure, Num.Contributors, defined as the number of coders who have contributed to a repository. A contribution is not only writing code, but contributing to the code generation process more broadly. Contributiions are defined as code reviews, code commits, creating issues related to a repository, and pull requests, i.e. proposing changes to a repository. To facilitate the estimation of our model, we create a categorical variable for each of our covariates using quartiles of the distribution. The reason to discretize the variables is mostly computational, as our algorithm is based on stochastic blockmodels with discrete types (Bickel et al., 2013; Vu et al., 2013). If the covariates are discrete, then the whole machinery for estimation of blockmodels can be adapted to estimate the unobserved heterogeneity (Vu et al., 2013; Babkin et al., 2020; Dahbura et al., 2021); while with continuous variables the computational costs of
estimation become prohibitive for such large networks.19 3 model We model the decisions of a team of coders contributing to a software repository consisting of closely related software packages. Coders decide whether to use functionality from other packages or develop it themselves. In addition to the four observables introduced above, also assume that there are K discrete types, unobservable to the researchers but observable to other coders, zi = (zi 1, ..., zi K ). The type of a software package can be thought of as the basic function of the package and examples include operating system components, graphical user interfaces, text processing software, compilers, and so on. It can also capture unobservable quality of the code and/or developers. A package of type k is denoted by zi k = 1 and ziℓ = 0 for all ℓ ̸= k. We use notation x and z to denote the matrix of observable and unobservable characteristics for all the packages. The utility function of coder i from network g, observables x, unobservables z and parameters θ = (α,β,γ) is:
Ui ( g,x,z;θ )= N∑ j=1 gi j ui j (α,β)+ N∑ j=1 N∑ r ̸=i , j gi j g j r wi j r (γ)+ N∑ j=1 N∑ r ̸=i , j gi j gr i vi j r (γ), (1)
19We provide more details on this in the model and the appendix. More technical details are provided in Babkin
et al. (2020) and Dahbura et al. (2021). The payoff ui j (α,β) := u ( xi ,x j ,zi ,z j ;α,β ) is the direct utility of linking to package j . It is a function of observables (xi ,x j ), unobservables (zi ,z j ) and parameters (α,β). This is the benefit of creating the dependencies to package j , net of costs—a coder will have to audit the code of the linked package and determine it’s quality—of maintaining the link. The cost also includes modification and adaptation of the code that the coder has to do to be able to use the functions and methods available in package j . Furthermore, using another coder’s code also increases the risk that the code contains an undetected bug (assuming that coders care more about the code of their own package than the code of their dependencies), which can be captured as a cost. And lastly, re-using code, while common practice in modern software development, means coders require fewer skills, which might negatively impact their future productivity and can thus be interpreted as another cost. We assume that the utility function ui j (α,β) is parameterized as follows
ui j (α,β) =  αw + P∑ p=1 βw p1{xi p =x j p } if zi = z j αb + P∑
p=1 βbp1{xi p =x j p } otherwise,
(2)
where the intercept α is interpreted as the cost of of forming the dependencies, and it is a function of unobservables only; and 1{xi p = x j p } are indicators functions equal to one when the p-th covariates are the same for i and j . In this parsimonious specification we allow the net benefits of link to vary with both observed and unobserved heterogeneity. The second term and third terms of the utility function (1) are externalities generated while linking to package j . Indeed, package j may be linked to other packages r as well. This means that the coder needs to check the quality and features of the code in these packages, to make sure they are free of bugs and compatible with the coder’s own code. On the other hand, any update to package r may compromise compatibility to package j and i , so this also includes costs of maintaining. In short, wi j r (γ) measures the net benefits of this externality when coders of i form a dependency to library j . Furthermore, if package i creates a dependencies to library j it may compromise compatibility
with other packages r that have a dependencies to i . This is accounted for by the third term vi j r (γ) in utility function (1). We assume that the second and third term in the utility take the form
wi j r (γ) = vi j r (γ) =  γ if zi = z j = zr 0 otherwise,
(3)
This specification assumes that the externalities are local and have the same functional form. The reason for this normalization is that it guarantees the existence of a potential function that characterizes the equilibrium. Furthermore, this assumptions facilitates identification for parameter γ, through variation within types (Mele, 2017, 2022; Schweinberger and Handcock, 2015; Schweinberger and Stewart, 2020; Babkin et al., 2020). To summarize, the coders need to make decisions on whether to link another package or not. When making the decision, they take into account some externalities from this network of dependencies, but not all the possible externalities. We assume a dynamic process of package development. Time is discrete and in each time period t only one coder is making decisions. At time t , a randomly chosen package i needs some code update. Package j is proposed for the update. If package j is not already linked to package i , we have gi j ,t−1 = 0. If the coder decides to write the code in-house there is no update and gi j ,t = 0. On the other hand, if the coder decides to link to package j , the network is updated and gi j ,t = 1. If the dependency already exists (i.e. gi j ,t−1 = 1), then the coder decision is whether to keep the dependency (gi j ,t = 1) or unlink package j and write the code in-house instead (gi j ,t = 0). Here, we make the simplifying assumption that coders cannot substitute a dependency for another dependency at the same time. Formally, with probability ρi j > 0 package i coders propose an update that links package j . The coder decides whether to create this dependency or write the code in-house. Before linking the coder receives a random shock to payoffs εi j 0,εi j 1, which is i.i.d. among packages and time periods. This shock captures that unexpected things can happen both when developing code in house and when linking to an existing package. So the coders of i form a dependency link to package j if:
Ui ( g′,x,z;θ )+εi j 1 ≥Ui (g,x,z;θ)+εi j 0 (4) where g′ is network g with the addition of link gi j . Throughout the paper we maintain the following assumptions (Mele, 2017):
1. The probability that coders of i propose an update that links to package j is strictly posi-
tive, ρi j > 0 for all pairs i , j . This guarantees that any dependencies can be considered. 2. The random shock to payoffs, εi j , follows a logistic distribution. This is a standard as-
sumption in many random utility models for discrete choice. As shown in Mele (2017) and Mele (2022), after conditioning on the unobservable types z, the sequence of networks generated by this process is a Markov Chain and it converges to a unique stationary distribution that can be expressed in closed-form as a discrete exponential family with normalizing constant. In our model, the stationary distribution is
π(g,x,z;θ) := K∏
k=1
eQkk
ckk
[ K∏
l ̸=k Nk∏ i=1 Nl∏ j=1 eui j (αb ,βb ) 1+eui j (αb ,βb ) ] , (5)
where the potential function Qkk can be written as:
Qkk (gkk ) := Q ( gkk ,x,z;αw ,βw ) =
N∑ i=1 N∑ j ̸=i gi j zi k z j k
( αw +
P∑ p=1
βw p1{xi p =x j p } ) +γ
N∑ i=1 N∑ j ̸=i N∑ r ̸=i , j gi j g j r zi k z j k zr k ,(6)
and gkk is the network among nodes of type k; the normalizing constant is:
ckk := ∑
ωkk∈Gkk eQkk (ωkk ). (7)
Here, ωkk ∈Gkk is one network in the set of all possible networks among nodes of type k.20 20As noted in Mele (2017), the constant ckk is a sum over all 2 Nk (Nk−1)/2 possible network configurations for the
The stationary distribution π represents the long-run distribution of the network, after conditioning on the unobservable types z. The second product in (5) represents the likelihood of links between packages of different unobservable types, while the first part is the likelihood of links among libraries of the same type. This simple decomposition is possible because the externalities have been normalized to be local. So the model converges to K independent exponential random graphs for links of the same type, and implies conditionally independent links for between-types connections. Because of this simple characterization of the stationary equilibrium, the incentives of the coders are summarized by a potential function whose maxima are pairwise stable networks. Because of the externalities, in general, the equilibrium networks will be inefficient, as they will not maximize the sum of utilities of the coders.21 4 estimation and empirical results  4.1 estimation Our novel method for estimation of a directed network formation model with observable and unobservable types is a crucial contribution of this paper. Although scalable estimation methods are available for simpler models, like stochastic blockmodels (Vu et al., 2013), conditionally independent links models (Mele et al., 2023), or models for undirected networks (Dahbura et al., 2021; Babkin et al., 2020; Schweinberger and Stewart, 2020), there is no scalable method for directed networks readily available. To understand the computational problem involved in this estimation, we write the likelihood
nodes of type k. This makes the computation of the constant impractical or infeasible in large networks. 21This is evident when we compute the sum of utilities. Indeed, the welfare function will include all the externalities and, therefore, will be, in general, different from the potential, where the externalities are multiplied by 1/2. See also Mele (2017) for more details. of our model as
L (g,x;α,β,γ,η) = ∑ z∈Z L ( g,x,z;α,β,γ,η )= ∑ z∈Z pη (z) π(g,x,z;α,β,γ). (8)
where each node’s type distribution pη(z) is multinomial, and types are i.i.d. Zi |η1, ...,ηK iid∼ Multinomial ( 1;η1, ...ηK ) for i = 1, ..., N
Maximizing (8) involves a sum over all possible type allocations of each node. Furthermore, the stationary distribution π(g,x,z;α,β,γ) (the conditional likelihood) is a function of an an intractable normalizing constant. Therefore direct computation of the objective function is infeasible. Our strategy is to separate the estimation of unobserved types from the structural utility parameters according to a two-step algorithm. First, we estimate the type of each node and, second, we estimate the structural parametersα,β, and γ, conditional on the type of each node (Babkin et al., 2020). In the first step, we extend variational approximations with maximization minorization updates to directed dependencies for approximating the intractable likelihood and estimate the posterior distribution of types (Vu et al., 2013). We then assign the types to each node according to the highest posterior probability. This step exploits the fact that our model corresponds to a stochastic blockmodel with covariates when γ = 0, i.e. when there are no link externalities. Although the variational mean-field approximation algorithm leads to a sequential algorithm, we augment a lower bound of the likelihood by maximizing a minorizing function, which is quadratic. Thereby, we have to solve a quadratic maximization problem whose terms are calculated via fast sparse matrix operations and whose solution is found via quadratic problem solvers with box constraints (Stefanov, 2004). Through this approximation, we can perform the estimation of unobserved types at scale while preserving the complex correlation of dependency links in our data (more details are provided in Appendix A). In the second step, we estimate the remaining structural parameters using a pseudolikelihood
estimator that maximizes the conditional choice probabilities of links conditional on the estimated types (Schweinberger and Stewart, 2020; Dahbura et al., 2021). This is a computationally feasible estimator that scales well with the size of the network. We provide the theoretical and computational details on the scalable estimation procedure in Appendix A. 4.2 results The results of our estimation are reported in Table 5. The first 3 columns show estimates of the structural parameters for links of the same type, while the remaining 3 columns are the estimates for links of different types. Following the 32 identified library types in Figure 1, we estimate a model with K = 32 unobserved types and initialize the type allocation for our algorithm using InfoMap (Rosvall et al., 2009).22
We estimate a positive externality (γ), suggesting that a simple stochastic blockmodel is unable to account for the dependence of the links in our data. This result aligns with our strategic model, suggesting that links are highly interdependent. Essentially, when coders form connections, they (partially) consider the indirect impacts of their actions. However, this behavior leads to a network architecture that might not be as efficient as one where a central planner aims to maximize the overall utility of each participant. Indeed, the externality encourages coders to establish more connections than would be ideal from an aggregate utility-maximization perspective. This also has implications for contagion, as we explain in the next section. The parameter α governs the density of the network and is negative both between and within types. This implies that—other things being equal— in equilibrium sub-networks of libraries of the same type tend to be more sparse than between-types. One interpretation is that α measures costs of forming links. By contrast, the parameter β1 is positive both within and across
22As suggested in the literature on variational approximation (Wainwright and Jordan, 2008), we started the algorithm from different initial guesses of the type allocations but only report the results that achieved the highest lower bound. types, indicating that coders are more likely to link to larger packages of the same and of different types. Interestingly, Popularity has a positive coefficientβ2 within types and a—much smaller—negative coefficient between types. The same is true for Num.Contributors, which is an alternative measure of popularity. Coders are, therefore, likely to link to more popular packages of the same type but less popular packages of other types. One interpretation consistent with these results is that coders use popular features of same-type packages to include in their own code but use infrastructure-style code from different-type packages, which is not necessarily very popular. This interpretation is also consistent with the coefficients β4 and β5 for Num.Contributors and Maturity, which are positive within types and negative between-type packages. Lastly, the coefficient β3 for Num.Forks is negative both within and across projects, but not only significantly for between-type links. This could be a mechanical effect, though: If a project has more forks, there are more similar versions of the same package, meaning that a coder has more possibilities to link to it. 5 contagious vulnerabilities A growing literature explores contagion on networks, including the statistical literature on infectious diseases (e.g., the contagion of HIV and coronaviruses). Such models (see, e.g., Schweinberger et al., 2022; Groendyke et al., 2012; Britton and O’Neill, 2002) usually first use a network formation model to generate a network of contacts among agents and then study how an infectious disease following a stochastic process—like the susceptible-infected-recovered model (Kermack and McKendrick, 1972; Hethcote, 2000)—spreads within this network. We adapt these ideas to the contagion of vulnerabilities in software dependency graphs. The prevalence of vulnerabilities in software have led to the creation of the Common Vulnerabilities and Exposure (CVE) program and the National Vulnerability Database (NVD) by the National Institute of Standards and Technology, which is an agency of the United States Department of Commerce (NIST, 2024). A software package depending on a vulnerable package is potentially vulnerable itself. To see how, consider, for example, the Equifax data breach (Federal Trade Commission, 2024) in 2017, caused by a vulnerability in the popular open source web application framework Apache Struts 2.23 Equifax did not itself develop or maintain Apache Struts 2, but instead used it as part of its own code base. The vulnerability allowed attackers to execute malicious code providing access to the server hosting the web application developed by Equifax using Apache Struts 2 and costing the company more than $1.5 Billion to date.24
Package managers are part of a large suite of tools that help systems administrators to keep their systems up to date and patch any known vulnerabilities. It takes time for vulnerable systems to be updated (Edgescan, 2022),25 not all vulnerabilities are publicly disclosed (Arora et al., 2008),
23Apache Struts 2 did not properly validate the Content-Type header of incoming http requests when processing file uploads, which allowed attackers to create http requests that include malicious code. Due to the lack of content validation, this allowed attackers to execute arbitrary code on the web server, ultimately granting them full control of the server. For details see the official NIST announcement on CVE-2017-5638 (https://nvd.nist.gov/vuln/ detail/CVE-2017-5638). 24See https://www.bankinfosecurity.com/equifaxs-data-breach-costs-hit-14-billion-a-12473. Article accessed 27 January 2024. 25It takes organizations between 146 and 292 days to patch cyber security vulnerabilities (Source: Statista. Accessed on 2024-01-31). and it could even be optimal to release vulnerable software packages (Arora et al., 2006). This leaves time for attackers to exploit vulnerabilities in downstream software packages and thereby attack the systems hosting code that depends on the vulnerable packages. Since it is not ex-ante clear which part of the dependency code is vulnerable, we assume that package i uses vulnerable code in dependency j with probability ρi j ∈ [0,1] and, for simplicity, we take ρi j = 1. This avoids relying on statistical measures of the extent of contagion. The resulting contagion process is very similar to the traditional susceptible-infected-recovered (SIR) model in epidemiology (Kermack and McKendrick, 1972; Hethcote, 2000) applied to contact networks.26
How contagious a vulnerability is can be measured in this setting by the number of downstream repositories it affects. While github and other repository hosting services make it easy to see the number of packages directly depending on a given repository, direct dependencies are only imperfect proxies for the total number of vulnerable repositories two or three steps away. Formally, the d-neighborhood N di (g ) of node i in network G is defined via:
N 1i (g ) = Ni (g ) , and N ki (g ) =N k−1i (g )∪  ⋃
j∈N k−1i (g ) N j (g )  where g is the adjacency matrix of G and Ni (g ) = { j ∈ G |gi j = 1} is the (in)-neighborhood of node i . Using this, we define a repository’s k-step systemicness as Syst.ki (g ) ≡ N ki (g ). We usually omit the network reference when it is clear from context which network we study and throughout this section we use the dependency graph of the Rust Cargo ecosystem. Table 6 shows the distribution of the k-step systemicness for the full sample of all repositories (Panel A) and for only the 1% of repositories with the highest in-degree. The most immediate result is that the distribution of k-step systemicness is highly skewed. While, on average, a vulnerable repository renders 52 repositories two steps away vulnerable, the most systemic repository affects 26,631 other repositories two steps away. In other words, a vulnerability in the most systemic repository would affect 75% of all existing repositories within two steps. 26See, among others, Rocha et al. (2023). This considerable heterogeneity across packages can also be seen from the standard deviation, which is almost twelve times as large as the mean. As we take into account how a vulnerable repository affects other repositories further away, the mean increases by a factor of four for k = 5, while the maximum reaches 31,951. Table 7 shows the systemicness of the ten repositories with the highest number of dependent packages (in-degree). Among those, serde-rs/serde is the package that has the largest impact two and three steps away, while rust-lang/libc is the package that has the largest impact four and five steps away. Now that we have a measure for how “infectious” a vulnerable software package is, we turn to the question of how to prevent the spread of vulnerabilities, staying with our analogy to epidemiological SIR models. Chatterjee and Zehmakan (2023) compare different vaccination strategies in network-based SIR models. Since it is not feasible to find an optimal vaccination strategy—the optimization problem is NP hard—the authors compare various heuristics based on a node’s network position and pathogen parameters. They find that a vaccination strategy
based on a node’s betweenness centrality and a heuristic they call expected fatality is most effective in preventing fatalities. The process of how a vulnerability in one software package affects other packages, as we discuss it above, is a special case of a SIR process. First, by setting ρi j = 1∀i , j ∈ N , we assume that all upstream neighbors of a vulnerable node (repository) are also vulnerable. And second, nodes cannot “die” in our setup, so they are not removed from the population. They can, in principle, recover from being exposed to a vulnerable package if the vulnerability is patched and the dependency is updated, but this takes time, as discussed above. Consequently, software systems are vulnerable for a period of time once a vulnerability is discovered. We are interested in contagion processes occurring during this time. Consequently, we study the effectiveness of a prevention strategy based on securing the most important nodes against vulnerabilities (e.g. by through government funding or regulation). Chatterjee and Zehmakan (2023) propose to use an equally-weighted combination of a node’s betweenness and expected fatality e f (i ) of node i , which in our case is computed as:
e f (i ) = ∑ j∈N (i ) 1 |N ( j )| . Next, we apply this method to the Rust Cargo ecosystem. We normalize betweenness centrality and expected fatality by dividing through the maximum of each so that they are more directly comparable. This gives us a value of expected systemicness for each repository i . We measure how effective this measure can be used to prevent vulnerability contagion by taking the ten and one hundred repositories with the largest expected systemicness and assuming that so much debugging, exception handling and error preventing effort is exerted that these repositories are invulnerable even if they depend on a vulnerable repository. We then compute the average kstep systemicness again and show it in Figure 2. Protecting l nodes is effective if it reduces the k-step systemicness by more than l , i.e. if there are amplification effects from the protection. We can take away five things from Figure 2. First, using the expected systemicness as an exante measure of importance is effective for k ≥ 3.27 Second, the effect of protecting only ten repositories can be significant. For k = 5, protecting the ten repositories with the largest expected systemicness reduces the average systemicness (the extent of vulnerability contagion) from 215.6 to 130.4, i.e. by almost 40%.28 Third, the effect is even more pronounced when protecting 100 repositories, but with diminishing returns. The average systemicness for k = 5 is reduced to 32.4, which is an 85% reduction. However, protecting 100 repositories reduces the amplification effect only marginally more than protecting 10 repositories. Fourth, we compare the expected systemicness with an alternative measure of a node’s importance to get a sense of how effective the refined measure is relative to other measures. Specifically, we use a node’s in-degree as a measure of expected systemicness and show this in blue in Figures 2 and A2. For k = 5, the in-degree reduces systemicness by about 10% less when the 10 nodes with the largest number of dependent repositories is protected relative to when the ten repositories with the highest expected systemicness are protected. Lastly, as we can see from Figure A2, where we restrict the sample to the 1% of repositories with the greatest number of dependent repositories, that the results are qualitatively similar and quantitatively even more pronounced. 27We show the k-step systemicness for the 1% of repositories with the largest number of dependents in Figure A2 in Appendix A. For those repositories, the expected systemicness is efficient for any k. 28Or 35% when only considering the amplification effect of the protection, i.e. without counting the 10 initially protected repositories. 6 conclusion The network of dependencies among software packages is an interesting laboratory to study network formation and externalities. Indeed, the creation of modern software gains efficiency by re-using existing libraries; on the other hand, dependencies expose new software packages to bugs and vulnerabilities from other libraries. This feature of the complex network of dependencies motivates the interest in understanding the incentives and equilibrium mechanisms driving the formation of such networks. In this paper, we estimate a directed network formation model to undertake a structural analysis of the motives, costs, benefits and externalities that a maintainer faces when developing a new software package. The empirical model allows us to disentangle observable from unobservable characteristics that affect the decisions to form dependencies to other libraries. We find evidence that coders create positive externalities for other coders when creating a link. This raises more questions about the formation of dependency graphs. We study how vulnerable the observed network is to vulnerability contagion and show that ensuring that even a relatively small number of packages is vulnerability free effectively curbs the extent of contagion. As a next step, we could study how changing the parameters of the estimated network formation model affects vulnerability contagion. The presence of an externality implies that maintainers may be inefficiently creating dependencies, thus increasing the density of the network and the probability of contagion. The extent of this risk and the correlated damage to the system is of paramount importance and we plan to explore it in future versions. Second, while we have focused on a stationary realization of the network, there are important dynamic considerations in the creation of these dependencies. While the modeling of forward-looking maintainers may be useful to develop intuition about intertemporal strategic incentives and motives, we leave this development to future work. Finally, we have focused on a single language, but the analysis can be extended to other languages as well, such as Java, C++ and others. a computational details for estimation Estimation of our model is challenging because of the normalizing constants ckk in the likelihood and the discrete mixture model for the block assignments. We bypass these issues by extending recently developed approximate two-step estimation methods to directed networks, estimating the structural parameters in two steps. Formally, the full likelihood of our model can be written as follows
L (g,x;α,β,γ,η) = ∑ z∈Z L ( g,x,z;α,β,γ,η )= ∑ z∈Z pη (Z = z) π(g,x,z;α,β,γ). (9)
where each firm’s type is i.i.d. multinomial, so the distribution pη(z) is
Zi |η1, ...,ηK iid∼ Multinomial ( 1;η1, ...ηK ) for i = 1, ..., N
Since maximizing (9) involves a sum over all possible type allocations of each node as well as a nested normalizing constant in π(g,x,z;α,β,γ), evaluating the function is infeasible. To ameliorate this issue, we divide estimation into two parts. First, we estimate the type of each node and, second, we estimate the parameters α,β, and γ conditional on the type of each node. This two-step algorithm exploits the fact that our model corresponds to a stochastic blockmodel with covariates when γ= 0, i.e. when there are no link externalities. A.1 STEP 1: Approximate estimation of unobserved block structure
A.1.1 Variational EM algorithm with MM updates
To recover the types and the block structure of the network, we approximate the model using a stochastic blockmodel. Define
L0 ( g,x,z;α,β,η ) := pη(z)π(g,x,z;α,β,γ= 0) (10)
as the likelihood of a stochastic blockmodel, which means that
1. Each node belongs to one of K blocks/types;
2. Each link is conditionally independent, given the block structure
Then under some conditions – namely, that the network is large and each block/type is not too large with respect to the network (Babkin et al., 2020; Schweinberger, 2020; Schweinberger and Stewart, 2020) – we have
L ( g,x,z;α,β,γ,η )≈ L0 (g,x,z;α,β,η) (11) Variational methods for the stochastic blockmodel are relatively standard and involve estimating an approximating distribution qξ(z) that minimizes the Kulback-Leibler divergence from the true likelihood. This can be achieved in several ways, but usually, the set of distributions is restricted to the ones that can be fully factorized to simplify computations. Then the log-likelihood of our model stated in (9) can be lower bounded as follows. Let qξ(z) be the auxiliary variational distribution characterized by parameter ξ approximating the distribution pη(z), then the log-likelihood has a lower-bound, calculated as follows:
ℓ(g,x,α,β,γ,η) := log ∑ z∈Z L ( g,x,z;α,β,γ,η ) ≈ log ∑
z∈Z L0
( g,x,z;α,β,η ) = log ∑
z∈Z qξ(z)
L0 ( g,x,z;α,β,η ) qξ(z)
≥ ∑ z∈Z qξ(z) log [ L0 ( g,x,z;α,β,η ) qξ(z) ] = ∑
z∈Z qξ(z) logL0
( g,x,z;α,β,η )− ∑ z∈Z qξ(z) log qξ(z)
= Eq logL0 ( g,x,z;α,β,η )+H(q) =: ℓB (g,x,α,β,η;ξ). (12)
where
H(q) =− ∑ z∈Z qξ(z) log qξ(z)
is the entropy of auxiliary distribution qξ(z). In the third row, we multiply and divide by qξ(z) and apply Jensen’s inequality in the consecutive line. The best lower bound is obtained by choosing qξ(z) from the set of distributions Q that solves the following variational problem
ℓ(α,β,η) = sup ξ∈ [0,1]N×K ℓB (g,x,α,β,η;ξ)
However, this variational problem is usually intractable unless we impose more structure on the problem. In practice, researchers restrict the set Q to a smaller set of tractable distributions (Wainwright and Jordan, 2008; Mele and Zhu, forthcoming). In the case of the stochastic blockmodel, it is useful and intuitive to restrict Q to the set of multinomial distributions
Zi ind∼ Multinomial(1;ξi 1, ...ξi K ) for i = 1, ...,n
with ξi being the variational parameters. We collect the vectors of variational parameters in the matrix ξ. This leads to a tractable lower bound that can be written in closed-form
ℓB (g,x,α,β,η;ξ) ≡ ∑ z∈Z qξ(z) log [ L0 ( g,x,z;α,β,η ) qξ(z) ] (13)
= N∑
i=1 N∑ j=1 K∑ k=1 K∑ l=1 ξi kξ j l log πi j ,kl (gi j ,x)
+ N∑
i=1 K∑ k=1 ξi k ( logηk − logξi k ) where the function πi j ,kl is the conditional probability of a link between i and j of types k and
l , respectively,
log πi j ,kl (gi j ,x) := gi j log [ exp [ ui j ,kl (α,β)+u j i ,lk (α,β) ] 1+exp[ui j ,kl (α,β)+u j i ,lk (α,β)] ] (14)
+ (1− gi j ) log [
1 1+exp[ui j ,kl (α,β)+u j i ,lk (α,β)] ]
and
ui j ,kl (α,β) = u(xi ,x j , zi k = z j l = 1,z;α,β)
Note that gi i := 0 for all i by definition. The covariate information x encodes in our setting categorical information on the level of repositories. We, therefore, introduce for the pair of repositories (i , j ) the covariate vector xi j = (xi j ,1, . . . , xi j ,p ) encoding in the kth entry whether the respective repositories match on the kth covariate. Observing (15), we can write πkl (gi j ,xi j ,z) = πi j ,kl (gi j ,x) by dropping the dependence on the particular pair of repositories and the parameters α and β. Generally, we iteratively maximize (14) in terms of η,α, and β conditional on ξ and then the other way around. We denote the values of ξ,η,α, and β in the t th iteration by ξ(s),η(s),α(s), and β(s) and the two steps comprising the Variational Expectation Maximization algorithm are:
Step 1: Given ξ(s), find α(s+1) and β(s+1) satisfying:
ℓB (g,x,α (s+1),β(s+1),η(s+1);ξ(s)) ≥ ℓB (g,x,α(s),β(s),η(s);ξ(s));
Step 2: Given α(s+1) and β(s+1), find ξ(s)+1 satisfying:
ℓB (g,x,α (s+1),β(s+1),η(s+1);ξ(s+1)) ≥ ℓB (g,x,α(s+1),β(s+1),η(s+1);ξ(s)). Step 1: Taking first order conditions with respect to each parameter implies the following closed-form update rules for η, and π(s+1)kl (d , x1, . . . , xp ,z) follow
η(s+1)k := 1
n n∑ i=1 ξ(s+1)i k , k = 1, . . . ,K ,
and
π(s+1)kl (d , x1, . . . , xp ,z) := ∑n i=1 ∑ j ̸=i ξ (s+1) i k ξ (s+1) j l 1{gi j = d , X1,i j = x1, . . . , Xp,i j = xp }∑n
i=1 ∑ j ̸=i ξ (s+1) i k ξ (s+1) j l 1{X1,i j = x1, . . . , Xp,i j = xp }
,
for k, l = 1, . . . ,K and d , x1, . . . , xp ∈ {0,1}, respectively. The variables Xp,i j := 1{xi p = x j p } are indicators for homophily. Generalizations are possible, as long as we maintain the discrete nature of the covariates x. Including continuous covariates is definitely possible, but it may result in a significant slowdown of this algorithm. We note that when running the Variational EM (VEM) algorithm, we are not interested in estimating the parameters α, β and γ, but only the estimation of probabilities πkl (g ,x,z) for g ∈ {0,1} and x ∈ X , where X is the set of all possible outcomes of the categorical covariates used in Step 1. This feature allows us to speed up the computation of several orders of magnitude (Dahbura et al., 2021). Step 2: For very large networks, maximizing the lower bound in the second step with respect to ξ for given estimates of η,α, and β is still cumbersome and impractically slow. We thus
borrow an idea from Vu et al. (2013) and find a minorizing function M ( ξ;g,x,α,β,η,ξ(s) ) for the lower bound. This consists of finding a function approximating the tractable lower bound
ℓB (g,x,α,β,η;ξ), but simpler to maximize. Such function M ( ξ;g,x,α,β,η,ξ(s) ) minorizes ℓB (g,x,α,β,η;ξ) at parameter ξ(s) and iteration s of the variational EM algorithm if
M ( ξ;g,x,α,β,η,ξ(s) ) ≤ ℓB (g,x,α,β,η;ξ) for all ξ M ( ξ(s);g,x,α,β,η,ξ(s)
) = ℓB (g,x,α,β,η;ξ(s)) where α,β,η and ξ(s) are fixed. Maximizing the function M guarantees that the lower bound does not decreases. Extending the approach of Vu et al. (2013) to directed networks, the follow-
ing expression minorizes ℓB (g,x,α,β,η;ξ)
M ( ξ;g,x,α,β,η,ξ(s) ) := N∑ i=1 N∑ j ̸=i K∑ k=1 K∑ l=1 ξ2i k ξ(s)j l 2ξ(s)i k +ξ2j l ξ(s)i k 2ξ(s)j l  log πkl (gi j ,xi j ,z) (15) +
N∑ i=1 K∑ k=1 ξi k ( logη(s)k − logξ(s)i k − ξi k
ξ(s)i k
+1 ) . In Section A.1.2, we detail how sparse matrix multiplication can be exploited to speed up the updates of ξ(s+1)
We run this algorithm until the relative increase of the variational lower bound in below a certain threshold to obtain estimates for ξ̂ and η̂. We then assign each node to its modal estimated type, such that ẑi k = 1 if ξ̂i k ≥ ξ̂iℓ for all ℓ ̸= k and for all i s.
A.1.2 Exploiting sparse matrix operations for updating variational parameters
Next, we detail how the updates of ξ in Step 1 can be carried out in a scalable manner based on sparse matrix operations. Equation (15) can be written as a quadratic programming problem in
ξ with the side constraints that ∑K
k=1ξi ,k = 1 for all i = 1, . . . , N needs to hold. Therefore, we first rearrange the first part of (15):
N∑ i=1 N∑ j ̸=i K∑ k=1 K∑ l=1 ξ2i k ξ(s)j l 2ξ(s)i k +ξ2j l ξ(s)i k 2ξ(s)j l  log πkl (gi j ,xi j ) =
N∑ i=1 K∑ k=1 N∑ j ̸=i K∑ l=1 ξ2i k 2ξ(s)i k ξ(s)j l { log πkl (gi j ,xi j )+ log π(s)lk (g j i ,x j i ) } =
N∑ i=1 K∑ k=1 Ω(s)i k (g,x,ξ (s)) 2ξ(s)i k ξ2i k
with
Ω(s)i k ( g,x,ξ(s) ) := N∑ j ̸=i K∑ l=1 ξ(s)j l { log π(s)kl (gi j ,xi j )+ log π(s)l k (g j i ,x j i ) } . (16)
This yields for (15)
M ( ξ;g,x,α,β,η,ξ(s) )= N∑ i=1 K∑ k=1
( Ω(s)i k (g,x,ξ (s))
2ξ(s)i k − 1 ξ(s)i k
) ξ2i k + ( logη(s)k − logξ(s)i k +1 ) ξi k
= N∑
i=1 K∑ k=1 A(s)i k ( g,x,ξ(s) ) ξ2i k +B (s)i k ( g,x,ξ(s) ) ξi k
where
A(s)i k ( g,x,ξ(s) ) := Ω (s) i k
( g,x,ξ(s) ) 2ξ(s)i k − 1 ξ(s)i k
is the quadratic term and
B (s)i k ( g,x,ξ(s) ) := logη(s)k − logξ(s)i k +1
the linear term of the quadratic problem. To update the estimate of ξ, we need to evaluate A(s)i k ( g,x,ξ(s) ) and B (s)i k ( g,x,ξ(s) ) for i = 1, . . . , N
and k = 1, . . . ,K , which, when done naively, is of complexity O(N 2 K 2). ComputingΩ(s)i k ( g,x,ξ(s) ) for A(s)i k ( g,x,ξ(s) ) is the leading term driving the algorithmic complexity. It is thus prohibitively difficult for a large number of population members and communities in the network. Note that in our application, this problem is exasperated as we have more than 35,000 nodes and our theoretical result on the adequacy of the estimation procedure assumes that K grows as a function
of N . To avoid this issue, we show howΩ(s)i k ( g,x,ξ(s) ) can be evaluated through a series of matrix
multiplications. The underlying idea of our approach is thatΩ(s)i k ( g,x,ξ(s) ) has an easy form if g is an empty network with all pairwise covariates set to zero, i.e., gi j = 0 and xq = 0∀ i , j = 1, . . . , N and q = 1, . . . , p. Given that, in reality, these are seldom the observed values, we, consecutively, go through all connections where either gi j = 1 or Xi j ,1 = x1, . . . , Xi j ,p = xp for ∑pq=1 xq ̸= 0 holds
and correct for the resulting error. In formulae, we employ the following decomposition:
Ω(s)i k (g,x,ξ (s)) := N∑ j ̸=i K∑ l=1 ξ(s)j l { log π(s)kl (gi j ,xi j )+ log π(s)lk (g j i ,x j i ) } =
N∑ j ̸=i K∑ l=1 ξ(s)j l { log π(s)kl (0,0)+ log π(s)l k (0,0) } +
N∑ j ̸=i K∑ l=1
[ gi jξ (s) j l { log π(s)kl (1,xi j )
π(s)kl (0,0)
} + g j iξ(s)j l { log π(s)l k (1,x j i )
π(s)lk (0,0)
}]
+ N∑
j ̸=i K∑ l=1 [ (1− gi j )ξ(s)j l { log π(s)kl (0,xi j )
π(s)kl (0,0)
} + (1− g j i )ξ(s)j l { log π(s)lk (0,x j i )
π(s)l k (0,0)
}]
=Ω(s)i k ( 0,0,ξ(s) )+Λ(s)i k (g,x,ξ(s)) , (17) with
Λ(s)i k ( g,x,ξ(s) )= N∑ j ̸=i K∑ l=1
[ gi jξ (s) j l { log π(s)kl (1,xi j )
π(s)kl (0,0)
} + g j iξ(s)j l { log π(s)lk (1,x j i )
π(s)lk (0,0)
}] (18)
+ N∑
j ̸=i K∑ l=1 [ (1− gi j )ξ(s)j l { log π(s)kl (0,xi j )
π(s)kl (0,0)
} + (1− g j i )ξ(s)j l { log π(s)l k (0,x j i )
π(s)lk (0,0)
}]
being the error in Ω(s)i k ( g,x,ξ(s) ) arising from assuming an empty network with all categorical
covariates set to zero. Following, we show how both terms, Ω(s)i k ( 0,0,ξ(s) ) and Λ(s)i k ( g,x,ξ(s) ) , can be computed through matrix operations, completing our scalable algorithmic approach. We, first, assume an empty network with all categorical covariates set to zero, gi j = xq = 0 for all i ̸= j and q = 1, ..., p, to computeΩ(s)i k ( 0,0,ξ(s) ) through matrix multiplications. Observe that
Ω(s)i k ( 0,0,ξ(s) )= N∑ j ̸=i K∑ l=1 ξ(s)j l ( log π(s)kl (0,0)+ log π(s)lk (0,0) ) =
K∑ l=1 N∑ j ̸=i ξ(s)j l ( log π(s)kl (0,0)+ log π(s)lk (0,0) ) =
K∑ l=1
( N∑
j=1 ξ(s)j l −ξ(s)i l
)( log π(s)kl (0,0)+ log π(s)lk (0,0) ) =
K∑ l=1 ( τ(s)l −ξ(s)i l )( log π(s)kl (0,0)+ log π(s)lk (0,0) )
holds, where
τ(s)l := N∑
j=1 ξ(s)j l . With
A(s)0 :=  τ(s)1 −ξ(s)11 τ(s)2 −ξ(s)12 . . . τ(s)K −ξ(s)1K τ(s)1 −ξ(s)21 τ(s)2 −ξ(s)22 . . . τ(s)K −ξ(s)2K ... ... . . . ...
τ(s)(1)−ξ(s)n1 τ(s)2 −ξ(s)n2 . . . τ(s)K −ξ(s)nK  and
Π(s)0 :=  log π(s)11 (0,0) log π (s) 12 (0,0) . . . log π (s) 1K (0,0) log π(s)21 (0,0) log π (s) 22 (0,0) . . . log π (s) 2K (0,0) ... ... . . . ...
log π(s)K 1(0,0) log π (s) K 2(0,0) . . . log π (s) K K (0,0)
 ,
it follows that
A(s)0 {( Π(s)0 )⊤+Π(s)0 }= (Ω(s)i k (0,0,ξ(s)))i k holds. Put differently, we are able to write Ω(s)i k (0,0,ξ) as the (i ,k)th entry of multiplying A (s) 0 andΠ(s)0 (z) ⊤+Π(s)0 (z). The matrix π(s)(1,0) can be computed via sparse matrix operations:
π(s+1)(1,0) = {( ξ(s+1) )⊤ g ◦ (J −X1)◦ · · · ◦ (J −Xp ) ( ξ(s+1) )}⊘ (19){( ξ(s+1) )⊤ (J −X1)◦ · · · ◦ (J −Xp ) ( ξ(s+1) )} ,
where A ◦B denotes the Hadamard (i.e., entry-wise) product of the conformable matrices A and B, J is a N × N matrix whose off-diagonal entries are all one and whose diagonals are all zero. We follow the approach Dahbura et al. (2021) to calculate (19) without breaking the
sparsity of the covariate matrices and g. We can then set
Π(s)0 (z) = log ( 1−π(s+1)(d = 1, X1 = 0, . . . , Xp = 0) ) and calculateΩ(s)i k ( 0,0,ξ(s) ) for i = 1, . . . , N and k = 1, . . . ,K . Next, we correct for the error arising from assuming that g = 0 and x = 0 holds. Since all covariates are assumed to be categorical, the pairwise covariate vector xi j can have at most 2p possible outcomes, which we collect in the set X . Given this, one may rewrite the sum over j ̸= i as a sum over all possible outcomes of the pairwise covariate information:
Λ(s)i k ( g,x,ξ(s) ) = N∑ j ̸=i K∑ l=1
[ gi jξ (s) j l { log π(s)kl (1,xi j )
π(s)kl (0,0)
} + g j iξ(s)j l { log π(s)lk (1,x j i )
π(s)l k (0,0)
}]
+ N∑
j ̸=i K∑ l=1 [ (1− gi j )ξ(s)j l { log π(s)kl (0,xi j )
π(s)kl (0,0)
} + (1− g j i )ξ(s)j l { log π(s)lk (0,x j i )
π(s)l k (0,0)
}]
= ∑ x∈X N∑ j ̸=i K∑ l=1 gi j I(xi j =x)ξ(s)j l { log π(s)kl (1,x)
π(s)kl (0,0)
}
+ g j i I(x j i =x)ξ(s)j l { log π(s)lk (1,x)
π(s)lk (0,0)
} + (1− gi j ) I(xi j =x)ξ(s)j l { log π(s)kl (0,x)
π(s)kl (0,0)
}
+ (1− g j i ) I(x j i =x)ξ(s)j l { log π(s)lk (0,x)
π(s)lk (0,0)
}
With
Π(s) ( d ,x,ξ(s) ) :=  log π11(d ,x) π11(d ,0) log π12(d ,x) π12(d ,0) . . . log π1K (d ,x) π1K (d ,0) log π21(d ,x)π21(d ,0) log π22(d ,x) π22(d ,0) . . . log π2K (d ,x)π2K (d ,0) ... ... . . . ...
log πK 1(d ,x)πK 1(d ,0) log πK 2(d ,x) πK 2(d ,0) . . . log πK K (d ,x)πK K (d ,0)
 . and the functions Γ : {0,1} →Rn×n andΛs : {0,1} →Rn×n (s = 1, . . . , p) such that
Γ(d) :=  G (d = 1) J −G (d = 0)
and
∆(x) :=Xx11 ◦ (J −X1)1−x1 ◦ · · · ◦X xp p ◦ (J −Xp )1−xp
we can write
Λ(s)i k ( g,x,ξ(s) )=( ∑ x∈X ∑ d∈{0,1} Γ(d)◦∆(x)ξ(s)Π(s) (d ,x,ξ(s))+ (Γ(d)◦∆(x))⊤ ξ(s)Π(s) (d ,x,ξ(s))⊤) i k
Extending (19) to generic covariate values x yields
π(s+1)(1,x) ={(ξ(s+1))⊤g ◦Xx11 ◦ (J −X1)1−x1 ◦ · · · ◦Xxpp ◦ (J −Xp )1−xp (ξ(s+1))}⊘{( ξ(s+1) )⊤ Xx11 ◦ (J −X1)1−x1 ◦ · · · ◦X xp p ◦ (J −Xp )1−xp ( ξ(s+1) )} ,
enabling the evaluation ofΠ(s) ( d ,x,ξ(s) ) for x ∈X . Following Dahbura et al. (2021), we can still evaluate this matrix by sparse matrix operations. In sum, we have shown how to compute the termsΩ(s)i k ( 0,0,ξ(s) ) andΛ(s)i k ( g,x,ξ(s) ) through matrix multiplications. Plugging in these terms into (17) enables a fast computation ofΩ(s)i k (g,x,ξ (s)), which is the computational bottleneck for evaluating the quadratic term needed to update ξ(s) to ξ(s+1). A.2 STEP 2: Estimation of structural utility parameters
In the second step, we condition on the approximate block structure estimated in the first step ẑ and estimate the structural payoff parameters. We compute the conditional probability of a link within types and between types
pi j (g,x,α,β,γ; ẑ) =  Λ ( ui j (αw ,βw )+u j i (αw ,βw )+4γ∑r ̸=i , j Ii j r g j r gi r ) if ẑi = ẑ j Λ ( ui j (α,β)+u j i (α,β) ) otherwise
where Ii j r = 1 if ẑi = ẑ j = ẑr and Ii j r = 0 otherwise; and Λ(u) = eu/(1+ eu) is the logistic function. The maximum pseudolikelihood estimator (MPLE) solves the following maximization problem
(α̂,β̂, γ̂) = arg max α,β,γ ℓPL(g,x,α,β,γ; ẑ)
= arg max α,β,γ n∑ i ̸= j [ gi j log pi j (g,x,α,β,γ; ẑ)+ (1− gi j ) log(1−pi j (g,x,α,β,γ; ẑ)) ] In practice the estimator maximizes the log of the product of conditional probabilities of linking. The asymptotic theory for this estimator is in Boucher and Mourifie (2017) and Stewart and Schweinberger (2023). It can be shown that the estimator is consistent and asymptotically normal. As long as the estimator forz provides consistent estimates, the estimator for the structural parameters is well-behaved. b additional figures and estimates Figure A1: Value of the lower bound of the likelihood in the first step of our estimation-
−4500000
−4000000
−3500000
−3000000
−2500000
−2000000
0 100 200 300 Iteration (s)
V al
ue o
f t he
L ow
er B
ou nd
o f t
he L
ik el
ih oo
d
Figure A2: Comparison of k-step systemicness for the 1% of repositories in the Rust Cargo ecosystem with the highest number of dependent repositories (in-degree). The k-step systemicness measures how many repositories the average vulnerable repository renders vulnerable k steps away. The solid black line shows the average k-step systemicness when no node is protected, while the dashed and dot-dashed black lines show the average k-step systemicness when the 10 and 100 most systemic repositories are perfectly protected against vulnerabilities, respectively. The dashed and dot-dashed blue line shows the average k-step systemicness when systemicness for protected nodes are measued as the number of depdents a repository has (indegree).