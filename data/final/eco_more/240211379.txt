We propose an indirect inference strategy for estimating heterogeneous-agent business cycle models with micro data. At its heart is a first-order vector autoregression that is grounded in linear filtering theory as the cross-section grows large. The result is a fast, simple and robust algorithm for computing an approximate likelihood that can be easily paired with standard classical or Bayesian methods. Importantly, our method is compatible with the popular sequence-space solution method, unlike existing state-of-the-art approaches. We test-drive our method by estimating a canonical HANK model with shocks in both the aggregate and cross-section. Not only do simulation results demonstrate the appeal of our method, they also emphasize the important information contained in the entire micro-level distribution over and above simple moments. 1 introduction Over the past decade, the tremendous progress in developing models featuring rich heterogeneity with aggregate shocks has coincided with the proliferation of innovative and novel datasets at the household or individual level. These micro data were originally used to calibrate model parameters by matching cross-sectional moments of relevant variables (e.g. asset holdings, marginal propensity to consume, amongst others) at the model’s stationary equilibrium. Subsequent advancements in computational methods opened the door to formal parameter estimation using aggregate timeseries data.1 Until recently however, the ability to leverage similar informational content in the entire distributions of the micro data to discipline the dynamics of these models remained elusive.2
This paper contributes to that literature by proposing an indirect inference strategy with a simple, fast and effective algorithm for approximating the model-implied likelihood of repeated cross-sections of micro data. The algorithm can easily be paired with a maximization routine for maximum-likelihood estimation, or any MCMC posterior sampling algorithm for Bayesian estimation. One differentiating feature of our strategy is its compatibility with sequence-space solution (Auclert et al. 2021a, Boppart et al. 2018), since most available methods for estimating models with micro-data require a solution in state-space form.3
The method relies on two key assumptions. The first is that the number of units (e.g households or individuals) in the repeated cross-sections are large. In other words, our data must be high dimensional. This a natural property of micro-data. The second is that the dynamics of the heterogeneous-agent model is approximately low rank – that the dynamics of the high-dimensional vector of observables can be well-approximated by relatively few factors. This appears to be the case in even the more complex heterogeneous-agent models of today. Under these two assumptions, indirect inference consists of treating a Dynamic Factor Model (DFM) as auxiliary to the intractable heterogeneous agent model of interest. Even so, the assumed high-dimensional nature of the data may render the DFM itself intractable. By leveraging and extending insights in Sargent
1For example, Bayer, Born, and Luetticke (2024) estimate a medium-scale HANK model in state space using a Bayesian approach. Auclert, Rognlie, and Straub (2020) estimate a HANK model in sequence space by matching the impulse response to identified monetary policy shocks. Both approaches use only aggregate time-series data or time-series of cross-sectional moments. 2To our best knowledge, the only available method is the full-information approach developed by Liu and Plagborg-Møller (2023). 3Examples of state-space solution methods include Reiter (2009), Winberry (2018), Bayer and Luetticke (2020), and Ahn, Kaplan,
Moll, Winberry, and Wolf (2017) for continuous-time models
and Selvakumar (2023), we show that the likelihood implied by a reduced-rank first-order vector autoregression in the observables is an unbiased estimate for that of the DFM. This result paves the way to approximate the likelihood of the DFM and therefore the model of interest. We implement that computation using the Dynamic Mode Decomposition, a workhorse tool in the fluid dynamics literature, which provides a consistent estimate of the reduced-rank first-order VAR coefficients.4 Figure 1 summarizes the key idea of our indirect inference strategy. Existing methods To our best knowledge, there exist two ways to estimate a model solved with sequence-space methods. The first is to directly use the moving average representation, as employed by Auclert et al. (2021a). This approach is primarily suited for situations in which the observables are either aggregate data or only a few moments of the cross-sectional distribution. Using the entire distribution renders it infeasible. To see why, note that the MA representation approach requires stacking the full MT × 1 observation vector, where M is the number of units in the cross-section and T the number of periods in time. The associated covariance matrix is therefore of dimension MT ×MT . For a conservative M = 300 and T = 120, the dimensions of the covariance matrix are 36, 000× 36, 0005. The second is the use of Whittle likelihood approximation in frequency domain, as in Hansen and Sargent (1981) and Christiano and Vigfusson (2003). While estimation is feasible, its quality of the approximation relies on large T , which is unrealistic for existing micro-datasets. In contrast, our method relies on large M , which seems to us a far more satisfiable requirement. 4See Brunton et al. (2015) and Brunton and Kutz (2022) for a reader’s guide. 5For further discussion, see Auclert et al. (2021b) pp 28, footnote 31. Application We illustrate our methodology by considering a small-scale HANK model that features both aggregate and cross-sectional shocks. We solve the model using the sequence-space Jacobian method of Auclert et al. (2021a) and demonstrate the finite-sample properties of our method via Monte-Carlo simulation. We compare finite-sample properties from our method (MicroDMD) with that of an estimation using only aggregate data (Agg). We find that MicroDMD has superior finite-sample properites over Agg, demonstrating the value of information contained in micro data6. We also implement an estimation using aggregate data plus a few cross-sectional moments (Agg+). Though the finite-sample properties improve over Agg, MicroDMD appears superior yet again, suggesting the existence of additional informational content beyond simple cross-sectional moments of the micro-data. Finally, we compare the results of MicroDMD compared to an estimation using Whittle Likelihood MicroFD. Again, we document that MicroDMD has more favorable finite-sample properties than MicroFD. One reason for this is that consistency in MicroFD requires large T . The rest of the paper is structured as follows. In Section 2, we lay out our estimation framework and formally justify our method. In Section 3, we illustrate our method with a small-scale HANK model and compare its performance with other conventional approaches. Section 4 discusses the robustness of our method and extends our method for Bayesian inference. Section 5 concludes with suggestions for future research. 2 estimation framework Consider a fully-specified structural general equilibrium model with heterogeneous agents and aggregate shocks, called M. Let yi,t ∈ R denote the observable (e.g. consumption) of individual i at time t. Moreover, let yt ∈ RM×1 denote a vector of individuals’ consumption at time t.
Assumption 1. We make the following assumptions on the available micro-data{yt}Tt=1
1. The observations are repeated cross-sections of individuals from a given sampling scheme
2. The observation vector yt is high dimensional (i.e M is large)
6This finding underscores the insights of Liu and Plagborg-Møller (2023)
Assumption 2. The heterogeneous-agent model M is approximately low rank (N )
Both sets of assumptions are crucial to the theoretical results that justify our algorithm. The first condition in Assumption 1 requires that we sample individuals from the same states over time. Practically speaking, we have in mind a dataset where individuals are grouped and binned by their state variables implied by M. It is important to our theory that we do not have any ’gaps” in the dataset, i.e. that for a given grid of states, there is always at least one unit in each grid point. Note that the grid need not cover the whole state-space of the model, we only require that were it to be included in our dataset, there are no ”missing datapoints” in the time-series. We discuss extensions to this data requirement in Section 4. The second condition in Assumption 1 requires that we essentially sample ”enough” individuals. In our simulated example below, we set M = 300, which we believe is also realistic in empirical settings. The first condition in Assumption 2 implies that micro-data generated by M is approximately low rank This assumption may seem a priori, since one of the main benefits of heterogeneous-agent models is that they do not aggregate, providing rich insights into the effects of heterogeneity on macroeconomic outcomes and vice versa. Crucially however, we do not require that the model is low rank, but only that it is approximately low rank. This condition is easily verifiable for any model. For example, consider a simulated data matrix Ỹ = [ỹ1, . . . , ỹT ] ∈ RM×T from M. Taking the (reduced) singular value decomposition provides Ỹ = ŨΣ̃Ṽ ⊤ where Ũ ∈ RM×M , Σ̃ ∈ RM×M is a diagonal matrix and Ṽ ∈ RT×M . Definition 1. The singular values {s1, s2, . . . , sM} of Ỹ are the elements on the diagonal of Σ̃ and are listed in decreasing order. Ỹ is said to be approximately low rank if there exists an N such that si ≫ sj for all positive integers i ≤ N < j and N ≪ M . The definition characterizes the rankness of the matrix by its singular values. For example, in the special case that rank(Ỹ) = N then si >> 0 for all i ≤ N and si = 0 otherwise. It also suggests an intuitive way to study the rankness of the data matrix (and therefore the model that generated it) by plotting its singular values. For an approximately low rank Ỹ, there would only be a small number of large singular values, and the rest relatively small. Is this a plausible assumption for heterogeneous-agent models? We argue that it is. Figure
2 plots the singular values for data generated by four well-known heterogenous-agent models: Krusell-Smith model (Krusell and Smith 1998), a One-Asset HANK (Section 3.1), a Two-Asset HANK (Auclert et al. 2021a) and a heterogeneous-firm model (Khan and Thomas 2008). For all four models, we generate repeated cross-sectional data with M = 300 and T = 10, 000.7 The figure shows that all four models appear to be approximately low rank. Krusell-Smith appears to have N = 2, one very large singular value and one smaller one, with the rest being negligible. The One-Asset HANK model appears to have a slightly higher approximate rank. Though the Two-Asset HANK has a much larger rank than the Krusell-Smith or the One-Asset HANK models, with N = 7, it is still approximately low rank. The same applies for the model with heterogeneous firms. That even complex heterogeneous agent models feature an approximately low-rank structure suggests the generality with which Assumption 2 holds in the existing class of heterogeneous-agent models. From a theoretical standpoint, Bayer et al. (2024) provide an intuitive discussion for why one might expect the possibility of a significant dimension reduction using insights from the sequence space method.8 In Section 4.1, we provide a sufficient condition on equilibrium matrices of M for whether there exists a low-rank representation. Furthermore, Assumption 2 appears to be plausible empirically. Sargent and Selvakumar (2023) construct a dataset of quarterly time-series of percentiles of private income, post-tax income and consumption from the Consumer Expenditure Survey (CEX) and show that the data matrix is approximately low rank. The ”indirect inference” part of our strategy comes from the use of an auxiliary model to approximate M.9 We leverage Assumption 2 to consider a Dynamic Factor Model (DFM) with N factors as such a model, where the approximation quality improves the closer M is to being exactly low rank. Moreover, a question remains as to how one might calculate the likelihood of the DFM when M is large. For example, computational feasibility might preclude estimation for a larger number of observables.10 In the next section, we provide a solution that builds upon results of Sargent and Selvakumar (2023). 7A short description of the simulations can be found in Appendix C. The One-Asset HANK serves as our benchmark model and is described in Section 3.1. 8For a full discussion, see Bayer et al. (2024) Appendix C.2 9An auxiliary model is a model that well-approximates M but whose likelihood is easier to compute. 10See Section 2.2 for further discussion. 2.1 computing the likelihood of high-dimensional factor models This section further develops insights in Sargent and Selvakumar (2023) to compute the likelihood of a high-dimensional factor model with N factors. Let xt ∈ RN×1 be a vector of unobserved factors at time t = 1, . . . , T . We will suppose that they are generated by the linear state-space model
xt+1 = Axt+Cwt+1 (1)
yt = Gxt+vt,
where shocks wt+1 ∼ N (0, IN×N ), measurement errors vt ∼ N (0,R) and ws ⊥ vτ for all s, τ ; here A ∈ RN×N , C ∈ RN×N and G ∈ RM×N and R ∈ RM×M . In addition, we make the following assumptions. Assumption 3. Dynamic Factor Model (1) satisfies the following restrictions
1. M ≫ N
2. G,A has full column rank (i.e. rank(G) = rank(A) = N ) 3. ∥∥G⊤G∥∥ = O(M), where ∥·∥ denotes the Frobenius norm
4. R = σ2vIM for some σv > 0
The first condition states that a large number of observables are generated by relatively few factors. The second condition requires that the columns of G and A to be linearly independent. In
a HA model, the rows of G represents the policy function of different agents, so this condition is equivalent to assuming enough heterogeneity in the cross-section. For example, in a two-agent New Keynesian model with many shocks, rank(G) = 2 and the condition is not satisfied. This condition highlights our identification strategy which exploits the rich heterogeneity in the cross-section. The assumption on A simply means that there is no redundant state. The third condition concerns the asymptotic property of the model when the number of observables grows and is standard in the factor analysis literature (e.g. Chamberlain and Rothschild 1982, Stock and Watson 2002, Bai and Ng 2006). We have in mind an underlying ”grand” model that defines the measurement equation for each potential observable. For instance, a HANK model includes an infinite number of consumption policies, one for each household. In this context, the assumption means that the second moment of the cross-sectional consumption policy exists and the sampling of the observables is purely random such that the Law of Large Number holds. Lastly, the fourth condition is the standard assumption that the measurement error is homoscedastic. We make this for ease of exposition, though it can be loosened if necessary. Before delving into the large-M theory, let’s recall the celebrated VAR representation of linear
state-space model:
Proposition 1. There exists an infinite-order VAR representation of DFM (1) in yt, given by
yt = ∞∑ j=1 B∞j yt−j +at (2) E[at y⊤t−j ] = 0 for all j ≥ 1 E[at aTt ] =: Ω B∞j = G(A−KG)j−1K ∀j ≥ 1 (3) rank(B∞j ) = N ∀j ≥ 1
where K = AΣ∞G⊤Ω−1 and Σ∞ = CC⊤+KRK⊤+(A−KG)Σ∞(A−KG)⊤
Proposition 1 demonstrates the formula for forming the best forecast for yt, given the information up to time t− 1. In general, one needs to use the whole history yt−1 to form the forecast, and finite truncation of the history induces non-trivial efficiency loss. Nonetheless, we show that this is not necessary if the number of observables is large. Lemma 1. Under Assumption 3, as the number of observables grows (M → ∞), the matrix A−KG → 0
Corollary 1. When A−KG = 0, E[xt+1 |yt] = Kyt and E[yt+1 |yt] = GKyt
What’s the intuition? When the number of observables is large, one can estimate the hidden state xt accurately using only information contained in yt. Then by the Markovian property of the model, one can use merely the time-t information to form the best forecast for yt+1. With these preliminary results done, we now state the two main theoretical results justifying
our estimation procedure. Theorem 1. Suppose Assumption 3 holds. Then as M → ∞,
1. B∞j → 0 ∀j ≥ 2. Furthermore, lim sup(M j−1 ∥∥B∞j ∥∥) < ∞ ∀j ≥ 2
2. The infinite-order VAR representation of DFM (1) collapses to a first-order VAR representation where
yt = B ∞ 1 yt−1+at (4) E[at y⊤t−1] = 0 E[at aTt ] =: Ω B∞1 = GK ∀j ≥ 1 rank(B∞1 ) = N
Theorem 1 states that in a high-dimensional DFM, the observables yt has a low-rank VAR(1) representation. This motivates the use of the first-order VAR (4) to evaluate the DFM-implied likelihood, bypassing any Kalman filter computation. Theorem 2 validates this algorithm. Theorem 2. Suppose Assumption 3 holds. Let ℓDFM (Y;A,C,G,R) denote the likelihood of Y implied by DFM (1), and let ℓ1(Y;A,C,G,R) denote that implied by the first-order VAR (4). Then as M → ∞,
E |ℓDFM (Y;A,C,G,R)− ℓ1(Y;A,C,G,R)| → 0 (5)
In other words, the bias from using the first-order VAR to evaluate the likelihood vanishes asymptotically. Therefore, one may approximate the likelihood of the DFM by computing the likelihood of the first-order VAR, with its approximation quality improving as M increases. 2.2 reduced-rank first-order var Our theoretical results in the previous subsection lay the groundwork for a fast algorithm that computes the likelihood of DFM (3) that, under Assumption 1 and 2 is a good approximation for the likelihood implied by M. One final question remains of how to compute the rank-N first-order VAR coefficients B∞1 , given the model M. Anderson and Rubin (1949) and Anderson (1951) were among the first to propose strategies to estimate reduced-rank VARs in a two step procedure. The first is to estimate the unrestricted OLS coefficient matrices, and then impose the restrictions in the second step. We pursue a different, computationally efficient route by building on the Dynamic Mode Decomposition (DMD). The DMD, introduced by Schmidt and Sesterhenn (2010) and later developed by Tu et al. (2014), is a workhorse tools in the fluid dynamics literature. Existing applications of the DMD also include epidemiology, neuroscience and video processing (Brunton and Kutz (2022)). We use and extend part of the the DMD algorithm to suit our own purposes in the following way. Our approach involves two sets of data, the empirical data {yt}T+1t=1 and simulated data from the model {ỹt}J+1t=1 . Using the simulated data, create two matrices by stacking the observations of ỹt for t = 1, . . . , J + 1 in the form11
Ỹ = [ỹ1, ỹ2, . . . , ỹJ ] Ỹ ′ = [ỹ2, ỹ3, . . . , ỹJ+1]
For a desired rank, call it N , the DMD estimates the reduced-rank VAR associated with the
simulated data by solving
B̃ = argmin rank(B)=N ∥∥∥Ỹ′ −BỸ∥∥∥ (6) where ∥·∥ denotes the Frobenius norm. To compute B̃, represent Y with a reduced Singular Value Decomposition (SVD)
Ỹ = ŨΣ̃Ṽ ⊤
where Ũ is M ×M , Σ̃ is M ×M and Ṽ is T ×M . We compress Ỹ by using its N largest singular
11Within the context of this paper, we implement the DMD on simulated data. In more conventional applications, they are real-world data. For example, in Sargent and Selvakumar (2023) they are percentiles of the real consumption distribution. values:
Ỹ ≈ UΣV⊤,
where U = Ũ[:, : N ], Σ = Σ̃[: N, : N ] has N singular values as its only non-zero entries, and V⊤ = Ṽ ⊤ [: N, :]. Here U is M × T , V is T ×N , Σ is N ×N , and V⊤ is N × T .12
We use this reduced-order SVD approximation of Ỹ to compute13
B̃ = Ỹ ′ Ỹ + , (7)
where by construction B̃ is rank N . The covariance matrix of the residuals, ãt = ỹt − B̃ỹt−1, is computed via
Ω̃ = 1T−1 T∑ t=1 ãtã ⊤ t (8)
Finally, to calculate the likelihood, first compute the residuals with empirical data ât =
yt−B̃ yt−1. Then the log-likelihood is standard, given by
f(y1, . . . ,yT+1) = T∑ t=1 −1 2 log(2π)− 1 2 log det(Ω̃)− 1 2 â⊤t Ω̃ −1ât (9)
A discerning reader at this point might question why not evaluate the likelihood of DFM (1) with the Kalman filter? The answer is that evaluating the likelihood via the Kalman filter requires knowing the matrices A,C,G,R, which itself must be estimated from the simulated data. To see why this might be a problem, consider an example where M = 300 and M is approximately rank N = 2. Then estimating D requires estimating 606 parameters of A,C,G,R14 Since they will also depend on structural parameters, one would need to insert an additional loop in the estimation procedure, making it highly computationally inefficient. 12Note that all we need here is a truncated SVD, which can be very efficiently computed using existing machine-learning packages (e.g. scikit-learn). 13See Sargent and Selvakumar (2023, sec. 2.1) for the full details of the DMD algorithm. 14G has 300× 2 parameters, R has one parameter, A has 2 parameters and C has 3 parameters. 2.3 estimation strategy The above sections set out the theoretical and computational arguments for our estimation strategy. To recap succinctly, the logic is as follows: Assumption 2 implies that a dynamic factor model with N factors is a plausibly good auxiliary model with which to approximate the likelihood of M. Yet, it is unclear how one should fit such a DFM and compute its likelihood. Assumptions 1 and 3 imply that such a likelihood can be approximated by that of a rank-N first-order VAR in yt; and that the approximate quality improves as M becomes large. We compute the rank-N first-order VAR and the associated likelihood by extending the Dynamic Mode Decomposition algorithm. Algorithm 1 Likelihood approximation
1. Fix some structural parameters θ
2. Simulate time-series ỹ1(θ), . . . , ỹJ+1(θ) from M for a large J and create data matrices Ỹ(θ) and Ỹ ′ (θ)
3. Choose the rank, N , as discussed in section 2.4
4. Calculate B̃(θ) and Ω̃(θ) in (7) and (8) 5. Approximate the log- likelihood f(y1, . . . ,yT+1 |θ) implied by M by computing (9)
Algorithm 1 presents pseudo-code for approximating the likelihood of observable data {yt}
implied by M. 2.4 how to choose n? A natural question in our strategy is what is approximate rank of M. In this section, we suggest a multitude of heuristic and quantitative procedures that offers insights into an appropriate choice of N . Given a simulated data set Ỹ from M, the common heuristic test used by Dynamic Mode Decomposition practitioners is to plot the singular values like in Figure 2.15. An example of this can be seen in Figure 3. Gavish and Donoho (2014) adopt a more quantitative approach and find the optimal threshold N . Assuming a generating model like (1) with measurement error covariance matrix R = σIM ,
15See, for example, Brunton and Kutz (2022, sec. 7.2))
they show that the optimal threshold is
N = λ(MT ) √ Tσ
where λ(β) = √ 2(β + 1) + 8β
β+1+ √ β2+14β+1 . The authors prove that for a fixed low-rank (say
N∗) factor model, the choice of N dominates the rule-of-thumb approach in terms of asymptotic mean squared error, when M,T → ∞ such that MT → β ∈ (0, 1]. From the principle components literature, Bai and Ng (2002) show that consistent estimation of
the number of factors can be attained by minimizing the information criterion16
IC(n) = V (n) + n
( M + T
MT
) log ( MT
M + T
) (10)
where V (n) = (MT )−1 ∑M
i=1 ∑T t=1(a n it) 2. Finally, we propose a method for choosing N for our particular setting. Given simulated data Ỹ, and a fixed N , calculate the N -rank VAR coefficient matrix B̃N (where we note the dependence on N for clarity). Then, calculate the VAR residuals by
ãt = ỹt − B̃N ỹt−1
Denote R2m,N as the individual-level R 2 for the VAR regression for m = 1, . . . ,M (i.e. for each
row of yt), given by
R2m,N = 1− ∑T t=2 ã 2 m,t∑T
t=2 ym,t− 1 T ∑T t=2 ym,t−1
where ãm,t is the m-th element of ãt. Then, calculate the aggregate R2N of the approximating
model by a weighted sum of the cross-sectional R2m for m = 1, . . . ,M . 16Though the analysis in Bai and Ng (2002) is done for principle components estimation of factor models, the same theory applies to any other consistent estimation procedure, as M,T → ∞. R2N = 1 M M∑ m=1 w(m)R2m,N (11)
where w(m) is some weighting function.17
Our proposed N is the value above which the aggregate R2 no longer increases. Indeed, if M is indeed approximately low rank, R2N convereges as N increases. Intuitively, this signals that increasing the number of factors in the DFM does not improve the forecasting ability of the approximate model. We therefore select the appropriate N such that the difference R2N −R2N−1 is sufficiently close to zero. Model validation An additional implication of our Proposition 1 is that the VAR residuals ãt
must be serially uncorrelated. We use this result as an additional check to validate our choice of N . Importantly, there is nothing in the first-order VAR that imposes such a restriction, it follows from the innovations representation of the DFM (1). To check this restriction, we construct the sample covariance matrix (12) and check how close it is to the zero matrix. Ê[at+1 a ⊤ t ] =
1 T T∑ t=1 ãt+1ã ⊤ t (12) 3 illustration with a canonical hank model We consider a small-scale HANK model as the laboratory of our method. The model features both aggregate shocks (e.g. TFP) that are common to RA business cycle models (e.g., Christiano, Eichenbaum, and Evans 2005, Smets and Wouters 2007) and a cross-sectional shock that directly affects the income distribution (e.g. Bayer et al. 2024). 3.1 model Time is discrete and runs forever, t = 0, 1, . . . . 17In our example below, we fix an equal weighting scheme, and sample individuals from the stationary distribution of M.
Household There is a unit measure of infinitely-lived households in the economy. Households face idiosyncratic risk to their labor productivity e and also transition risk to their employment status s ∈ {E,U}. For simplicity, we assume that the productivity process is independent of the employment status and that both idiosyncratic risks are exogenous to the business cycle.18 As a result, the productivity distribution is time-invariant. The average productivity E(e) is normalized to 1. Households can save and borrow through a risk-free asset, subject to an ad-hoc borrowing constraint a ≥ a. The Bellman equation of a household with asset a, productivity e, and employment status s at time t is given by:
Vt(a, e, s) = max c,a′
{ c1−σ
1− σ − φht(e)
1+ϕ
1 + ϕ + βEt
[ Vt+1(a ′, e′, s′)|s, e ]}
c+ a′ = (1− τt)yt(e, s) + (1 + rt)a
yt(e, s) = [1{s = E}+ 1{s = U} · b]wtht(e)e
a′ ≥ a
where rt is realized real return of the asset at time t, τt is labor tax, and yt is real labor income. When employed (s = E), the household supplies its labor service ht(e) to the unions at real wage per efficiency unit wt and earns yt(e, E) ≡ wtht(e)e. The hour choice ht(e) is determined by the union through a time-varying allocation rule of the form:
ht(e) = nt eξt∫
sit=E e1+ξtit di
∀e
where nt ≡ ∫ sit=E ht(eit)eit di is the total efficiency unit of labor. The variable ξt governs the dispersion of labor income, with the uniform allocation rule nested in the case ξt = 0. We will assume that ξt follows an AR(1) process and call it the income-dispersion shock. Finally, when unemployed (s = U ), the household receives unemployment benefits from the government which replaces a fraction b of her labor earnings, so that yt(e, U) ≡ b · wtht(e)e.
18In particular, the productivity still evolves during unemployment. Firms Final-goods firms operate in a perfectly competitive market. They demand labor services from the unions and transform them into the final goods using a CES technology with elasticity of substitution ϵ. The firm’s problem is given by:
max nit,yt
PtYt − ∫ Witnit di
s.t. Yt = e Zt (∫ n ϵ−1 ϵ it di ) ϵ ϵ−1
where Zt is TFP shock. In the symmetric equilibrium where Wit = Wt, nit = nt ∀i, we have the real wage equation
wt ≡ Wt Pt = eZt
Labor unions A continuum of labor unions operate in a monopolistically competitive market. Each union i sets its real wage wit subject to a quadratic adjustment cost a la Rotemberg (1982) and demands labor nit from the employed households to satisfy the demand from the firms. Following Alves and Violante (2023), we simplify the union’s problem by assuming that the union maximizes the utility of a representative employed household, subject to the exogenous labor allocation rule. Specifically, the union’s problem is given by:
max wit+k,nit+k Et ∞∑ k=0 βk {[ (CEt+k) −σ(1− τt)wit+k − φΩt+kH̄ϕt+k ] nit+k − ϵ 2κw log ( wit+k wit+k−1 Πt+k )2}
s.t. nit+k = ( wit+k wt+k )−ϵ nt+k
where CEt+k is total consumption of employed households, H̄t+k is total labor hours, Ωt+k ≡∫ sit=E eξtit di/ ∫ sit=E e1+ξtit di is the labor wedge associated with the allocation rule, and Πt+k is the gross inflation rate. In the symmetric equilibrium, the first-order condition leads to the wage Phillips curve:
log Πwt = κw [ φΩ1+ϕt n ϕ t −
ϵ− 1 ϵ
(1− τt)(CEt )−σwt ] nt + β log Π w t+1
Government Monetary policy follows standard Taylor rule:
(1 + rnt+1) = (1 + rss)(Πt) ϕπ(Yt) ϕyev r t
Government maintains balance budget every period by adjusting the labor tax τt:
rtBss + ∫ sit=U yit di = τt ∫ yit di
Aggregate shocks There are three aggregate shocks: TFP shock, monetary policy shock, and income dispersion shock. Each follows an independent AR(1) process. Zt = ρzZt−1 + σzϵ Z t vrt = ρrv r t−1 + σrϵ r t
ξt = ρξξt−1 + σξϵ ξ t
Equilibrium A rational expectation equilibrium consists of a sequence of policy functions {ct, at, ht}, a sequence of value functions {Vt}, a sequence of prices {wt, rnt ,Πt,Πwt , τt}, a sequence of aggregate objects {Yt, CEt ,Ωt, H̄t, nt}, a sequence of distribution {Ft}, a sequence of exogenous states {Zt, vrt , ξt}, and a sequence of beliefs over prices such that
1. Given the sequence of value functions, prices, and policy functions, the household Bellman
equation holds. 2. Given the sequence of beliefs over prices, all agents optimize. 3. The evolution of the distribution is consistent with the policy. 4. The sequence of beliefs over prices is rational. 5. All markets clear. 3.2 solution in sequence-space We employ the sequence-space Jacobian (SSJ) method of Auclert et al. (2021a) to obtain a linearized solution of the model in Section 3.1. Although the original SSJ method is developed for computing
the aggregate dynamics, it can be easily extended to obtain a solution for the micro consumption dynamics. Let ct = (c1,t, . . . , cM,t)⊤ be the vector of cross-sectional consumption and ϵt ∈ Rr be the vector of fundamental shocks at time t. In our model, ϵt consists of three shocks: TFP shock, monetary policy shock, and income dispersion shock. We have the following proposition. Proposition 2. In the linearized equilibrium, ct has a moving average (MA) representation
ct = css+ ∞∑ j=0 Ψcjϵt−j (13) furthermore, the ma coefficient matrix ψcj is given by Ψcj = ∑ p∈P J cpF jIpe
where P denotes the set of aggregate inputs that enter the household’s problem and
• J cp ∈ RM ×R∞ is the cross-section of gradients of consumption wrt. the future path of aggregate
input p
• Ipe ∈ R∞×Rr is the impulse response functions of aggregate input p
• F is the shift-forward operator
In light of Proposition 2, simulation of the micro consumption dynamics is straightforward, and computation of the MA coefficient matrices is trivial because J cp and I p e are products of the SSJ method.19 In practice, we truncate the horizon at T = 300. 3.3 calibration The model is in quarterly frequency. As the purpose of the model is to illustrate our method, we choose a set of parameters directly from the literature. In the steady state, we fix the annual real rate at 2% and set the (annualized) government debt-to-GDP ratio to be .80. The persistent income process is AR(1) and we use the parameters estimated by Krueger, Mitman, and Perri (2016). The EU rate is 6% and the UE rate is 90%, leading to an unemployment rate of 6.25%.20 The slope of the
19The gradients J cp are computed by backward iteration in the first step of the ”Fake news algorithm”. 20Our choice is consistent with JOLTS. Due to our timing assumption, the transition rates should be interpreted as the effective rates
that take into account the possibility of finding a job within a quarter. wage NKPC is set to be .14, consistent with the estimate in Beraja, Hurst, and Ospina (2019). The persistence and standard deviation of the income dispersion shock is taken from Bayer et al. (2024). Table 1 reports the full calibration. 3.4 estimation We estimate three model parameters [κw, ϕπ, ϕy] and six shock parameters [ρz, σz, ρr, σr, ρξ, σξ] using our method outlined in Algorithm 1, which we label MicroDMD. The observables are a simulated dataset of repeated cross-sections of individual (log) consumption, according to (13). The dimensions of the observables is M = 300, and T = 120. To replicate a realistic dataset, we randomly sample the 300 individual states from the stationary distribution and add i.i.d. measurement errors to the data. The measurement error accounts for 20% of the total variation and its standard error is also estimated along with the parameters of interest. For the simulation step, we set J = 10, 000. Choosing N To choose the N of the auxiliary DFM, we simulate time-series of consumption for M = 300 households, each of length T = 10000. We demean the time-series for each household and stack them vertically to create the simulated data matrix Ysim. We perform the battery of tests
outlined in Section 2.4 to infer an appropriate N . Figure 3 shows the 10 largest singular values from the data matrix Ysim. There are 2 dominant singular values, the third has value 0.025. The rest of singular values are 0.005 and below; we compute that σ(Ysim, 3) = 0.02
Table 2 computes the R2 statistic in equation (11) and the information criterion of (10) for an increasing N . The first row of the table shows that the R2 doesn’t increase in N after N = 3. It suggests that the first-order VAR is no better at predicting yt+1 given yt if we set N > 3 compared to N = 3. The information criterion, which penalizes large N , is shown in the second row. It falls until N = 3 and then increases again, making N = 3 seemingly the appropriate choice. Finally, we study the residuals associated with the rank-reduced first-order VAR. The third row computes the maximum absolute autocovariance of the VAR residuals at, computed via (12). We find shows relatively significant autocorrelation for N = 1, 2, suggesting that N = 1, 2 is inadequate in satisfying the assumptions that define the first-order VAR. The maximum autocovariance is close to zero, 2.1e−4 for N = 3, and remains so for larger N . Finally, computing the optimal threshold formula from Gavish and Donoho (2014), with β =
M T = 0.03 gives N = 3. All of these statistics considered, we set N = 3. 3.5 simulation results Figure 5 presents the finite-sample distribution of estimates from our maximum-likelihood estimation of MicroDMD, calculated using 500 Monte-Carlo samples. Table 3 shows the mean parameter estimates and standard deviations from the Monte-Carlo samples. The mean of our estimators are remarkably close to the true values, except for the standard deviation of monetary policy (MP) shock which we underestimate. There are two reasons why the identification power for the size of the MP shock is relatively weak. First, due to the accommodative Taylor rule, the MP shock has small effect on consumption dynamics. Second, since we draw the individuals from the ergodic distribution, they have a similar level of asset holdings, limiting the differential consumption responses from the capital income channel. Moreover, the distributions of the estimates appear well-behaved with small standard deviations around the mean, even with only 500 MC samples. In the next sections, we compare our model with other typical estimation strategies and within that context highlight the benefit of using all the information available in the micro-data. 3.6 comparison with estimation using aggregate data Aggregate data only For comparison, we estimate the model parameters using aggregate data in two ways. The first is implemented using the MLE procedure by Auclert et al. (2021a) with only aggregate data (which we label Agg). The aggregate data consists of output, inflation, and nominal rate series, each of length T = 120. For a fair comparison with our method, we add measurement errors to the aggregate data which accounts for 10% of the total variation.21 The result of this estimation is presented in the Agg columns of Table 3. For most parameters, except the size of MP shock and income dispersion shock, the Agg performs worse than MicroDMD both in terms of
21Since the Auclert et al. (2021a) method computes the likelihood of the aggregate data exactly, without measurement errors, their method will definitely deliver a better estimate than ours. Aruoba et al. (2016) argues that measurement error accounts for 20% of the variation in official US GDP measures. Thus, we view the 10% measurement error as a useful benchmark. bias and standard error. Figure 6 shows the distribution of the estimates. It shows that that the finite-sample distribution is much more dispersed and ill-behaved than our method. Aggregate data plus cross-sectional moments Next, we compare our method against the population approach of including micro data into the estimation by constructing time-series for a few cross-sectional moments (e.g. Bayer et al. 2024 and Mongey and Williams 2017). We label this approach Agg+. The main advantage of this method is its simplicity and speed, since the likelihood of aggregate time-series can be efficiently computed via Kalman filter or using the full variance-covariance matrix in the same way as Agg. However, the ex-ante static aggregation of micro data may induce unnecessary information loss. In principle, MicroDMD makes better use of the micro data by utilizing the DMD algorithm to extract the most informative dynamic structures underlying the micro data. To illustrate this point, we append a cross-sectional moment, the variance of log consumption, to the macro data and redo the aggregate estimation exercise. The results are reported in the Agg+ columnds of Table 3. The inclusion of cross-sectional moment brings the mean estimate closer to the truth and substantially lowers the standard error (compared to Agg), most notably for the estimates of the slope of the wage NKPC and the TFP shock process. Nevertheless, the performance of the estimator is still significantly worse than our method, suggesting that our method retains cross-sectional information beyond simple moments. Do these these differences in the estimated parameters translate to meaningful differences in the objects that macro-economists care about? Figure 4 suggest that they do. It plots the impulse responses of aggregate output to the three shocks in the model: TFP, monetary policy and the income dispersion shock. For all three shocks the width of the confidence bands for MicroDMD is significantly reduced compared to both the Agg and Agg+. Overall, the results suggest that our method which exploits the rich information contained in the micro data is better able to recover the true parameters of the model, and generally with a lower standard error in finite samples. The results emphasizes the advantage of using micro data in the estimation of heterogeneous-agent models. NOTE. The impulse response is wrt. 1 standard deviation shock. Red line is the true value and black line is the mean of the estimates. Shaded area is 90 percent confidence interval computed from 500 Monte Carlo draws. NOTE. The statistics are computed using 500 Monte Carlo draws. 3.7 comparison with other estimation methods using micro data Frequency-domain estimation The difficulty of exact likelihood evaluation of the micro data is the high dimensionality (M × T ) of the variance-covariance matrix. One way to tackle this computational challenge is to evaluate the likelihood in the frequency-domain using the Whittle approximation, as in Hansen and Sargent (1981), Christiano and Vigfusson (2003), and PlagborgMøller (2019). We label this method MicroFD. The Whittle approximation decomposes the entire M × T -dimensional variance-covariance matrix into the sum of M -dimensional frequency-specific matrices. Thanks to the Fast Fourier Transform, the decomposition and associated likelihood evaluation is fast and only requires the sequence-space solution of the model. A key difference between the frequency-domain estimation method and ours is that it requires large T for accurate approximation, while our method requires large M . Since in reality the cross-sectional dimension of the micro dataset is usually larger than the time dimension, we argue that our method is more suitable in practice. We apply the frequency-domain estimation method to the simulated micro datasets and report the results in the MicroFD columns of Table 4.22 The results suggest that our method dominates the frequency-domain estimation method both in terms of bias and standard error, consistent with the asymptotic theory. 22The details of the estimation procedure can be found in Appendix B.1. NOTE. The statistics are computed using 500 Monte Carlo draws. Full-information estimation Liu and Plagborg-Møller (2023) develops a full-information likelihood based approach for the estimation of heterogeneous-agent models. Our method is different in two dimensions. First, their method requires both macro and micro data. The macro data is used to infer the conditional distribution of the aggregate states which pin down the (conditional) likelihood of micro data. In contrast, our method can do inference base solely on micro data and can still be intuitively extended to incorporate macro data, a point that we further discuss in Section 4. Second, to infer the aggregate states, their method requires a state-space solution of the model computed using dimension-reduction algorithms such as Winberry (2018). On the other hand, since our indirect inference strategy is simulation-based, we only require the ability to simulate from the model and hence can accommodate sequence-space solutions as well as state-space solutions of the model. That being said, when Liu and Plagborg-Møller (2023)’s method is applicable, the full-information nature guarantees that their estimator is more efficient. To summarize, our method provides a practical middle ground between the benchmark fullinformation method and conventional methods – it enjoys the substantive efficiency gain from using micro data but is no harder to apply – it can be coded up in only a few lines of code – than aggregate-data-based methods. NOTE. The plots are generated from 500 Monte Carlo draws. Red line is the true value and black line is the mean of the estimates. NOTE. The plots are generated from 500 Monte Carlo draws. Red line is the true value and black line is the mean of the estimates. 4 robustness and extension In this section, we provide additional analytical and simulation results on the approximation quality of the low-dimensional dynamic factor model and discuss multiple extensions of our method. 4.1 possibility of low-rank approximation The validity of our method relies on the assumption that the heterogeneous-agent model generating the micro data can be approximated by a low-dimensional DFM. In Section 2, we argue that this is a reasonable assumption for a wide range of models and suggest heuristic procedures for testing this assumption using data simulated from the model. While these are good enough for practitioners, there is still no theoretical guarantee that a low-rank approximation is possible. Here we fill this gap by deriving a sufficient condition on the sequence-space solution of the model that renders a low-dimensional DFM representation. Proposition 3. Let P be the set of endogenous aggregate inputs (e.g. real wages), E be the set of exogenous shock processes (e.g. TFP), and J := {J px : p ∈ P, x ∈ E} be the set of general equilibrium Jacobians. Suppose all the exogenous shock processes are AR(1). If for any x ∈ E and p ∈ P , we have the commutability condition,
FJ px = J pxF,
where F is the shift-forward operator, then a low-dimensional DFM representation exists. Intuitively, FJ px is the effect of the shock on the economy next period, while J pxF is the effect of a news shock on the economy today. The two effects will coincide if the HA distribution doesn’t move in response to shocks, as this is the only endogenous state variable. Although the commutability condition will not hold exactly for most models, the slackness of the condition serves as a lower bound for the low-rank approximation quality. We evaluate the normalized slackness ∥FJ px − J pxF∥/ ∥FJ px ∥ for each shock and input in our small-scale HANK model and report the results in Table 5. There are three endogenous ”prices” that the households care about – real interest rate, average real after-tax labor income, and average hours. Overall, the slackness is about 10% of the Frobenius norm of the GE Jacobian, except for average hours wrt. TFP shock which amounts to 23.8%. 4.2 bayesian indirect inference Our method can be easily paired with Bayesian methods to conduct Bayesian indirect inference, which sits within the Approximate Bayesian Computation (ABC) class of algorithms. Recall that object we want to target is the posterior distribution p(θ|Y) ∝ f(Y |θ)p(θ). Among others, one simple and intuitive method, proposed by Gallant and McCulloch (2009) and Reeves and Pettit (2005), replaces f(Y |θ) with the approximate likelihood computed in Algorithm 1. The target object is now the pseudo-posterior related to p(θ|Y), analogous to how Section 3.1 maximised a pseudo-likelihood in the frequentist case.23 Of course, in the special case that the auxiliary model nests M then the two posteriors coincide (Drovandi et al. (2015)). Though this may not be exactly satisfied in our heterogeneous-agent model settings (i.e. the M is not exactly a DFM), the proposed tests in Section 2 and associated discussion should offer insights into when the approximation is good. We compute the posterior sampling distributions of the model parameters in Section 3.1 via a Random Walk Metropolis Hastings (RWMH) algorithm. Algorithm 2 provides the pseudo-code for one iteration of the RWMH under our approach. For simplicity, we use a flat prior for all parameters. The computational details can be found in Appendix B.2. Figure 7 shows the posterior from 50,000 iterations of RWMH. Both the posterior mode and mean is near the true value. Overall, the posterior is tightly centered around the truth, even though the prior is completely uninformative. The simulation evidence thus suggests that our method can be used to conduct standard Bayesian analysis for HA business-cycle models (e.g. An and Schorfheide 2007). 23Drovandi et al. (2015) draws the same connection between this methods and the quasi-maximum likliehood approach of Smith Jr (1993). Algorithm 2 Bayesian Indirect Inference with Random Walk Metropolis Hastings For iteration n with structural parameter θn−1:
1. Draw θ∗ ∼ q(·|θn−1) 2. Approximate likelihood f(Y |θ∗) using Algorithm 1
3. Compute r = min { 1, f(Y |θ
∗)p(θ∗) f(Y |θn−1)p(θn−1) } 4. Accept θ∗ with probability r 5. if accept, θn = θ∗, else θn = θn−1 5 conclusion We develop an indirect inference method for estimating HA business-cycle models using micro data. The key idea is to approximate the data-generating process with a low-dimensional dynamic factor model and use the implied likelihood for inference. Employing the Dynamic Mode Decomposition algorithm, the likelihood evaluation is fast and simple. Moreover, our estimation procedure can seamlessly accommodate the sequence-space solution method, while most currently available estimation methods (e.g. Liu and Plagborg-Møller 2023) are designed for state-space solution only. Our method is based on two assumptions: 1) the HA model is well-approximated by a lowdimensional dynamic factor model, and 2) the cross-sectional dimension of the micro data is large. We show that the first assumption holds in a wide range of HA models and provide a theoretical justification for it. In our simulation study, we show that our method works well on a realistic dataset, verifying the empirical relevance of the second assumption. Comparing with other conventional methods including time-series estimation with crosssectional moments and frequency-domain estimation, our method delivers a better estimate both in terms of bias and standard error because of the more efficient use of cross-sectional information. As our method is based on approximated likelihood, we show that it can be easily pair with Bayesian methods to conduct Bayesian indirect inference. 