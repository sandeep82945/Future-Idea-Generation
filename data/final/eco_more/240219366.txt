The growing number of cases requiring digital forensic analysis raises concerns about law enforcement’s ability to conduct investigations promptly. Consequently, this systemisation of knowledge paper delves into the potential and effectiveness of integrating Large Language Models (LLMs) into digital forensic investigation to address these challenges. A thorough literature review is undertaken, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the utilisation of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating LLMs. In conclusion, the study asserts that the adoption of LLMs in digital forensics, with appropriate constraints, holds the potential to enhance investigation efficiency, improve traceability, and alleviate technical and judicial barriers faced by law enforcement entities. 1 introduction With the pervasive growth of Information and Communication Technology (ICT) and information systems, cybercrimes have witnessed a significant surge in recent years [6]. As a further compounding factor, the number of “traditional” police investigations including digital evidence is also ever-increasing [35]. Addressing and investigating this volume of cases presents substantial challenges. Artificial General Intelligence (AGI) and Large Language Models (LLMs) have become prominent topics of global discussion, prompting researchers to intensify their investigations by leveraging the capabilities of LLMs. The usage of LLMs within the scientific community experienced a rapid surge after 2022, notably with the advent of OpenAI’s ChatGPT platform. In a remarkably short period, this topic has garnered attention from academia, industry, and the research community at large [94]. Simultaneously, researchers are exploring the potential of LLMs across various domains and assessing their impact on the future of science and society. This inquiry also includes an examination of the potential harmfulness associated with the deployment of LLMs [17, 78]. In other words, the use of LLMs in various tasks can be a double-edged sword, necessitating careful consideration depending on the specific situations and contexts. Given the swiftly evolving landscape of LLMs, it is prudent to look into various types and their unique capabilities. A nuanced
understanding of the strengths and characteristics of different LLMs can contribute to more informed and effective applications within the dynamic field of digital forensics (DF). The main objective of this article is to explore the viability of integrating LLMs into the fundamental phases of the modern DF process. This exploration involved a meticulous and systematic review, adhering to the standards set by Kitchenham [51]. The literature selected for this review was exclusively in English, and non-article sources such as books were excluded. The focus of the review encompassed empirical studies, case studies, and literature reviews within the domain of DF investigation methods, models, frameworks, efficiency, and challenges, as well as LLM capabilities, challenges, and architectures. Additionally, the study underscores the utilisation of autonomous agents based on LLMs, emphasising their capabilities and the potential efficiency gains achievable in the field of DF. In light of the fast-paced advancements and recent explosion in LLM-focused research, a substantial influx of LLMfocused research papers has occurred since the launch of ChatGPT in late 2022. Due to this fast pace, a large number of research articles exist solely as preprints on preprint services, e.g., arXiv or OpenReview. To give two examples, the initial papers for GPT-4 [2] and LLaMA [112] are both only published on arXiv, but have garnered thousands of citations each. Despite their preprint status, these papers offer essential insights critical for contemporary research and dialogue within the domain, making their incorporation into this paper necessary to provide the most up-to-date knowledge and perspectives. The paper is structured as follows: Section 2 provides a comprehensive background for the review, delving into existing DF process models, the challenges inherent in DF, and a detailed overview of the current work conducted with the utilisation of LLMs within DF. In Section 3 the paper delves into the realm of Natural Language Processing (NLP), elucidating the working principles of LLMs, their architectural foundations, and the specifics surrounding specially trained LLMs. Section 4 provides an in-depth review focusing on the capabilities and benchmark information of LLMs trained for coding tasks, as well as those tailored for vision assistance. Section 4 explores the synergy between DF and LLMs, detailing how LLMs can be effectively employed in each phase of the DF process model. Finally, in Section 6, the paper summarises the future challenges associated with integrating LLMs with automated agents within the DF domain. It outlines potential avenues for future research and development, shedding light on the path of future DF investigations employing LLMs. The discussion encompasses not only the potential negative impacts but also the practical difficulties and risks in real-world environments. ar X iv :2
40 2. 19 36
6v 1
[ cs
.C R
] 2
9 Fe
b 20
24 2 background DF is a process to identify, preserve, analyse, and document digitally recorded data, which originate in electronic devices such as computers, servers, smartphones, and IoT devices [11]. This exercise is required in most criminal cases. Data collected in this process are kept unchanged and safe to present in a court case or to support future investigations conducted by law enforcement agencies [75]. 2.1 digital forensic process models DF process models consist of a series of activities that help to standardise the investigative process [34] and outline the phases; collection, preservation of evidence, examination or analysis, and reporting. DF encompasses various subdisciplines such as computer forensics, mobile device forensics, memory forensics, network forensics, and cloud forensics, each employing distinct processes reflected in a plethora of models within the literature [99, 134]. These models often share phases but differ in their focus and execution. For instance, Al-Dhaqm et al. [3] proposed a mobile forensic model that adds a preparatory phase and bifurcates the analysis stage into examination and analysis phases. To accommodate the complexities of computer, network, cloud, and smart device forensics, Lutui [69] introduced a multidisciplinary model that necessitates diverse skills for effective investigation, covering incident detection to evidence storage. Casey’s model, as shown in Figure 1, includes phases like incident recognition, evidence collection, preservation, and presentation, with the examination phase detailed into recovery, harvesting, reduction, and classification [20]. This model emphasises the critical nature of maintaining evidence integrity and the requirement for expert analysis to extract and interpret pertinent information, culminating in a report suitable for legal scrutiny [20]. Notably, the analysis or examination stage is pivotal in all models, demanding specialised knowledge in the relevant DF area [73, 84]. The advent of cloud computing has led to the Digital Forensic as a Service (DFaaS) model by van Baar et al., integrating evidence preservation and analysis into an automated, secure software service, marking a significant evolution in forensic methodologies [34, 118, 119]. 2.2 existing challenges in digital forensics DF is an evolving field, yet the literature highlights that it still undergoes changes to address ongoing challenges and advancements. Dubey et al. [36] assert that DF faces key challenges, including the
complexity of data and its volume, a lack of standardisation, inadequacies in the power of existing tools to support investigations and issues related to timelines. In addition to the previouslymentioned challenges, issues such as scope creep in cases due to complexity and vast data, as well as the critical tasks of selecting and prioritising the right set of evidence, and efficiently allocating time and investigators for the chosen evidence, were highlighted [46]. Koper et al. [53] focus on certain issues from the investigator’s perspective, including challenges in adapting to a system, unexpected time-saving negations, and frustrations among officers arising from timelines and the adoption of complex systems. Automating the DF process using existing technology appears to be a promising solution for addressing issues related to time management and effectiveness [71]. However, an ongoing challenge revolves around measuring the accuracy of investigations and ensuring the verification of the automated process. This aspect remains an open area that requires further attention and resolution [45]. 2.3 existing work with llms in digital forensics Scanlon et al. [99] analysed using ChatGPT for DF. In their assessment, the authors evaluated the programming, incident narration, keyword list creation, and DF teaching abilities of ChatGPT. Their conclusion highlighted that while ChatGPT exhibited some hallucinations in the output results, it still serves as an effective assistant for code generation. Timeline reconstruction aids investigators in deducing the behaviour of an event. In line with timeline regeneration, Silalahi et al. [105] proposed a method to detect anomalies in a drone flight by employing sentiment analysis with the assistance of a pre-trained LLM. Their approach successfully discerned differences between normal and abnormal events with an accuracy of 92.5%. Hansken, a DFaaS platform created by the Netherlands Forensic Institute, is designed to assist investigators in handling evidence and conducting investigations more efficiently [117]. ChatGPT has been utilised as an assistance for the Hansken DFaaS system, contributing to streamlined processes and improved support for investigators [43]. In a successful experiment, evidence traces were provided to ChatGPT, and it was tasked with analysing the case, ultimately providing conclusions. This utilisation of ChatGPT showcases its potential in assisting with the analytical aspects of investigations, highlighting its ability to process and interpret evidence data [43]. 3 large language models This section explores LLMs, concentrating on three principal aspects. Initially, it explores the architecture of LLMs, detailing their design and function. Next, it assesses the usability of LLMs, underscoring the features and capabilities that render them apt for a wide range of tasks. Finally, it showcases the versatility of LLMs by discussing their application across various fields, demonstrating their wide-reaching impact and the extensive scope of their applications. 3.1 natural language processing Popular LLMs such as Generative Pre-trained Transformer (GPT) [30], Language Model for Dialogue Applications (LaMDA) [111], Pathways Language Model (PaLM) [28], Bidirectional Encoder Representations from Transformers (BERT) [33], and Large Language Model Meta AI (LLaMA) [112] stem from advancements in Natural Language Processing (NLP). NLP, focusing on language-based tasks, utilises both traditional and deep learning models to enable applications such as language translation, text processing, and speech recognition [95]. Deep learning, a branch of machine learning, uses complex computational layers and adaptive weights to enhance prediction accuracy, offering a more refined analysis than conventional machine learning [55]. It has excelled in image and speech recognition, and natural language understanding, mimicking the decision-making process of the human brain through artificial neurons. These neurons form networks capable of intricate pattern recognition and data analysis. Central to deep learning are neural networks with multiple hidden layers, which autonomously learn and featureextract from data, bypassing the need for manual variable selection. This automatic feature extraction makes them exceptionally adept at handling complex tasks [123]. 3.2 llms An LLM is a language model employing neural networks with billions of parameters, trained on extensive text data. These models are engineered to comprehend and generate human language. Fundamentally, they rely on multiple neural network architectures, enabling them to recognise the relationships between words and phrases within sentences [38, 103]. These architectures have been a transformative force in the field of natural language processing. Its capability to excel across a diverse array of language-related tasks distinguishes it as a game-changer, in contrast to being tailored for a singular, specific task. 3.3 architecture of llms LLMs utilise deep learning, particularly neural networks, to process and produce human language. Fundamentally, a language model operates with letters or words, but since machine learning algorithms and neural networks require vector inputs, words are vectorised. Each word in the vocabulary is assigned a unique numerical value for input into neural networks. Through initial random weight assignments and subsequent backpropagation, words acquire numerical positions reflecting their semantic similarity, culminating in a word embedding model [54]. 3.3.1 Word to Vectors. Word embeddings, as introduced byMikolov et al. [72], entail precise and high-dimensional vector representations for words, particularly suited for extensive datasets comprising billions of text entries. The authors exploredmodel architectures for word vectorisation, achieving substantial enhancements in accuracy while requiring lower computational resources and reduced training time [72]. In the realm of LLMs, the primary objective is to generate new text based on the extensive dataset, on which it was trained. For this purpose, Vaswani et al. [120] introduced the transformer model, aided by the word-to-vector model. This
architecture incorporates a self-attention mechanism, as well as encoder and decoder processes, enabling the model to rapidly and simultaneously focus on pertinent information. 3.3.2 Transformer Models. The transformer model initially aimed at machine translation, translating input words into another language, begins with word embedding, where inputs, termed as tokens, are vectorised. Recognising word order is achieved through positional encoding, with two main techniques: absolute and relative. Absolute positional encoding assigns unique vectors to each position, enhancing themodel’s ability to recognise word placement and facilitate position-specific attention [49]. Relative positional encoding, on the other hand, calculates the relative positions of words by introducing a bias term that quantifies distances between positions, improving the model’s capacity to understand word relationships within a sequence [49]. Self-attention, a core mechanism within the transformer, calculates the relationship among words in a sentence, allowing the model to assess eachword’s similarity to others and generate unique representations for each [4]. The decoder mirrors the encoder’s steps but uses different weights, starting with positional encoding and computing self-attention values to identify the sentence’s initial translation word. This transformer process, leveraging stacked self-attention and unique positional encoding, has significantly advanced NLP tasks, including machine translation, text generation, and summarisation, by executing these processes in parallel and optimising weights for both encoder and decoder [1, 120]. 3.4 specifically trained llms The transformer model and the self-attention mechanism have paved the way for researchers to train language models on trillions of tokens with billions of parameters. Several LLMs have been trained and harnessed, each tailored with specific capabilities for diverse fields such as security, chemistry, engineering, medicine, business, tourism, and language-related applications. These models are employed in tasks ranging from detecting security threats, analysing data, and generating synthetic actions to teaching, code generation, structured query generation, planning, assisting in medical education, clinical decision-making, leveraging clinical settings, clinical validation, understanding general patterns and decisionmaking, bias detection, addressing ethical issues, language translations, question answering, information extraction, and business process automation, among others [14, 15, 18, 19, 22, 37, 41, 44, 47, 74, 79, 92, 110, 113, 121, 130, 136, 149]. The fine-tuning and retraining capabilities of LLMs empower them to be adapted to specific tasks or behaviours in a predefined manner. Fine-tuning involves taking an already trained language model and retraining its existing weights and bias values using a new dataset specific to a particular domain. This process allows the LLM to be customised and refined for tasks beyond its original training, enhancing its applicability in specific contexts [150]. This process results in a new model that is more tailored and focused on the specified domain. In existing literature, it is frequently observed that LLMs are fine-tuned with a particular emphasis on engineering and research-related fields. This targeted fine-tuning ensures that the model is adept at handling
tasks and generating content specifically relevant to the intricacies of these domains [22]. 4 capabilities of large language models This section focuses on the abilities and capabilities of Language Model Models (LLMs) as outlined in Section 3.4. Additionally, it discusses the currently available fine-tuned LLMs that exhibit potential for application in DF. 4.1 programming/coding The ability to generate source code within a specific context is a crucial proficiency inherent to a language model [8]. Xu et al. [135] conducted a systematic evaluation of six LLMs for code generation across twelve diverse programming languages. The benchmarking process involved the use of the HumanEval benchmark and an evaluation dataset designed to assess the functional correctness of programs generated by an LLM [26]. Initially, the dataset was employed for benchmarking Code, a GPT language model finetuned on publicly available code sourced from GitHub [26]. The Mostly Basic Programming Problems (MBPP) stands as another benchmark, comprising 974 programming tasks. It serves as a frequently employed evaluation dataset for LLMs specialising in coderelated tasks [129]. Several LLMs explicitly trained for code generation include Code LLaMA, CodeGen, StarCoder, PanGu-Coder, PanGu-Coder2, WizardCoder, InCoder 6B, CodeGen-Mono 16B, Code-Davinci-001, Code-Davinci-002, PaLM-Coder-540B, CodeT5+, InstructCodeT5+, GPT-4 with Reflexion, CodeGeeX, AlphaCode, Santa-Coder [7, 17, 23, 28, 29, 39, 58, 59, 68, 76, 98, 102, 104, 127, 138, 145, 146]. A higher value for both HumanEval and MBPP indicates greater accuracy in code generation for a given task. For detailed information, refer to Table 1, which presents the counts for HumanEval and MBPP, along with the trained parameter size for each LLM. Even with the benchmarking rates from HumanEval, Liu et al. [64] raised questions about the accuracy of the functionality of the generated code. To address this concern, they proposed EvalPlus, an automated test generation engine, to verify the functional correctness of LLM-generated code. 4.2 vision assistance Traditional vision assistant systems face limitations in image processing or recognition, as they are typically trained on fixed types of datasets. However, with the emergence of LLMs, this paradigm has shifted towards utilising raw text as the source of supervision [89, 108]. Research on visual recognition language models is experiencing exponential growth, with the number of models surpassing 1,500 in 2023 [140]. Radford et al. [89] introduced a novel method called Contrastive Language-Image Pre-training (CLIP). This method is efficient and capable of performing a wide range of tasks during pre-training. It enables the training of a model to learn a shared representation space for both images and text, facilitating a deeper understanding of the relationships between the two modalities. Ramesh et al. [93] proposes a model for text-to-image
* Estimated parameter count as value is not officially released [97]. generation, capable of generating images as combinations derived from textual input or sentences. Moreover, with the model named Generating Images with Large Language Models (GILL), it becomes feasible to generate text, retrieve images, generate novel images, and interleave the results into coherent multimodal dialogues [52]. VisionLLM is a framework leveraging LLMs for diverse vision tasks with unified language instruction, demonstrating generality and flexibility [125]. It incorporates a language-guided image tokeniser and an LLM-based task decoder, capable of handling open-ended tasks based on provided language instructions [125]. Visual instruction tuning leverages language-only models, such as GPT-4, to generate multimodal language-image instruction following data. This data is then utilised to instruction-tune large multimodal models, such as Large Language and Vision Assistant (LLaVA) [2, 62, 63]. The open-source LLaVA project introduces an end-to-end trainedmodel, integrating a vision encoder with an LLM. Notably, LLaVA showcases multimodal chat capabilities. LLaVa exhibits the capability to engage with images, providing detailed descriptions and responding to queries with a reported accuracy of 92.53% [63]. This demonstrates its effectiveness in understanding and generating contextually relevant information about visual content [63]. MiniGPT-4 is an open-source, powerful visual instructiontuned LLM, and it demonstrates versatility by generating stories and poems inspired by provided images and teaching users how to cook based on visual cues from food photos. This showcases its ability to understand and respond creatively to diverse visual stimuli [148]. Position-enhanced Visual Instruction Tuning (PVIT) represents an extended version ofMultimodal Large LanguageModels (MLLMs). It facilitates region-level encoding in an image, enabling the model to discern and identify information within specific regions [24]. Like PVIT, GPT-4 Region of Interest (ROI) is an instruction-tuned LLM capable of extracting information such as colour, shape, material, and action from a specified region [141]. This model enables users to interact with both language and drawing bounding boxes to indicate the area of interest within an image [141]. Other MLLMs, such as Visual ChatGPT, InternGPT, Flamingo, BLIP-2, and Kosmos, are noted in the literature for their capacity to assist users in visual-related information [5, 57, 65, 82, 90, 132, 143]. Video information is gaining prominence in vision assistance, and Zhao et al. [144] has introduced a novel approach to automatically narrate lengthy videos using LLMs. UniVL is another language pre-trained model designed for both multimodal understanding and generation. It is capable of retrieving a video segment based on text descriptions, generating captions for given video clips, segmenting a video according to a provided text input, and conducting multimodal sentiment analysis of a video segment [67]. VidIL and MOV are additional MLLMswith similar capabilities, demonstrating proficiency in video classification and video-language operations such as video captioning, video question answering, video caption retrieval, and video future event prediction [87, 128]. These MLLMs adhere to a shared task set, encompassing visual question answering, visual captioning, visual common-sense reasoning, visual generation, multimodal affective computing, visual retrieval, vision-language navigation, multimodal machine translation, visual question generation, and visual dialoguing, as summarised in Table 2 [50, 114]. 4.3 conversation Specific LLMs are trained explicitly for meaningful and coherent dialogues with humans. An example is Dialogue Generative Pretrained Transformer (DialoGPT), a fine-tuned model trained on 174 million conversations from Reddit [142]. DialoGPT exhibits the ability to provide human-like answers in tested conversations [142]. Dettmers et al. [32] introduced a fine-tuning mechanism for LLMs named Quantized Pre-trained Language Model into Low-Rank Adapters (QLoRA). This allows the fine-tuning of large-parameter LLMs with low training costs. They introduced Guanaco, a finetuned LLM with 65 billion parameters, which achieved a performance level of 99.3%. Falcon-180B and Falcon-40B represent another set of open-source LLMs with 180 billion and 40 billion parameters. These models are trained to communicate in multiple languages, allowing users to engage in conversations in languages other than English [81]. To evaluate the accuracy of human-like dialogue systems, Ou et al. [80] proposed a dialogue evaluation benchmark named DialogBench, comprising 12 dialogue tasks to assess the capabilities of LLMs. In their evaluation, they assessed 28 pre-trained and instruction-tuned LLMs, demonstrating that GPT-4, ChatGPT, and KwaiYii-13B-Chat emerged as the top three models for conversations in domains related to daily life and professional knowledge. In DF chat conversations, the significance lies in facilitating nontechnical investigators to elucidate terminologies and areas lacking
understanding. This serves a dual purpose, acting as an interactive teacher to enhance comprehension in discussions [99]. 4.4 prompt engineering Achieving quality outputs from LLMs often relies on providing well-crafted, meaningful, and precise input queries, known as input prompts. However, even human-defined natural language instructions may not consistently yield the best results. Prompt engineering is amethodology that involves carefully defining and instructing LLMs to generate more accurate and desirable outputs. Through thoughtful refinement of input prompts, prompt engineering aims to enhance the performance and effectiveness of LLMs in generating outputs that align more closely with user expectations and requirements [130, 147]. This plays a crucial role in biasing LLMs towards specific domains or topics, enabling a more targeted and nuanced response. By carefully crafting prompts, users can guide LLMs to delve deeper into the nuances of their queries, leading to more accurate and relevant outputs. This approach enhances the model’s responsiveness to specific areas of interest, allowing users to fine-tune and tailor their interactions with the LLM for more precise and meaningful outcomes. Prompt engineering with LLMs is employed across various sectors, including but not limited to medical, engineering, construction, and healthcare [10, 83]. ChainForge is an open-source Graphical User Interface (GUI) tool developed specifically for prompt engineering and hypothesis testing derived from LLMs that can be used in the above-mentioned fields to generate accurate and swift outputs [9]. 4.5 autonomous agents The evolution of LLMs, with their capacity to generate information and communicate in a manner resembling human interaction, has led to the development of autonomous agents. The expectation is that these agents will effectively execute a wide array of tasks, capitalising on the human-like capabilities inherent in LLMs [124]. These autonomous agents follow a four-stage architecture, encompassing profiling, memory, planning, and action. Profiling defines the agent’s role, privileges, domain, and expertise [124]. Memory stores information about tasks and profile data relevant to the environment. Planning involves breaking down given tasks into subtasks and solving them individually. The action stage is the final phase where all decisions and subtasks are translated into actions executed by the agent. Zhang et al. [139] developed a framework designed to facilitate collaboration between AGI agents and humans. This framework enables planning and communication for specific tasks, leveraging the capabilities of LLMs Similarly, AgentSims, ToolBench, GameGPT, ChatDev, Voyager, and RecMind represent a diverse array of autonomous AGI agents developed with distinct goals and objectives [25, 61, 86, 88, 122, 126]. Certainly, AutoGen stands out as a multiagent framework with the capability to autonomously perform tasks or collaborate with human feedback. This flexibility makes it a versatile tool for various applications [133]. 4.6 limitations and risks As explored in the preceding sections, it appears that LLMs possess a vast range of capabilities. However, it is crucial to acknowledge
that they are not without limitations and risks. In multimodal LLMs, it is a common issue that they are over-reliant [131]. Hadi et al. [42] highlighted significant drawbacks associated with LLMs, including issues such as bias, explainability challenges, reasoning errors, logical errors, hallucinations, vulnerability to prompt injections, and spelling and grammar errors. These limitations underscore the importance of a cautious and critical approach when utilising LLMs in various applications. Additionally, the literature shows limitations in LLMs, including statistical inconsistency, the absence of emotional attributes in linguistic responses, and challenges related to fact verification [40, 107]. These factors contribute to a comprehensive understanding of the constraints and potential shortcomings when working with LLMs. Thapa et al. [109] contend that while LLMs can indeed diminish the time and costs associated with annotation tasks, they fall short of completely supplanting human annotation. This is because they struggle with intricate linguistic constructions, such as idioms, irony, sarcasm, and metaphor, which can potentially impact the precision of annotations. Similar limitations are associated with MLLMs. Issues such as over-reliance on training data, sensitivity to word order in the input prompts, and vulnerability to prompts containing extra knowledge represent challenges in the capabilities of MLLMs [85]. There are several more concerns associated with LLMs including restricted input and output text lengths, limited comprehension of syntax, ethical considerations with the generated information, constraints with multilingual capabilities, elevated costs associated with training
and maintenance, inadequate understanding of human behaviours, and limited ability to learn incrementally [27, 42, 137]. Despite their considerable capabilities, LLMs are not without risks. Bommasani et al. [13], Lund and Wang [66], Rahman and Santacana [91] provide comprehensive overviews of risks linked to LLMs. These include the homogenisation of outcomes, wherein defects or biases from the foundation model are inherited by all downstream models. There is also the risk of a monopoly for the owners of the foundationmodels, leading to increased centralisation of power in a single entity. Ethical and legal concerns are intertwined with privacy and intellectual property issues. Additionally, there are economic and environmental impacts, raising concerns about the potential displacement of human workers. Furthermore, inequity and misuse of LLMs, such as the creation of deepfakes and their application in criminal and unethical activities, pose additional challenges. Given that LLMs do not inherently prioritise the accuracy of information, Bender et al. [12] have underscored the risk of generating social turbulence, especially, when employed on social media platforms. Additionally, the use of LLMs is associated with significant costs, leading to a direct environmental impact due to their substantial energy consumption [96]. The risks associated with LLMs are predominantly emphasised within Information Communication and Technology (ICT) and cyberspace. Primary concerns include the disclosure of personal information, the generation of malicious text, and the creation of malicious code [31]. In response to the increasing prominence of LLMs and Artificial General Intelligence (AGI), the EU Artificial Intelligence Act (AIA) presently classifies AI systems into four risk categories based on broad fields of application. However, this categorisation approach may result in inaccurate risk estimation and ineffective enforcement. To rectify this issue, the authors advocate for a risk assessment model that aligns the risk categories with specific AI scenarios. This model integrates the AIA with the risk assessment approach employed by the Intergovernmental Panel on Climate Change (IPCC) and relevant literature, facilitating a more precise estimation of the magnitude of AI-related risks [77]. The Beyond the Imitation Game benchmark (BIG-bench), serves as an evaluation framework for LLMs. It encompasses 204 distinct language-related tasks, spanning contextual and context-free question-answering, reading comprehension, etc. [106]. It is acknowledged that the challenge of social biases and English language dependency persists across nearly all LLMs. 5 large language models for digital forensics Section 5 summarises existing work with LLMs in DF, the feasibility of employing them, and potential future directions. As discussed in Sections 3 and 4, despite the widespread use of LLMs in various fields to enhance the efficiency and accuracy of tasks within specific domains, their application in the field of DF is still relatively new. Conducting a thorough analysis of the utilisation of LLMs in conjunction with the stages of the DF process model, as highlighted in Section 2.1, proves to be a valuable undertaking. 5.1 incident recognition phase In the initial phase of Casey’s DF process model, which delineates the recognition of an incident, LLMs can serve as a valuable detection mechanism. In cases of cybercrime, the primary artefacts often involve data logs, data dumps and network dumps. Fine-tuning an LLM to monitor text-based logs and related files enables it to discern and identify potential or ongoing incidents within the environment. In network-related activities, anomaly detection plays a pivotal role in initiating an incident response. Various existing anomaly detection techniques are employed in systems for this purpose. Leveraging its capability to identify patterns in a series of text data sets, LLMs exhibit potential as an Intrusion Detection System (IDS) within such systems [60]. 5.2 collection phase While evidence collection or seizure traditionally involves physical tasks requiring human interaction, LLMs can play a role in identifying and listing potential pieces of evidence at a crime scene. For instance, in the examination of photographs or video records from a crime scene, an investigator can enlist the assistance of a MLLM like LLaVa, GPT-4, or VisionLLM. These models are capable of processing information within the images and generating a text-based output, facilitating the interpretation and categorisation of visual data. While this task may seem simple and within the capabilities of a human agent, the efficiency becomes particularly evident, when dealing with a massive-scale investigation involving thousands of collected artefacts and photographs. Utilising an MLLM for initial
processing can significantly save time, with human agents then focusing on the crucial task of verification and validation. 5.3 preservation/acquisition phase Preserving evidence is centred on maintaining integrity. To achieve this, various tools such as EnCase and FTK Imager have been employed, aiding investigators in streamlining their work processes [101]. In the context of preserving disk evidence, it becomes feasible for an investigator to articulate their requirements in natural language, LLM to generate source code tailored to the specific needs. Indeed, LLMs specialised in code generation, such as StarCoder, Code LLaMA, and others, can be fine-tuned and retrained for specific tasks, including those related to preserving disk evidence through customised code generation. In certain instances, gathering live data for forensic investigations becomes crucial, particularly data collected at the crime scene. For this purpose, investigators may leverage DFaaS platforms like Hansken. Hansken possesses the capability to amalgamate custom extraction APIs for data extractions, and these APIs can be developed using code-generative LLMs [117]. This approach enhances the adaptability and efficiency of the investigative process. As stated in Section 4.5, the automation of code generation and unit testing can be facilitated by autonomous agents that utilise LLMs as their core. AutoGen, being an open-source framework, provides the means to develop AI agents tailored for specific tasks. These AutoGen agents are not only customizable and conversational but can also operate in diverse modes, employing combinations of LLMs, human inputs, and various tools [133]. Positively, automated agents, particularly those developed within frameworks like AutoGen, can be effectively employed in the preservation phase. Their capabilities will enable the automated preservation within this phase, streamlining and enhancing efficiency in the preservation of digital evidence [133]. 5.4 examination phase This phase constitutes a pivotal component of the investigation, playing a crucial role in elucidating the case through activities such as data recovery, collection, reduction, and classification. For each of these components, LLMs fine-tuned for scripting can significantly assist, especially at a larger scale. Within these components, tasks such as keyword search, file recovery, pattern matching, and fragment reassembly can be achievedwithminimal technical knowledge using LLMs. LLMs can provide valuable assistance in these tasks by generating new codes, crafting regular expressions, generating passwords and/or password hash lists for decryption, and creating sample logs or files. LLMs can generate a set of instructions, queries, and Application Programming Interface (API) validations from natural language provided by a human. This opens up the possibility of integrating third-party tools like Scapy, tshark, John the Ripper, and others seamlessly into the investigative process, enhancing the toolkit available for DF investigations and the ability to automate these processes enhances efficiency and effectiveness in the examination phase of the investigation. 5.5 analysis phase The analysis phase involves comprehending the incident and arriving at a conclusive understanding based on the information gathered during the examination phase. As also highlighted in Section 2.3, it has been demonstrated that LLMs are effective in case analysis [43]. The use of MLLMs, which possess the capability to interpret images, broadens the scope for analysing a crime case more comprehensively. LLMs can be specifically fine-tuned for the analysis of various data types, including log files, email contents, chat transcripts, call records, file metadata, hex dumps, memory dumps, and registry hives. Incorporating contents such as event logs, timestamps, and network traffic captures further enables the effective recreation of incidents by correlating each data set with the assistance of LLMs. Also, audio and video-specificMLLMs can assist in analysing the contents within these formats. This specialised capability has the potential to significantly save time for investigators in their analyses of audio and video data during investigations. The use of automated agents can distribute the analysis workload. Simultaneously, leveraging Augmented Large Language Models (ALLM) and Retrieval Augmented Generation (RAG) techniques can enhance real-time knowledge searching, thereby improving the accuracy of analysis and decision-making processes [56]. 5.6 reporting phase The quality and validity of evidence, along with the thoroughness of the analysis, are encapsulated in the final report. The reporting phase holds significant weight, as the entire judgement may hinge on this crucial stage. Notably, DF is experiencing heightened scrutiny with regard to the quality of the reports, emphasising the importance of precision and clarity in this phase [48]. As pointed out by Champod et al. [21], there is no standard framework for evaluating and reporting scientific findings to authorities and stakeholders. To provide assistance and alleviate scrutiny, incorporating LLMs for report creation is a viable solution. While by definition, LLMs do not produce deterministic outputs, they could be retrained using standards such as ISO/IEC 27043:2015, the international standard for the DF investigation process [115]. A first feasibility study has already been released byMichelet and Breitinger [70]. Naturally, LLMs could also help to automate forensic reports and alternative formats (i.e., not MS Word) such as HTML or Latex. 5.7 other possibilities In general, LLMs can be effectively utilised in each phase of the DF process model. By integrating the DF process life cycle with automated agents, it becomes feasible to establish a fully automated DF process throughout an investigation. For instance, if automated agents are assigned specific roles and maintain synchronisation within each agent, employing frameworks such as AutoGen, there is a substantial potential to expedite the completion of one or multiple DF cases compared to relying solely on human agents. This approach enhances efficiency, speed, and consistency in the DF investigative process. Scanlon et al. [99] highlights that LLMs can play a significant role in DF teaching scenarios. This involvement extends to activities such as storyboarding, synthetic content creation, and synthetic character profiling. Additionally, fine-tuned LLMs prove to
be valuable tools for establishing evaluation criteria for DF agents, facilitating the creation of exams, measuring the accuracy and efficiency of investigations through an automated weighing system, and effectively presenting a case to non-technical or non-domain individuals, such as judges. 5.8 challenges for llms in digital forensics To optimise results, LLMs likely need to be trained with specific forensic data (i.e., previous case data) to achieve the best results. Given the complexity and variation of cases, it is questionable how good the training data is and whether there is sufficient data [16]. Biases present in training data can lead to skewed interpretations and unjust outcomes. Investigative findings and drawing conclusions are also heavily based on investigator experience and intuition, which are difficult to replicate using LLMs. Initially, they will only prove effective for certain subtasks within digital forensics, which raises questions about their superiority over deterministic programs in certain contexts. For instance, data may be parsed/converted using a regular piece of software or an LLM. While the former works deterministically, the latter is more general, but requires the practitioners to validate the output. This leads to the fundamental necessity for explainability (i.e., not using blackboxes) for investigations. The explainability of LLM-generated results remains a daunting challenge, as understanding the reasons for their, frequently non-deterministic, results is often elusive [70]. Maintaining the energy- and resource-intensive infrastructure required to train and deploy LLMs is a significant financial burden, making it impractical for smaller forensic laboratories to utilise such technologies. Centralised systems may be an option but require clear guidelines on how data can/may be shared and exchanged. While LLMs can serve as valuable tools to support forensic investigations, it must be recognised that they currently function best as an aid, not a substitute for human expertise [100]. There is a risk that people may place too much trust in the results generated by LLMs (over-reliance), which could lead to complacency and overlook the need for detailed human expert analysis and validation. Finally, ethical and legal considerations must also be discussed. Determining accountability in cases where LLMs produce false information or are compromised by hacking. Clarifying responsibilities between developers, users and regulators is crucial to establish a framework for accountability. If LLM-generated DF results lead to incorrect information, the responsibility may lie with the developers for ensuring the accuracy of the model and with the users for the appropriate interpretation and validation of the results. 5.9 risks of integration The integration of LLMs within the DF process comes with inherent risks – extra to those general LLM limitations outlined in Section 4.6. Notably, in the examination, analysis, and reporting phases, the utilisation of LLMs introduces the risk of producing inaccurate information, primarily due to the phenomenon of inheritance hallucinations associated with these models [70, 100]. Additionally, the biases and obscurities present in an inheritance model may significantly impact the performance of a DF-focused
LLM – potentially leading to the unacceptable generation of biased or inaccurate information within the DF process. It is also crucial to acknowledge that DF LLMs, like any complex models, are susceptible to adversarial manipulation [151]. This vulnerability poses a substantial risk in the context of sensitive domains such as DF, where the integrity of information gained is paramount. Adversarial attacks can compromise the reliability of LLM-generated outputs, potentially influencing the outcomes of various phases within the DF process. Indeed, despite incorporating human verification, outputs and reports generated by LLMs within DF applications may encounter challenges regarding acceptance within the legal systems of different countries. This highlights a significant usability risk associated with LLM-based DF applications, but one that can be carefully mitigated by limiting the technology’s deployment as a human-inthe-loop investigative aid as opposed to directly feeding into any investigative/judicial decision-making processes. 6 conclusion The convergence of LLMs with an array of technologies represents an exciting synergy. Although the utilisation of LLMs in the realm of DF is still in its nascent stages, there is evidence of their substantial potential to significantly augment the efficiency of investigations. The exploration of investment for LLMs across the entire DF process is considered, aiming to enhance the productivity and efficiency of investigations. Additionally, the integration of LLMs into current DF tools is posited to reduce user training times, as these models comprehend natural language input and provide outputs accordingly. In the dynamic landscape of LLM applications within DF, promising avenues for further exploration and advancement unfold. While the surge in LLM research is promising, it is crucial to balance enthusiasm with an awareness of existing challenges. The propensity for LLMs to produce hallucinations highlights the need for human oversight in critical decision-making processes, underscoring the irreplaceable value of human judgement, intuition and expertise. A notable limitation is the language dependency issue, as most LLMs are predominantly trained on English data, reducing their effectiveness with non-English content. Additionally, the deployment of LLMs in DF involves significant costs related to infrastructure for processing evidence. Questions also arise about the validation of task correctness and quality when automated by LLMs, as well as the legal and professional acceptance of results obtained with limited human intervention. Integrating LLMs with automated agents offers a promising path to automating DF processes, potentially allowing multiple cases to be handled concurrently for more timely and precise outcomes. This integration could significantly streamline investigations. Future research should explore the role of LLMs and AI in DF decisionmaking. It is essential to focus on validating LLM-generated outputs to ensure their scope, accuracy, reliability, and trustworthiness in investigations. Further studies comparing DF outcomes with and without LLM integration are critical, as they could highlight LLMs’ benefits and controlled applicability in DF and similar fields. In essence, while LLMs offer exciting prospects for the future of digital forensics, a balanced approach that integrates their strengths
with human oversight is essential for harnessing their full potential. Inevitably, LLM-facilitated DF processes will themselves become the focus of future investigation.