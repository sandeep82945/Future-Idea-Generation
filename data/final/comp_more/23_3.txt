Large Language Model (LLM) has demonstrated significant ability in various Natural
Language Processing tasks. However, their effectiveness is highly dependent on the phrasing
of the task prompt, leading to research on automatic prompt optimization using labeled task
data. We reveal that these prompt optimization
techniques are vulnerable to distribution shifts
such as subpopulation shifts, which are common for LLMs in real-world scenarios such as
customer reviews analysis. In this light, we
propose a new problem of robust prompt optimization for LLMs against distribution shifts,
which requires the prompt optimized over the
labeled source group can simultaneously generalize to an unlabeled target group. To solve
this problem, we propose Generalized Prompt
Optimization framework , which incorporates
the unlabeled data from the target group into
prompt optimization. Extensive experimental
results demonstrate the effectiveness of the proposed framework with significant performance
improvement on the target group and comparable performance on the source group. 1 Introduction
LLMs have gained significant attention for their
remarkable performance in a broad range of Natural Language Processing (NLP) tasks (Ouyang
et al., 2022; Chung et al., 2022; Brown et al., 2020;
Touvron et al., 2023). This success has led to a
shift in the paradigm of solving NLP tasks, moving away from training task-specific deep models
towards developing task-specific strategies to effectively utilize LLMs (Wei et al., 2022; Kojima et al.,
2022; Wang et al., 2022a; Ye et al., 2023b). In the
new paradigm, the prompt becomes a crucial factor
in ensuring the effectiveness of LLM on the NLP
task, since even slight variations in prompt phrasing can largely affect LLM output (Reynolds andMcDonell, 2021; Gao et al., 2021), making prompt
optimization a promising research direction.
Existing research has explored automatic prompt
optimization methods to eliminate manual effort
in identifying effective prompts for a given task.
These methods can be gradient-based or gradientfree, depending on the availability of model gradients. Gradient-based methods optimize the prompt
by calculating its gradients through the LLM
(Schick and Schütze, 2021b,a; Hu et al., 2022).
Gradient-free methods update prompts based on
LLM outputs using techniques such as an iterative search-and-select over the prompt space (Zhou
et al., 2023; Prasad et al., 2022; Pryzant et al.,
2023). This work focuses on gradient-free prompt
optimization as LLMs are evolving into black-box
API services (Sun et al., 2022).
Current gradient-free prompt optimization methods ignore distribution shifts (Wang et al., 2023), where the data an LLM serves may differ from
the labeled data for prompt optimization. Realworld NLP applications often encounter distribution shifts, such as new user groups with distinct
linguistic habits in customer review analysis. It is
unclear if prompts hinder the robustness of LLMs
against distribution shifts. To answer this question,
we conduct experiments with the representative gpt3.5-turbo-0301 model and prompts optimized by
APE (Zhou et al., 2023) over paired data groups
with distribution shifts. Results on 30 pairs of data
groups from six tasks show the risk of significant
performance gaps under certain distribution shifts.
Based on this finding, we propose a new robust
prompt optimization problem, which aims to optimize task-specific prompts with consideration of
performance on both source and target groups under different distributions. Given an NLP task such
as sentiment analysis, our problem setting has a
labeled source group similar as the conventional
prompt optimization setting and a unlabeled target
group. We keep the target group unlabeled for the
consideration that distribution shifts happen along
time in practice. Labeling the newly coming target
group will cause unnecessary labor cost and latency.
Accordingly, the main challenge for solving this robust prompt optimization problem is incorporating
unlabeled data into prompt optimization.
To this end, we propose the Generalized Prompt
Optimization (GPO) framework to obtain a taskspecific prompt for both source and target groups.
To jointly considering the two groups in prompt optimization, the key lies in labeling the target group
in an automatic and reliable manner by adapting
knowledge from the labeled source group. Towards
this goal, we leverage the strong power of LLM
in zero-shot labeling, and prompt ensemble to enhance the labeling robustness. Experimental results
on three tasks demonstrate the effectiveness of our
framework in improving the performance on the
target group and simultaneously preserving a comparable performance on the source group. To sum
up, our contributions are threefold:
• We reveal the robustness issue of prompt optimization against distribution shifts and propose
a new robust prompt optimization problem.
• We propose the Generalized Prompt Optimization framework, which generates robust prompts
considering both labeled and unlabeled data.
• We conduct extensive experiments on three NLP
tasks, validating the rationality and effectiveness
of our proposed framework.
2 Preliminary Experiments
Prompt optimization aims to find the best prompt
p that can instruct LLMs to predict the output y
based on the concatenation of p and task input
x, where x, y and p are all sequences of tokens.
Formally, given an NLP task with a dataset {(x, y)}
following a distribution P, the goal is to obtain
p
o = arg max
p∈Z
E(x,y)∼P [r(LLM(p, x), y)], (1)
where Z denotes the prompt optimization space
and r is the evaluation metric to compare the LLM
output with the ground truth output y, e.g., Accuracy. Existing studies usually leverage gradientbased or gradient-free methods to automatically
optimize the prompts. Since LLMs are evolving
into black-box API services, gradient-free methods
become increasingly important. However, they ignore distribution shifts between training and testing
data. In this light, we conduct controlled experiments to answer the following research question:
Are prompts optimized by existing gradient-free
methods robust to distribution shifts?
2.1 Evaluation Protocol
We conduct the controlled experiments between
a pair of data groups with distribution shifts, i.e.,
a source group {(xs, ys
)} following a distribution
Ps, and a target group {(xt
, yt
)} with a distribution Pt
, where Pt ̸= Ps. We intend to examine
whether the prompt p
s optimized on the source
group can generalize to the target group. Specifically, given p
s
and p
t optimized on the target group,
we compare the performance of p
s on the target
group E(x,y)∼Pt
[r(LLM(p
s
, x), y)] with that of p
t
E(x,y)∼Pt
[r(LLM(p
t
, x), y)].
Datasets. We select 16 datasets from six popular NLP tasks, where each pair of groups under
the same task is treated as the source and target groups. Following recent out-of-distribution
(OOD) research (Yang et al., 2022), we take each
dataset as a group and regard different backgrounds
and topics across the datasets as the distribution
shifts. For the sentiment analysis task, we adopt
Yelp (Zhang et al., 2015), Flipkart (Vaghani and
Thummar, 2023), IMDB (Maas et al., 2011) and
Amazon (Zhang et al., 2015) of different topics. For the natural language inference task, we utilize MNLI (Williams et al., 2018), and ANLI (Nie
et al., 2020) which is an adversarial dataset for
MNLI. For the textual entailment, we use RTE
(Wang et al., 2018) and its OOD dataset HANS
(McCoy et al., 2019). For commonsense QA, we
use SocialIQA (Sap et al., 2019), PIQA (Bisk et al.,
2020), and OpenbookQA (Mihaylov et al., 2018),
which focus on different types of commonsense
knowledge. For the multi-turn dialog reasoning,
we use DSTC7 (Gunasekara et al., 2019), Ubuntu
Dialog (Lowe et al., 2015), and MuTual (Cui et al.,
2020). Besides, for the numerical QA task, we use
the samples of two different answer types (i.e., numerical values and text spans) in DROP (Dua et al.,
2019) as two groups. See Appendix A.1 for details.
Experimental Setup. We adopt APE (Zhou et al.,
2023), an effective gradient-free prompt optimization method, for prompt generalization analysis.
To highlight the effect of prompts, we conduct experiments under the zero-shot setting without incontext examples. For the backbone LLMs, we
leverage gpt-3.5-turbo-0301 by calling the OpenAI
API1
. For all classification tasks (all tasks except
for DROP), we use accuracy as the evaluation metric. For DROP, we utilize its standard evaluation
metric — F1. Following the setting of APE, we
randomly sample N-shot training and N-shot validation samples for prompt optimization, and repeat
the experiments for five runs with different sampled data to report the averaged results. More implementation details can be found in Appendix A.2.
2.2 Experimental Results
Demonstration of Generalization Performance
Gap. Table 1 shows the tasks without a large
generalization gap between the performance of
prompts p
s
and p
t
, and Table 2 shows the tasks
with large gaps (Accuracy gap>8.0) on some
groups. The row headers refer to the source groups
for prompt optimization while the column headers
show the target groups to test optimized prompts.
The generalization performance gap between p
s
and p
t
can be observed by comparing the values in
the same column.
From the tables, we can observe: 1) The generalization performance gap may not exist for previously studied OOD and adversarial groups (see
Table 1), including the groups of the natural language inference and the textual entailment tasks.
This is possibly attributed to the strong generalization ability of LLMs. 2) However, under some data
groups of Table 2 such as the sentiment analysis
datasets (e.g., Flipkart and Yelp) and the commonsense QA datasets with different topics (e.g., PIQA
and OpenbookQA), and the DROP groups with
different answer types, there are still significant
generalization performance gaps, demonstrating
the existence of the generalization issue of prompt
optimization. 3) Surprisingly, the prompt p
s optimized from the source group does not always
perform worse than the prompt p
t optimized on the
target group. In Table 2(b), p
s
from OpenbookQA
performs even better than p
t
for SocialIQA. Besides, for DROP in Table 2(c), p
s
from Spans also
performs better than p
t
from Number. In the following section, we try to explore the reasons for
the above three observations.
Exploration on the Factors Affecting Prompt
Robustness. Based on the above observations,
we further explore two research questions.
Q1: Why do the prompts optimized on source
groups perform differently on a target group?
Q2: Why does the prompt optimized on the source
group perform even better than the prompt optimized on the target group in some cases?
For Q1, we conjecture that the varied performance gaps are attributed to different distribution
shifts between the source and target groups. To
verify this, we examine two metrics to measure
two kinds of distribution shifts: 1) the label shifts
measured by the KL divergence, and 2) the input
similarity quantified by the n-gram similarity of
the input corpora of the two groups. Their detailed
implementation is illustrated in Appendix A.3. We
show the results of the sentiment analysis task as
an example in Table 3. We can observe that the
smallest label distribution shifts and the largest input similarity in Table 3 generally coincide with
the best generalization performance on each target group in Table 2, indicating the correlation
between distribution shifts and generalization performance. Nevertheless, the two metrics cannot
perfectly explain the performance on all tasks (cf.
Appendix A.3).
For Q2, we conjecture that the outstanding generalization performance is because a source group
with large diversity covers heterogeneous patterns
in the target group, leading to a more robust prompt
p
s
than p
t
. To explore this, we measure the heterogeneity of source and target groups by calculating the percentage of unique n-grams, and the
percentage of n-grams of the target group covered
by the source group. For illustration, we present
the results of the commonsense QA task in Table 4.
From Table 4(a), we can observe that OpenbookQA
has the most diverse input according to the n-gram
statistics. Moreover, OpenbookQA covers a large
proportion of n-grams of SocialIQA and PIQA.
These partly explain the superiority of the prompts
optimized on OpenbookQA (see Table 2).
3 Robust Prompt Optimization
In this section, we first formulate a robust prompt
optimization problem and propose a GPO framework to enhance the robustness of the prompts.
3.1 Problem Definition
To enhance the generalization ability of prompts,
we propose a robust prompt optimization problem. Specifically, given an NLP task such as sentiment analysis, it aims to optimize a task-specific
prompt for the data groups with different distributions. We consider the popular scenario where
a source group Gs = {(xs, ys
)} following a distribution Ps and {xt} in a unlabeled target group
Gt = {(xt
, yt
)} ∼ Pt (Pt ̸= Ps) are available
while {yt} is unseen during prompt optimization.
The objective becomes utilizing Gs = {(xs, ys
)}
and {xt} to optimize a task-specific prompt robust
to the samples from either Ps or Pt
.
Reasons for Access to Unlabeled Target Group.
In a real-world deployment, LLMs continually encounter the testing data with distribution shifts. Collecting the input features {xt} of the target group
is feasible. For example, when using LLMs as web
services to solve user queries of certain NLP tasks,
it is easy to collect extensive user queries as unlabeled target groups. However, labeling {xt} may
be time-consuming and costly, and thus we intend
to optimize robust prompts without the labels of
the target group.
A Task-Specific Prompt vs. One Prompt for
Each Group. To tackle the generalization issue
of optimized prompts, an intuitive approach is to
optimize a separate prompt for each data group, yet
this simplistic approach faces several limitations
in real scenarios. In real-world deployment, it not
only requires additional computation costs to construct more prompts, but also needs to accurately
classify each testing sample into the appropriate
group of the same distribution, thereby resulting
in increased computation costs, latency, and new
challenges for precise group classification. Furthermore, the collected source group data cannot cover
all potential target groups, and the prompts optimized on the source groups may inevitably test on
the examples from previously unseen groups. Thus,
we aim at improving the generalization ability of
one task-specific prompt across different groups.
3.2 GPO Framework
To obtain a robust prompt for both the source and
target groups, it is natural to jointly consider Gs
and Gt for prompt optimization. However, Gt
lacks
the labels {yt} that are commonly required by the
gradient-free optimization methods (refer to Table 5 for the inferior results without labeling). With
the impressive capabilities of LLMs on zero-shot
labeling, we propose to utilize LLMs to label {xt}.
Considering that noisy labels may damage the quality of optimized prompts, we further present two
strategies to improve labeling accuracy.
As illustrated in Figure 2, we first propose a Meta
Prompt to instruct LLMs to acquire knowledge
from the labeled source group and generate a series
of prompts. Thereafter, we utilize a prompt ensemble labeling strategy to apply generated prompts
to an LLM for precise labeling of {xt}. In detail,
we derive a three-step framework to perform the
labeling with two strategies, and then conduct joint
prompt optimization as shown in Figure 2.
1. Prompt Generation via Meta Prompt. Following APE, we utilize a Meta Prompt to ask
LLM to generate prompts for labeling by feeding the examples of Gs (see an example in Figure 2). Based on strong language understanding
and reasoning abilities, LLMs can infer the relationships between the inputs and outputs of
the examples and provide general and precise
task prompts. We use different splits of Gs to
generate K different prompts in total.
2. Prompt Ensemble Labeling Strategy. Given
K prompts, we utilize each of them to label {xt}
with an LLM, and thus obtain K candidate labels for each example. We adopt an ensembling
strategy and select the label with the highest
consistency among the K candidate labels for
each example. Besides, inspired from Wang
et al. (2022a), we set a consistency threshold
T ∈ [0, 1] to only accept the labeled examples
that have more than T percent of prompts agreed
on the label. Eventually, we obtain a filtered labeled set G∗
t
for the target group.3. Joint Prompt Optimization. Finally, we mix
Gs and G∗
t
to run APE for joint prompt optimization and obtain the final optimized prompt.
As G∗
t may have fewer samples than Gs after
filtering with T, we perform a random upsampling on G∗
t
to have the same data number as Gs
before running APE. A brief illustration about
APE can be found in Appendix A.2.
4 Experiments
4.1 Setup
Datasets. We experiment GPO with three tasks:
sentiment analysis, commonsense QA, and DROP.
For each task, we select a pair of groups with generalization performance gap as source and target
groups, and ablate the labels for the target groups.
Compared Methods. We adopt the following baseline methods: 1) APE; 2) APO (Pryzant et al.,
2023), the state-of-the-art gradient-free prompt optimization method for LLM; 3) APE-ut, a naive
generalization solution by incorporating the unlabeled target group input into APE; 4) the Upper
Bound, which represents the performance of the
prompt optimized on the target group data with
ground-truth labels by APE; and 5) our proposed
GPO; We also show the results of simple humanwritten prompts that are general for the task, and
the revised versions by PromptPerfect2 which is an
automatic prompt engineering website.
Evaluation Protocol. We utilize two strategies
for testing: Top 1 and Ensemble. Top 1 refers to
using the single optimized prompt with the best
validation performance, while Ensemble refers to
labeling with all obtained K prompts and accept
the output with the most agreement on the prompts.
We utilize the same N-shot data as the preliminary
experiments and also report the averaged results
for five runs. More implementation details are illustrated in Appendix A.4.
4.2 Performance Comparison
Compare to Generated Prompts. From Table 5,
we can observe the followings: 1) GPO achieves
superior performance for all target groups in both
Top 1 and Ensemble testing, validating its effectiveness. However, there is still space for improvement
towards the Upper Bound for all tasks, showing the
challenge of the generalization problem. 2) GPO
achieves comparable source group performance for
all tasks, showing its improvement on the targetgroup does not largely hinder the source group.
Compared with APE, GPO shows increased performance on the source groups of SocialIQA and
Number by incorporating the target group data,
which is in line with the finding in Table 2. 3)
Across baselines, APO outperforms APE on the
source groups of the last two tasks and achieve comparable performance on sentiment analysis, showing its effectiveness for prompt optimization. However, the generalization ability is only comparable
to APE since APO performs worse than APE on
several target groups. 4) APE-ut achieves improved
target group performance for the first two task, indicating the benefit of incorporating unlabeled target
group data for generalization. However, for Spans
where obtaining accurate target labels is challenging (as shown by the low F1 values), APE-ut largely
underperforms GPO, showing the importance of
target group labeling especially for difficult tasks.
Compare to Human-written Prompts. From
Table 6, we further observe that GPO outperforms
human-written prompts and PromptPerfect for sentiment analysis and commonsense QA tasks. However, on the most difficult task DROP, GPO underperforms human-written prompts. This is potenWe study the effect of prompt ensemble labeling
and joint prompt optimization by evaluating two
modifications of GPO: (1) setting the consistency
threshold as 0, denoted as w/o cons; and (2) removing the target group training data during the final
prompt generation, denoted as w/o cons+t-train.
From Table 7, we can observe that: 1) In all cases
except for Flipkart with Top 1 evaluation, GPO
performs better than w/o cons on target groups,
showing the effectiveness of the consistency threshold. 2) Among the three tasks, DROP has large
improvement between w/o cons and GPO on both
source and target groups then the other two tasks.
We hypothesis that this discrepancy is related to the
different degrees of improvement in the labeling
accuracy by the consistency threshold, which will
be further discussed in Section 4.4. 3) Comparingw/o cons and w/o cons+t-train, removing the target
group training data benefits the Top 1 results of the
source group, but harms the Ensemble results of
the target groups. It has less effect on the target
group Top 1 results since the two methods still use
target group validation data.
4.4 In-depth Analysis
Analysis on the Effect of the Consistency Threshold. To further reveal the effect of consistency
threshold, we first show the labeling accuracy of
the target group training and validation data for
GPO and w/o cons in Table 8. We can observe that
applying the consistency threshold can improve
the labeling accuracy for all target groups. By
examining the relationship between this labeling
accuracy improvement and the performance difference between GPO and w/o cons in Table 7, it can
be explained that for Flipkart and OpenbookQA,
where the labeling accuracy is already high under
w/o cons, further improving the labeling accuracy
by the consistency threshold is unlikely to achieve
large performance gain. Conversely, in the case
of Spans with low labeling accuracy, even a minor
improvement can result in significant performance
gains. To explore the connection between labeling
accuracy and target group performance further, we
conducted an experiment where we manually assigned incorrect labels to varying proportions (0%,
50%, and 90%) of the target training and validation data. The results are illustrated in Figure 3.
It can be observed that as the percentage of incorrect labels increases, the overall performance on
the target group generally decreases, emphasizing
the importance of labeling accuracy for achieving
effective generalization.GPO with Different Backbone LLMs. We also
conducted experiments with GPO using different
backbone LLMs, including Vicuna 7B and 13B
(Chiang et al., 2023) which are notable smallersized LLMs, and GPT-4 (OpenAI, 2023). Table 9
shows the generalization results on Flipkart with
Yelp as the source group for APE and GPO on different backbone LLMs. Due to the small sizes of
the Vicuna models, generating the exact sentiment
label as the answer can be challenging. Therefore,
we extract the sentiment labels from their outputs
before calculating the accuracy. The results show
that there is room for enhancing the generalization
performance in APE across various LLMs, and
GPO consistently outperforms APE in all cases.
Notably, when applying GPO to the smaller Vicuna
7B model, there is a significant improvement that
allows it to reach the same performance level as
the Vicuna 13B model. Across LLMs, the smallersized Vicuna models achieve relatively worse performance, and the powerful GPT-4 achieves the
best performance on GPO.
5 Related Work
Generalization Ability and Robustness of LLM.
Researchers have been investigating the generalization ability and robustness of LLMs since
their recent breakthrough. LLMs like ChatGPT
have shown significant improvement in out-ofdistribution (OOD) and adversarial tasks (Wang
et al., 2023), although they are still imperfect (Chen
et al., 2023). Some LLMs still rely on shortcuts
and spurious correlation (Tang et al., 2023; Stolfo
et al., 2022). Moreover, LLMs remain vulnerable
to adversarial perturbations and achieve inconsistent results (Wang et al., 2023; Ye et al., 2023a;
Liang et al., 2022). Additionally, LLMs demonstrate high sensitivity to the prompt (Reynolds and
McDonell, 2021; Zhu et al., 2023) and the selection of in-context examples (Liu et al., 2022; Rubin et al., 2022). Lastly, instruction tuning allows
LLMs to generalize to novel tasks (Ouyang et al.,
2022; Wang et al., 2022b,a). We specifically focus
on the generalization issue of prompt optimization
on the distribution shifts within one task.
Prompt Optimization. Obtaining effective
prompts for applying LLM in NLP tasks is a popular research area. Prompt tuning methods (Li and
Liang, 2021; Lester et al., 2021; Qin and Eisner,
2021; Gu et al., 2022) learn soft continuous vectors
as prompts in the LLM input using gradients
from the task objective. Recent studies have also
focused on gradient-free prompt optimization for
black-box LLM, such as reinforcement learningbased methods (Zhang et al., 2023; Deng et al.,
2022; Diao et al., 2022), search-based methods
(Brown et al., 2020; Prasad et al., 2022; Pryzant
et al., 2023), and other gradient-free optimization
techniques like evolutionary algorithms (Sun et al.,
2022) and boosting (Hou et al., 2022). Among
them, the state-of-the-art methods leverage the
power of LLMs for prompt optimization, such as
prompt generation and evaluation by LLM (APE
(Zhou et al., 2023)) and prompt editing following
critiques (APO (Pryzant et al., 2023)), where we
mainly compare with them. Notably, while some
previous work on prompt tuning has addressed
generalization across tasks and models (Su et al.,
2022; Vu et al., 2021; Qin et al., 2023), and domain
adaptation (Tam et al., 2022; Guo et al., 2022), this
paper specifically focuses on the generalization
issue of gradient-free prompt optimization.
6 Conclusion
In this paper, we revealed the generalization issue
of prompt optimization for LLMs under distribution shifts. We observed that the prompt optimized
on the source data group may have a performance
drop on the target group with distribution shifts.
We performed an initial analysis aiming at identifying the factors that correlate to the varied generalization performance across groups, including
label distribution shift and input distribution similarity. To enhance the generalization ability of
LLMs, we proposed a Generalized Prompt Optimization framework to jointly consider the source
and target groups for robust prompt optimization.
Experimental results validated the effectiveness of
our proposed framework in boosting the robustness
of the prompts on the source and target groups.  Limitations
Firstly, this work discusses the generalization ability of prompts while ignoring the effect of other
LLM inputs such as in-context examples. The
choice of in-context examples might also affect
the robustness of LLMs. Secondly, this work
assumes the availability of the inputs {xt} of the
target group. It is under-explored how to achieve
generalized prompt optimization to completely unseen groups without {xt}. Thirdly,
we acknowledge that the scope of our research
is limited to black-box LLMs capable of understanding instructions, where gradient-free prompt
optimization with instructing LLM is a suitable
choice. For smaller LMs without instruction understanding abilities, e.g., BERT (Devlin et al., 2019)
and T5 (Raffel et al., 2020), they are generally not
black-box and are more advantageous to utilize
gradient-based prompt optimization methods. 