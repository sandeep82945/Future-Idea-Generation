Neural machine translation has achieved great success in the past few years with the help of transformer architectures and large-scale bilingual corpora. However, when the source text gradually grows into an entire document, the performance of current methods for documentlevel machine translation (DocMT) is less satisfactory. Although the context is beneficial to the translation in general, it is difficult for traditional methods to utilize such long-range information. Previous studies on DocMT have concentrated on extra contents such as multiple surrounding sentences and input instances divided by a fixed length. We suppose that they ignore the structure inside the source text, which leads to under-utilization of the context. In this paper, we present a more sound paragraph-to-paragraph translation mode and explore whether discourse structure can improve DocMT. We introduce several methods from different perspectives, among which our RST-Att model with a multi-granularity attention mechanism based on the RST parsing tree works best. The experiments show that our method indeed utilizes discourse information and performs better than previous work. 1 introduction Transformer (Vaswani et al., 2017) based approaches, together with adequate bilingual datasets, have led to significant progress on machine translation (MT). However, the performance of MT models usually drops dramatically when processing long texts. Although document-level machine translation (DocMT) can be solved with sentencelevel MT by translating each sentence separately, the potential information in the long-range context may be ignored. To address these problems, many methods in DocMT have been proposed to better utilize the contextual information and improve the overall translation quality of the document. Among these methods, the dominant approaches still adhere to the sentence-by-sentence mode, but
they utilize additional contextual information, including the surrounding sentences (Zhang et al., 2018; Miculicich et al., 2018; Kang et al., 2020; Zhang et al., 2020b, 2021a), document contextual representation (Jiang et al., 2020; Ma et al., 2020) and memory units (Feng et al., 2022). In recent years, many researches have turned to translating multiple sentences or the entire document at once (Tan et al., 2019; Bao et al., 2021; Sun et al., 2022; Li et al., 2022). However, previous work (Zhang et al., 2018; Liu et al., 2020; Sun et al., 2022) has demonstrated that direct Doc2Doc translation may cause the model not to converge. Therefore, those methods adopt pertinent measures such as data augmentation, text truncation, and specific frameworks for sentence alignment. Despite the effectiveness of previous efforts on DocMT, some research, such as Kim et al. (2019), suggested that existing methods may not fully utilize the context and the improvement may come from regularization. Kang et al. (2020) also indicated that dynamic context selected from the surrounding sentences can improve translation more efficiently. The additional contexts used in most previous work are simply based on the distance from the current sentence or the length of the text, which is somewhat arbitrary. In this paper, we aim to explore a more reasonable way to encode context through the discourse structure. Discourse structure refers to how elementary text units are organized to form a discourse and logically linked to one another. Early studies have demonstrated that discourse parsing can benefit various downstream NLP tasks, including sentiment analysis (Bhatia et al., 2015), relation extraction (Wang et al., 2021), text summarization (Gerani et al., 2014; Xu et al., 2020), machine translation evaluation (Guzmán et al., 2014; Joty et al., 2014, 2017; Bawden et al., 2018) and so on. RST parsing, based on Rhetorical Structure Theory (Mann and Thompson, 1987), is one of
13889
the most influential parsing methods in discourse analysis. According to RST, a text is segmented into several clause-like units (EDUs) as leaves of the corresponding parsing tree. Through certain rhetorical relations among adjacent spans, underlying EDUs or larger text spans are recursively linked and merged to form their parent nodes, representing the concatenation of them. Although RST parsing would be better conducted on an integral text to maintain discourse structure, existing models perform poorly on long texts. As a result, we present a new paragraph-toparagraph translation mode, where the original document is divided into several shorter paragraphs. Our paragraph segmentation is generated by the TextTiling tool (Hearst, 1997) based on subtopic shifts and discourse cues, since frequently-used datasets of DocMT do not contain paragraph alignment tags. We suppose the discourse analysis of paragraphs is a proper compromise and more sound than previous partitioning methods. And it is more labor-saving when dealing with other multilingual corpora equipped with paragraph alignments. We employ the end-to-end method of Hu and Wan (2023) to train our RST parsing model. The parsing task is reformulated into a Seq2Seq task through a linearization process and then trained based on a pretrained language model. Several attempts have been made from different perspectives to investigate how to utilize the discourse structure, including the integration of the RST sequence, an RST-specific attention mechanism, and graph learning based on the RST tree. Among them, our RST-Att model, designed with a multi-granularity attention mechanism to inject the RST tree into the translation encoder, achieves the best performance. Specifically, each token at the first encoder layer can only notice other tokens from the EDU it belongs to, when calculating self-attention scores. As the encoder layer moves backward, the range of attention continuously expands to complete context according to the structure of the RST tree. We believe such a progressive pattern can reduce the difficulty of modeling long-range context. Overall, our main contributions are as follows:
1) We generate the paragraph segmentation and introduce a more sound paragraph-to-paragraph translation mode than traditional text partition. 2) We explore several methods to take advantage of the discourse structure predicted by the RST
parsing model to improve document-level machine translation. 3) Our RST-Att model achieves superior performance on three widely used datasets of DocMT and further linguistic evaluation, compared with existing works.1 2 methodology In this section, we will elaborate on the relevant steps to obtain the discourse structure information and explore its utilization in DocMT. In Section 2.1, we compare two text segmentation methods and determine the TextTiling tool for our paragraph segmentation according to the distribution of results. In Section 2.2, we introduce the brief training process of the RST parsing model and how to linearize the corresponding RST tree. Section 2.3 describes the construction and details of our proposed RST sequence integration method, as well as the RST-Att model with the RST-specific attention mechanism of multiple levels of granularity. 2.1 paragraph segmentation In this step, we consider two distinct approaches for paragraph segmentation. The first is the traditional TextTiling method, which detects topic and lexical co-occurrences. The other is a neural segmentation model proposed in recent years by Lukasik et al. (2020), based on a pretrained language model and cross-segment attention. TextTiling Algorithm TextTiling is an algorithm for tokenizing a document into multiparagraph units that represent subtopics, proposed by Hearst (1997). The algorithm identifies subtopic shifts based on the discourse cues of lexical cooccurrence and distribution patterns. It first divides the whole document into several parts, each of which contains multiple unbroken sentences. Then the similarity scores are calculated between sentences, and the peak differences between them are found to become the final segmentation boundaries after the normalization. Neural Segmentation Model Lukasik et al. (2020) proposed a neural model with transformerbased architectures. They represented each candidate break using its left and right local contexts and then turned to the pretrained language model
1Codes and data are available at https://github.com/herrxy/RST-DocMT. for judging whether the candidate break was reasonable. The LM was finetuned on a large-scale dataset, Wiki-727K (Koshorek et al., 2018). We apply the two methods above to the most commonly used dataset of document-level machine translation and obtain the respective paragraph segmentation. We take the dataset News as an example, and the distributions of the number of tokens and sentences contained in each paragraph are shown in Figure 1 and Figure 2. The vertical axis of the legends shows the proportion of the corresponding category in all samples. The paragraphs from the TextTiling method have a more reasonable distribution, and most of them contain moderate numbers of tokens and sentences. On the other hand, a considerable part of the paragraphs obtained by the neural segmentation model only contain a few sentences, which is not conducive to subsequent discourse parsing. Therefore, we finally choose the results of the TextTiling method for the following experiments, and the statistical details of paragraph segmentation can be found in Table 1. 2.2 rst parsing Previous studies have proposed many methods for RST parsing (Lin et al., 2019; Zhang et al., 2020a; Kobayashi et al., 2020; Nguyen et al., 2021). However, most of them split the parsing process into two steps: EDU segmentation and RST tree prediction, for which the gold EDU labels are often required. Considering that the datasets of DocMT are not equipped with such information, we follow Hu and Wan (2023) to train an end-to-end RST parser from scratch through a Seq2Seq reformulation method. The training data comes from the standard RST Discourse TreeBank (Carlson et al., 2001) and has been processed with a similar length distribution to our paragraph segmentation. Linearization Based on the priority of brackets, we represent hierarchical architecture by nesting several pairs of brackets. The linearization is carried out from the bottom up, according to postorder traversal. We replace each leaf that represents a single EDU with a sequence comprised of a left bracket, text content, a right bracket, and its nuclearity and rhetorical relation labels. The same process is performed for other nodes, except that the concatenation of new representations of two child nodes serves as the text content. The linearized sequence is designed to contain the complete original input text for better performance, according to the observation of Paolini et al. (2021). More details can be found in Hu and Wan (2023), and an example is shown in Figure 3(d). Seq2Seq Training Since the input and new output of the task are both sequences, we have trained our RST parsing model on the pretrained Seq2Seq model T5-base (Raffel et al., 2020). The related latent knowledge may be transferred and utilized during training since the reformulated sequences are close to natural language text, which aligns with the pretraining of T5. Moreover, we modify and align the output predicted by the model with the format we design before to obtain the final RST parsing tree through a recursive algorithm. 2.3 utilizing rst structure  2.3.1 rst sequence integration We first attempt a simple method that directly integrates the discourse parsing tree into the inputs of the model, called RST-Seq. The source input during training is replaced with the corresponding linearized RST tree, and the target output is
kept the same. The experiments have been made on the well-trained mBART25 (Liu et al., 2020), similar to the previous work. We expect that the pretrained language model can encode the information of discourse structure together with the context through latent knowledge. The results of our RSTSeq method and the baseline without the discourse structure will be shown in Section 4. 2.3.2 multi-granularity attention To further take advantage of the discourse structure, we propose the RST-Att model with a multigranularity attention mechanism based on the RST tree, inspired by Wu et al. (2018). From the first layer of the encoder to the last, the range that each token can attend to continuously expands according to the bottom-up nodes of the RST parsing tree. According to Beltagy et al. (2020), context may be better integrated in the higher layer, whereas the lower layer tends to encode local information. We suppose the model will better understand the source context in a progressive way under the instruction of discourse structure. Specifically, we first transform the RST parsing tree, combining the leaf nodes and their parent nodes. Then each node in the tree is assigned its height, which refers to the number of edges on its longest path to a leaf node. We ignore the label information and replace each node with the range of EDUs it represents, as shown in Figure 4. Obviously, each leaf node has a height of 0, and only the root has the largest height. We construct the initial node set Sl which consists of all nodes with heights no more than l. Then we delete the node from Sl if it is a descendant node of another node in Sl, until
the set will not change. It can be simply proved that the text ranges represented by the nodes of finally obtained set Ŝl perfectly cover the entire paragraph content. We assume that Ŝl = (r1, r2, · · · , rnl), where the node rk covers the token range from the position of kbegin to kend in the paragraph. In the encoder layer l of the original transformer model, the multihead self-attention is computed as:
Al = MultiHead(Softmax( QlK T l√ d ))
where Ql and Kl are the query and key matrices with the vector dimension of d in the l th layer. We then modify the calculation of the attention matrix Al to Âl according to the text ranges in Ŝl:
Âl = MultiHead(Softmax( QlK T l√ d +M l))
M lij = { 0 ∃ rk ∈ Ŝl, kbegin ≤ i, j ≤ kend −∞ else
where the matrix M l has the size of N ×N and N presents the number of tokens in the paragraph. Through the modified attention mechanism, different granularities for context modeling can be implemented by different encoder layers with specific attention ranges of tokens. Specifically, at the first layer, each token can only notice other tokens from the EDU that it belongs to. As for the last layer, each token can attend to all tokens in the paragraph, which turns back to the original full attention mode. We simulate as many levels of granularity as possible, and our method is illustrated with a simple example in Figure 4. The RST-Att model
does not introduce additional parameters; instead, it theoretically reduces computational overhead and improves efficiency with incomplete attention. Moreover, it should be mentioned that the encoder contains a fixed number of layers, while the heights of RST trees vary. So we should construct a mapping to guarantee each encoder layer is linked to a certain node set. To keep the mapping uniform, the new set S̃i is calculated as follows:
S̃i = ˆSmi , mi = ⌊ H
L− 1 i⌋
where i = 0, 1, · · · , L − 1 and L,H denote the number of encoder layers and the height of the RST tree respectively. And mi refers to the old layer index to be mapped. 3 datasets and settings We evaluate our models on three widely used datasets for document-level machine translation for English to German, from Maruf et al. (2019). TED The corpus includes TED talks and corresponding translations from IWSLT 2017. tst20162017 is used as the test set and the rest as valid. News The corpus comes from News Commentary v11. Newstest2016 is used as the test set and newstest2015 as valid. Europarl The corpus is extracted from Europarl v7, which is split into the training, test, and valid sets, as mentioned in Maruf et al. (2019). The detailed statistics of these datasets and their paragraph segmentation are displayed in Table 1. Similar to the previous work, Moses (Koehn et al., 2007) is used for data processing and sentence truecase. We apply BPE (Sennrich et al., 2016) with 32K merge operations on both sides for all datasets. Following previous work (Bao et al., 2021; Li et al., 2022; Sun et al., 2022; Feng et al., 2022), we apply sentence-level BLEU score (s-BLEU) and document-level BLEU score (d-BLEU) as the metrics of evaluation. Since our methods are focused on the DocMT and do not involve sentence alignments, the d-BLEU score is our major metric, which matches n-grams in the whole document. Since our method can be directly applied to pretrained language models that have been commonly employed in current research, we conduct our experiments based on mBART25 (Liu et al., 2020). It is a strong multilingual model with the transformer architecture and contains about 610M parameters. The setting aligns with the previous state-of-theart model G-Transformer (Bao et al., 2021), which serves as our primary comparison objective. More experiment details are described in Appendix A. 4 experiments and analysis We compare the experimental results of our methods with previous approaches to document-level machine translation on TED, News, and Europarl, as shown in Table 2. We choose the best result of Sent2Sent here from Sun et al. (2022), which even surpasses some DocMT models on d-BLEU, indicating the underuse of context in their work. Moreover, some earlier studies are ignored since they only reported s-BLEU scores and performed much worse than recent work such as G-Transformer. And we categorize all methods based on whether they are based on pretrained language models. Above all, we introduce our Para2Para baseline, which is finetuned on the original mBART25 model. It simply performs paragraph-to-paragraph translation based on the segmented paragraphs without utilizing any RST discourse information. For better comparison, we also present the results of the traditional sub-document baseline SDoc2SDoc from Bao et al. (2021). As described in Section 1, many studies on DocMT have adopted splitting each document into sub-documents for translation
based on a specified maximum length, since standard Doc2Doc translation was pointed out to be prone to training failures. The results show that our Para2Para baseline outperforms the traditional SDoc2SDoc with the same mBART25 model, proving the improvement of our new translation mode. It can be attributed to the fact that simple SDoc2SDoc mode, without any other optimization, may arbitrarily ignore the interactions between strongly related sentences. In contrast, our proposed paragraph-to-paragraph method takes them into consideration and is expected to make better use of contextual information. Furthermore, we introduce discourse information and propose our RST-Seq and RST-Att models. Both of them follow the Para2Para translation mode based on mBART25, and contain the same number of parameters as the baselines mentioned above. Although RST-Seq performs better than the Para2Para baseline, the improvements are not prominent. We suppose that much longer inputs of linearized RST trees may increase the difficulty of training. Moreover, it may be difficult for mBART25 to directly adapt to the format of the RST sequence since it
has not been finetuned for such parsing tasks and there are not enough hints in the translation task. In addition, our RST-Att model achieves the best results and improves d-BLEU scores by 1.07, 0.57, and 0.06 on TED, News, and Europarl, respectively, showing the effectiveness of our proposed attention mechanism. Its superiority over the RSTSeq demonstrates that discourse knowledge may be applied to the model structure more effectively than the input data. Furthermore, Table 1 shows that the data scales of TED, News, and Europarl are increasing while the improvements on the corresponding dataset are decreasing. We believe that our RST-specific attention module can alleviate the negative impact of small datasets to a certain extent. When the training data is sufficient, like in the case of Europarl, the performance gaps between all existing models are not significant. 4.1 granularity discussion To demonstrate the effectiveness of our introduction of the RST discourse structure, we evaluate the RST-Att model with different settings of the involved multi-granularity attention mechanism. Different Number of Levels We first verify whether using as much granularity as possible can improve the performance of the model. The node sets are instead mapped to just six encoder layers, corresponding to half of the original levels of granularity. We make a copy of each mapped layer to fill the whole encoder progressively. Furthermore, we attempt the case of much less granularity in a clearer manner of partitioning, including three levels of EDU, sentence and paragraph. And similarly, each layer of these three levels is repeated four times after mapping. Equally Divided Tree We also pay attention to the impact of discourse guidance in our multigranularity attention module. To exclude the discourse information, we conduct a more direct implementation of multiple granularity based on the
equally divided tree. Specifically, each range of content is divided into two equal ones, layer by layer, until the current content contains no more than three tokens.2
The results are shown in Table 3. The drops in performance become larger from the model with six-level granularity to the model with three-level granularity, proving that more elaborate granularity levels contribute to the improvement of the RSTAtt model. On the other hand, despite also containing multiple granularity, the model constructed through arbitrarily equal division gets even worse results, which further demonstrates the crucial and important role of discourse structure in our method. 4.2 linguistic evaluation Furthermore, we have conducted linguistic evaluations about the performance of models when dealing with discourse phenomena, based on the frequently used contrastive test sets (Voita et al., 2019) in existing works. They were designed for targeted evaluation of several discourse phenomena: deixis, ellipsis (two test sets for VP ellipsis and morphological inflection), and lexical cohesion. Each test instance consists of a true example and several contrastive translations that differ from the true one only in the considered discourse aspect. The translation system needs to select the one that it considers to be true, which is evaluated by accuracy. We compared the results of Sent2Sent, Concat and CADec from Voita et al. (2019), LSTM-Transformer from Zhang et al. (2020b), G-Transformer, and MR-Doc2Doc, Chat-
2Smaller text units will cause the failure of training according to our experiments. GPT3 and GPT-4 from Wang et al. (2023) with our RST-Att model, as shown in Table 4. Our RST-Att model has achieved superior performance, with the highest accuracy in three aspects: lexical cohesion, VP ellipsis, and morphological inflection, while having lower performance in deixis compared to SOTA. And the improvement in terms of lexical cohesion over previous methods is notably significant. We suppose it may be because the RST discourse structure that pays attention to the relationship between text units can promote the model to better handle lexical cohesion. Moreover, our approach outperforms LLMs comprehensively, including ChatGPT and GPT-4, and more discussions with LLM are described in Appendix B. These results indicate the enhanced ability of our model to adapt to contextual information and handle discourse phenomena, which shows the promising way to introduce discourse information into document-level machine translation. 4.3 context length Next, we compare our RST-Att model with the baseline with respect to different lengths of input context. Since our translation is based on paragraphs, we follow the d-BLEU to calculate the p-BLEU score, which matches the n-grams in each paragraph. Figure 5 depicts the results of two models on News. Surprisingly, the baseline, whose structure is still the vanilla transformer, does not fail on long contexts, contradicting the findings of Li et al. (2022). We consider that the difference may be due to the knowledge of the well-trained LM. Moreover, the result of our RST-Att model exceeds the baseline at each length, which is more distinct as the length increases. And it maintains a relatively stable performance when the input length is more than 256, indicating the advancement of our model in dealing with long inputs. 4.4 label utilization Since the RST-Att model ignores nuclearity and relations on the RST parsing tree, we have further explored whether the in-depth utilization of label information can lead to more improvements. We apply graph learning to enhance the comprehension of the model for each EDU. RST parsing trees are transformed into dependency trees according to Hirao et al. (2013), so that each node will represent a different EDU. We serve the dependency
3https://chat.openai.com. 18.0
21.0
24.0
27.0
30.0
33.0
36.0
(0, 64) [64, 128) [128, 192) [192, 256) [256, 384) [384, 512) [512, ∞)
pB
LE U
Number of tokens Baseline RST-Att
Figure 5: The p-BLEU scores of the baseline and our RST-Att model on different input lengths. tree as a graph, and the connectivity and the path to other nodes with label sequences can be calculated for each EDU. These features are encoded to update the representation of each EDU with the architecture of GraphFormer (Yang et al., 2021). Then we integrate the learned representation into the calculation of each token the EDU contains. Although this method surpasses the baseline, there is no significant improvement over the RSTAtt model. We suppose that the path with label information can represent the relationship between nodes at both ends, but it may be too complicated for the model to encode such knowledge. On the other hand, there are still many errors in the predicted labels on account of the limitations of current RST parsing research, which may mislead the model during training. We hope our exploration can inspire future research to come up with more effective approaches to utilizing RST labels. 5 related works Document-level Machine Translation Although early work has achieved great success on machine translation, a document is often processed by translating each sentence separately. As a result, the information included in context is ignored. Recently, document-level machine translation has attracted more attention and many methods have been proposed to better utilize the contextual information to improve translation quality. Most early attempts still followed sentence-tosentence translation, but they applied various frameworks to utilize the context during training. Zhang et al. (2018); Miculicich et al. (2018); Zhang et al. (2020b); Zheng et al. (2020); Zhang et al. (2021a) utilized surrounding sentences and integrated the contextual information into encoding of the current
sentence. Kang et al. (2020) indicated dynamic context selected from the surrounding sentences can improve the quality of the translation more effectively. Jiang et al. (2020); Ma et al. (2020) designed the specific module to encode the document into a contextual representation. Feng et al. (2022) introduced a recurrent memory unit to remember the information thorough the whole document. Although Zhang et al. (2018); Liu et al. (2020) have shown that direct Doc2Doc translation may cause the failure of training, many recent studies have focused on translating multiple sentences or the entire document at once (Tan et al., 2019; Bao et al., 2021; Sun et al., 2022; Li et al., 2022). They used various specific methods to avoid this problem and achieved more advancement on DocMT. Discourse Parsing Discourse parsing describes the hierarchical tree structure of a text and can be used in quality evaluations like coherence and other downstream applications. RST parsing is the most important role of discourse parsing, and the existing approaches can be mainly divided into two classes: top-down and bottom-up paradigms. Bottom-up methods have been first proposed since hand-engineered features were suitable for representing local information. Models with CKYlike algorithms (Hernault et al., 2010; Joty et al., 2013; Feng and Hirst, 2014; Li et al., 2014) utilized diverse features to learn the scores for candidate trees and selected the most possible one. Another common bottom-up method is the transition-based parser with actions of shift and reduce (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018). Recent advancements in neural methods made global representation more effective, which promoted top-down parsers. Lin et al. (2019) first presented a Seq2Seq model based on pointer networks (Vinyals et al., 2015) and Liu et al. (2019) improved it with hierarchical structure. Then Zhang et al. (2020a) extended their methods to documentlevel RST parsing. Kobayashi et al. (2020) constructed subtrees with three levels of granularity and merged them together. Despite the better performance of top-down models, most of them still need gold EDU segmentation and drop a lot in performance when using automatic segmenters. To address the problem, Nguyen et al. (2021) introduced an end-to-end parsing model, relying on specific frameworks for different tasks. Zhang et al. (2021b) proposed a system with rerankers to improve the performance. 6 conclusions In this paper, we explore the role of discourse structure in document-level machine translation. We introduce a more sound paragraph-to-paragraph translation mode than the several surrounding sentences or fixed length of texts used in previous studies. To better take advantage of the RST parsing tree, we propose the RST-Att model with a multigranularity attention mechanism depending on the tree structure. The experiment results prove the superiority of our method, and further evaluation indicates that both the guidance of discourse structure and more levels of granularity contribute to the improvement. 