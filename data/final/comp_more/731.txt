Textbooks are one of the main mediums for delivering high-quality education to students. In particular, explanatory and illustrative visuals play a key role in retention, comprehension and general transfer of knowledge. However, many textbooks lack these interesting visuals to support student learning. In this paper, we investigate the effectiveness of vision-language models to automatically enhance textbooks with images from the web. We collect a dataset of e-textbooks in the math, science, social science and business domains. We then set up a text-image matching task that involves retrieving and appropriately assigning web images to textbooks, which we frame as a matching optimization problem. Through a crowd-sourced evaluation, we verify that (1) while the original textbook images are rated higher, automatically assigned ones are not far behind, and (2) the precise formulation of the optimization problem matters. We release the dataset of textbooks with an associated image bank to inspire further research in this intersectional area of computer vision and NLP for education. 1 introduction Students use textbooks as one of the primary mediums of learning. It is thus imperative that textbooks are designed to provide a rich learning and engaging environment. Visuals enhance learning through a number of means, including the ability to retain information, as well as its ability to promote comprehension and knowledge transfer (Carney and Levin, 2002; Dimopoulos et al., 2003; Katsioloudis, 2007; Hibbing and Rankin-Erickson, 2008; Panjwani et al., 2009; Mayer, 2019). Automatic approaches for retrieving and assigning images from the web to textbook chapters can therefore assist textbook designers in the creation of better textbooks. However, this is a very challenging task
∗ Work done during an internship at ETH Zürich 0Code & data: github.com/eth-nlped/textbook-enrichment
as an ideal illustrative visual should not only be related to the textbook material, but also have pedagogical value (see Figure 1). Agrawal et al. (2010, 2011) already enhance textbooks with images from the Internet. They use similarity scores of textual captions from images and the text present in the textbooks for image assignment. This approach requires the image captions to be available. Beyond the problem of availability, the captions are also highly context dependent which reduces their utility in our setting. In this work, we aim to reinvigorate this area, and present and analyse a real dataset of textbooks. We do so by setting up the problem of image assignment using textbook and Wikipedia images. In contrast to previous work, we rely on the recent advances in vision-language models, such as CLIP (Radford et al., 2021) and DALL-E (Ramesh et al., 2021). We analyze our dataset to gain insights into the organization of concepts and illustrative images within the textbooks. This analysis inspires the formulation of a new optimization
11931
problem focused on modeling of illustrations in textbooks. The solution to this problem maximizes the coverage of the illustrations while minimizing redundancy during image-to-paragraph assigments. Our approach uses CLIP to retrieve and appropriately assign images to long-form text, particularly a textbook section. Then, the overall assignment is obtained in following stages: 1) Given a piece of text, produce a set of concepts
that should be addressed by a visualization, 2) Given an image and a concept, determine their
mutual relevance, and 3) Given a piece of text (e.g., a textbook section),
produce an adequate assignment of images. Each of the three sub-problems listed above can be solved with a variety of approaches — indeed we explore several variants, which we describe in Section 4. Overall, we contribute the following: • A dataset that contains text and images drawn
from 35 textbooks covering math, business, social sciences, and science in addition to a secondary image bank of ∼312K images taken from Wikipedia (Section 3). • Formalization of multiple textbook enrichment optimization goals (Section 4). • Human evaluation and an in-depth examination of the possible failure modes and challenges of the proposed methods (Section 5). 2 related works Vision Language Models: Alignment between texts and images has seen rapid progress recently with models such as CLIP (Radford et al., 2021) and DALL-E (Ramesh et al., 2021). However, their usages are limited to a short and specific text prompt to which the performance is usually quite sensitive. We focus on the problem of retrieving images for very long textual inputs, specifically a textbook section, where it is unclear which part of the text specifically describes the relevant image. Image Text Matching for Long Texts: Recently, Wang et al. (2022a,b); Zeng et al. (2022) trained better language-vision representations with more nuanced associations, such as multiple vision tasks or finer image-text alignments. However, this progress is mainly confined to datasets, like MSCOCO (Lin et al., 2014) and Flickr-30K (Young
et al., 2014), that contain natural images and their captions, which are rather short. Additionally, Schneider et al. (2021) show that current multimodal models perform poorly at retrieving relevant images for longer and more complex textual inputs. The reason for this poor performance is the pre-training on shorter and very specific image captions. This is a strong requirement to our work, which is focused on even longer text inputs. We explore the problem of assigning images to lengthy text, which highlights issues such as ensuring comprehensive coverage of concepts and avoiding redundant image illustrations. To move even closer to a real-world setting, we perform this task with actual textbooks and a human study. Enriching Text with Images: The task of textbook enrichment was first explored by Agrawal et al. (2010, 2011), who assume that web images have associated relevant captions. We note that the caption of images are largely dependent on the context where the image was originally assigned.1 The language of the caption may not even match the textbook’s language. To alleviate all this, we do not assume that the images have associated relevant captions. Seo et al. (2015); Kembhavi et al. (2017); Lee et al. (2022) also studied associating images with textual information. However, their primary goal has largely been to comprehend the image content, and thus differs from our objective. Finally, there has been more past work on NLP applied to textbooks (Sachan and Xing, 2017; Sachan et al., 2017, 2020). However, the goal of these works also differ significantly from ours. 3 dataset We now present the process of curation and structure of our dataset with an analysis. 3.1 data collection OpenStax Books. We source both the text and assigned images from 35 textbooks from an online textbook publisher openstax.org, covering four subjects: business, social sciences, sciences, and maths. Each textbook is organized into chapters, sections, subsections, and paragraphs. See their distribution in our dataset in Table 1. For each sub-
1For example, the same image of a poster from Wikipedia has different captions on different pages: “The munitions industry heavily recruited women workers, as represented by the U.S. government’s Rosie the Riveter propaganda campaign” and “J. Howard Miller’s We Can Do It! poster from 1943s”. section of the textbook,2 we identify the following key elements:
• Text: Raw text from the subsection. • Phrases: Raw text from the subsection decom-
posed with overlapping sliding windows.3
• Concepts: Key concepts taught in the subsection are the bolded words or phrases, headings, index terms, and key terms (both marked explicitly in the book as such). • Images: Image(s) assigned within subsection. Wikipedia images. To mimic the task of textbook enrichment, we use a dataset of images from Wikipedia that are relevant to the concepts in the OpenStax Books dataset. This dataset serves as a proxy for images from the web. We search for relevant Wikipedia articles for each concept, with a maximum of 20 articles retrieved per concept. From these articles, we extract images and their captions by searching the article in the WIT dataset (Srinivasan et al., 2021) or directly from the article. The final dataset has approximately ∼312K unique images included in the relevant articles. Image Bank. We combine OpenStax Books images and the Wikipedia images to form the Image Bank. Our objective is to retrieve and assign relevant images from the Image Bank to each section that is present in the OpenStax Books dataset. 3.2 dataset analysis We profile the dataset according to a series of questions, which will inform the problem formulation. 2Example of a subsection: openstax.org/books/conceptsbiology/pages/5-1-overview-of-photosynthesis. 3A window-size of 75 tokens and an overlap-ratio of 1/3. Q1. How are concepts distributed? The patterns in concept mentions are similar across subjects (Figure 2b). The distribution of concepts within subsections (Figure 2a) reveals an average of 5.6 concepts per subsection. An average concept is mentioned 2.7 times within a subsection (Figure 2b) — that is, concepts are infrequently mentioned in the subsection. Notably, each section concept is mentioned in only 1.7 subsections on average (Figure 2c), emphasizing the high localization of concepts to specific subsections. Q2. What influences the number of assigned images in a subsection? On average each section consists of 5.5 subsections and 3.3 assigned images (Table 1). To answer the question at hand, we conduct a regression analysis with the number of images in a subsection as the predicted variable, and the following as features:
• concepts/words/paragraphs: their total # in the subsection. • concepts_uniq: # of unique concepts mentioned in the subsection. • %sec_concepts: % of unique concepts from the section in the subsection. • %sec_concept: % of total concept mentions from the section in the subsection. • %sec_words: % of total words in the section which are in the subsection. • %sec_paragraphs: % of paragraphs in the section in the subsection. • position of the subsection in the section from 0 (beginning) to 1 (end). • subject of the book. Based on the results in Table 3, the number of assigned images to a subsection can be best predicted from total number of concepts, words, and paragraphs of the subsection. Unexpectedly, this is not true for the number of unique concepts. Furthermore, the position of the subsection within the section is negatively correlated to the image count — that is, the subsections located later in the section have fewer images. The subject of the book also impacts the subsection’s image count, with differing coefficients for each subject. Overall, the regression model yields Pearson correlation of 0.59 with p<10−4 — a high degree of predictability. Q3. Are images exclusive to the assigned subsection? We use CLIP’s similarity scores for imagephrase relevance (detailed in Section 4). For each image in the textbook we distinguish between the
present subsection and the one before and after. Then, we assign images to the subsection with the highest-matching phrase. The percentages of mostsimilar images in the before and after subsections is nearly equal (Figure 3), which is contrary to the intuition that the subsections after the current one would refer to the concepts in the image. The difference between the before/after and present subsections is the greatest in the business category, indicating uniqueness of images assigned to a particular subsection compared. Such uniqueness, as determined by the CLIP scores, is most absent in mathematics books. Overall, the images do not exclusively best-match the phrases from the goldassigned subsection. Q4. Are concept mentions associated with assigned images? We rank subsection phrases using CLIP similarity scores with subsection images. We use this ranking to calculate the percentage of concepts that were mentioned in the top-similar phrases associated with the gold images in the subsection. This way, we evaluate whether textphrases with higher association to the gold-image also had more concept mentions. Indeed, there is a correspondence between the gold images and
phrase with concept mentions (Figure 4). This warrants further usage of CLIP scores as a measure for matching concepts to images. 4 image retrieval and assignment We first describe an image retrieval model and then formalize our task and the optimization approach. CLIP (Radford et al., 2021) is a stateof-the-art vision-language model trained on many image-caption pairs from the web by maximizing the dot-product similarity the image and caption encodings. We further fine-tune CLIP on image-text pairs from the OpenStax Books dataset. 4.1 image assignment formulation Our formulation focuses on the assignment of images to subsections.4 We begin with notation:
subsections u in a section s = ⟨u1, . . . , u|s|⟩ (1) concepts c in a subsection u = {c1, . . . , cK} (2)
4We can assign images to the entire book by concatenating all chapters into one long “section”. One can also assign images to paragraphs by treating them as a subsection each with a single paragraph. We decompose the text of a subsection into phrases using a sliding window approach. These phrases may mention a particular concept:
phrases t in a subsection u = ⟨t1, . . . , tL⟩ (3) Let m(tl, ck) denote mention of concept c in t
m(tl, ck) = { 1 if ck mentioned in tl 0 otherwise (4)
For a fair comparison, we assign the same number of images to each subsection as in the gold assignment. This can also be automated with the image count prediction (Section 3.2/Q2). 4.2 local assignment The most straightforward solution is to select an image for each subsection independently by maximizing the subsection text-image similarity. Specifically, we assign each subsection u, with an image i ∈ I, which maximises the following function:
S({i}, u) = ∑
t∈u sim(i, t) (5)
Here, I denotes set of all the images in the image bank. Moreover, sim(i, t) denotes probability (normalised dot-product similarity across images) of any image i and certain phrase t as given by the fine-tuned CLIP model. While local assignment is fast and simple, our qualitative analysis reveals that it lacks global coherence and may assign images depicting overlapping concepts to the same section. For example, if every subsection mentions the concept “molecule”, then all subsections can be assigned the same image of a molecule. This finding aligns with our previous results (Section 3.2/Q3) and is supported by the redundancy metrics in Section 5. 4.3 global assignment The analysis in Section 3.2 revealed that the mostrelevant phrases for gold images are not restricted to the assigned subsection and that the concepts are localized within their respective sections. Therefore, for better global coherence in our assignments, we assign images based on concepts rather than phrases. Specifically, we select a subset of images that covers most of the concepts (coverage) while avoiding overlaps (redundancy). To define coverage and redundancy functions for concepts in a section, we first define a boolean function for image, i, covering a concept, c, as follows:
cov(c, i) = Isim(i,t)≥τ ·m(t, c). Informally, cov(c, i) is 1 iff concept c is covered by image i, otherwise 0. Next, we formalize the coverage and redundancy. Coverage. Coverage for a section s and a subset of images I ′ is the number of unique concepts in section s which are covered by images in I ′:
C(I ′, s)= ∣∣{c ∈ s | ∃i ∈ I ′ : cov(c, i) = 1} ∣∣ (6)
Redundancy. Redundancy of a section s and a a set of images I ′ is the total number of times concepts in s are multiply covered:
R(I ′, s) = ∑
c∈s
∑ i∈I′ cov(c, i)− C(I ′, s) (7)
We now introduce the concept of set submodularity which will be necessary for proving approximation bounds of the optimization. Definition 4.1. A function f is said to be set submodular if and only if ∀A ⊆ B ∀x : f(A∪{x})− f(A) ≥ f(B ∪ {x})− f(B). Informally, the function yields diminishing returns for item x.
Theorem 4.2. The coverage function C is set submodular. That is for I ′′ such that I ′′ ⊆ I ′ ⊆ I and i ∈ I it holds that C(I ′′ ∪ {i}) − C(I ′′) ≥ C(I ′ ∪ {i})− C(I ′). Proof. Let C′ and C′′ be sets of concepts from a subsection covered by images in I ′ and I ′′ respectively. As per the definition of C, we have C′′ ⊆ C′. Let Ci be the concepts covered by image i. C(I ′ ∪ {i}, s)− C(I ′, s) = (8) = ∣∣C′ ∪ Cq ∣∣− ∣∣C′ ∣∣ (from def.) (9)
≤ ∣∣C′′ ∪ Cq ∣∣− ∣∣C′′ ∣∣ (from C′′ ⊆ C′) (10) = C(I ′′ ∪ {i}, s)− C(I ′′, s) (from def.) (11)
Theorem 4.3. The negative redundancy function −R is set submodular. That is for I ′′ such that I ′′ ⊆ I ′ ⊆ I and i ∈ I it holds that −R(I ′′ ∪ {i}) +R(I ′′) ≥ −R(I ′ ∪ {i}) +R(I ′). Proof. Similarly to Theorem 4.2, for redundancy function R we observe that:
R(I ′ ∪ {iq}, s)−R(I ′, s) = (12) = ∣∣C′ ∩ Cq ∣∣ (from def.) (13)
≥ ∣∣C′′ ∩ Cq ∣∣ (from C′′ ⊆ C′) (14) = R(I ′′ ∪ {iq}, s)−R(I ′′, s) (from def.) (15)
Observation 4.4. Both C and R are monotone. For the global assignment, we choose images I ′ ⊆ I such that the following is maximised:
G(I ′, s) = C(I ′, s)−R(I ′, s) (16)
G (Equation 13) is a submodular function because it is a sum of two submodular functions. Finding the optimal solution to G is NP-hard (Lovász, 1983). However, since G is a submodular function, greedy algorithm can lead to fairly good 1 − 1/e ≈ 63%-approximation of optimisation of G under cardinality constraints |I ′| ≤ B, where B is the budget (Nemhauser et al., 1978). Once images I ′ are greedily computed for a section, we assign an image i ∈ I ′ to the subsection, u, which maximises C({i}, u); to the subsection in which the image i covers the most concepts. 4.4 joint assignment The local assignment captures relevance while the global one also captures redundancy. To optimize both of them, we formulate the following objective with a trade-off hyper-parameter β ≥ 0:
J(I ′, s) = S(I ′, s) + β ·G(I ′, s) (17) = ∑
i∈I′
∑ t∈s sim(i, t) + β ·G(I ′, s)
Note that J is a submodular function, since it is the sum of two other submodular functions: β ·G and S. The former is submodular due to the nonnegativity of β and previously proven submodularity of G. The submodularity of S is proven below. Theorem 4.5. The local assignment function S is set submodular. That is, for a set of images I ′ and I ′′ such that I ′′ ⊆ I ′ ⊆ I and any image i ∈ I − I ′ it holds that S(I ′′ ∪ {i}) − S(I ′′) ≥ S(I ′ ∪ {i})− S(I ′). Proof. S(I ′ ∪ {i}, s)− S(I ′, s) = ∑
t∈s sim(i, t) (18)
≤ S(I ′′ ∪ {i}, s)− S(I ′′, s) (19)
Considering the submodularity of J , we select images greedily, similarly to optimizing G. Once a set of images, I ′, is greedily computed for a section, we assign an image i ∈ I ′ to the subsection, u, which maximises S(I ′, u) + β ·C(I ′, u); to the subsection in which the image i covers the most
concepts and has most similar text. Note that the local and global assignments are specific cases of this formulation. This formulation achieves our desideratum — images are assigned to specific subsections also with global context consideration. 5 human evaluation The goal of improving textbooks is to help students learn. Testing a wide range of models directly by monitoring learning progress would require a very expensive long-term evaluation. Instead we turn to an intrinsic crowd-sourced evaluation where we ask teachers what they think about the qualities of the assignment. 5.1 setup We selected 32 crowd-workers from Prolific who are native English speakers and work in education. We compare 4 different assignments: the gold one by a human and three automatic ones (Section 4). Each participant is assigned a single section and evaluates all 4 systems on this section. This methodology may cause an unwanted priming effect, which we address in Section 5.2. We chose this setup deliberately because a lot of the annotation time is spent on reading the section text and we wanted to share this cost by annotating multiple assignments at once. Our evaluation consists of close-ended (limited number of answers) questions that pertain to both the local (subsection) and global (section) textual context (full annotation guidelines are in Appendix A):
Local evaluation: • Is this image relevant to educational concepts
described in this subsection text? • Is this image redundant compared to previous
images in this subsection? • What is the type of this image? Global evaluation: • Is this image relevant to educational concepts
described in this section? • Is this image redundant compared to previous
images in this section? • Is this image didactically useful for explaining
this section text? The annotators first answer the local questions for all images and then the global questions. This way we made sure that they scanned the entire section and had a some overview of all images and how they relate to each other before answering the
global questions. The annotation pipeline (for 4 assignments, local/global) is shown in Figure 6 and the user-interface in Figure 5. 5.2 evaluation results We first verify the evaluation setup validity by checking whether the position in the evaluation queue has an effect on the evaluation scores. Recall that the annotators were shown the same textbook section with 4 alternate images assignments. While Figure 7 shows some variance along the independent variable of evaluation position, the differences are not significant, justifying our evaluation setup. We then focus on two most important evaluation criteria: relevancy and redundancy. The results for those, shown in Figure 8, clearly show preference for the Gold image assignment, suggesting that automatic assignment is still inferior to humans. From the automatic methods, the Local performs the best relevancy-wise. However, it is outperformed by Joint with respect to redundancy. We also note that there is very little difference between Local and Global evaluation category. This may be caused by evaluation bias (i.e. annotators are likely to give similar score for both local and global questions). Next, we examine all the remaining evaluation categories in Table 2. While Gold is the best across
all, the significance of the difference varies. The Joint assignment is never the worst, suggesting it to be a robust choice. 5.3 qualitative analysis The Joint optimization method aims to reduce the reader’s cognitive load compared to the Local method that aggregates scores from all text-phrases. The Local method results in repetitive covering of a single concept from the section with top-images. In contrast, Joint and Global assign images covering a wider range and variety of concepts in the section, enabling a greater level of text enrichment. Appendix B shows examples of assignments by these approaches. We remark that structured image types, such as graphs, multiple images, or those that are less identifiable, receive lower ratings systematically (Figure 9). We now elaborate on the two major limitations in our models. Varied domain of images. One limitation of our approach is demonstrated in Figure 10 where it struggles to model non-natural images such as diagrams, graphs, and plots. These images often rep-
resent abstract concepts, relationships, and events which cannot be well modeled by models like CLIP that are majorly trained on natural images. Long textual description of concepts. Another source of error was that some concepts had long textual descriptions. For example, the description of Stokes’ theorem spans multiple paragraphs. Learning to associate image with a part of text may lead to loose and spurious associations, resulting in poor downstream assignment performance. This highlights the need for vision-language representations that can effectively model long text descriptions and establish better image-text associations. 6 conclusion and future work We presented a dataset and a new task of enriching textbooks with visuals from the web. We proposed several technical solutions for this problem using neural image retrievers combined with a new assignment optimization setup. Annotations by workers in the education industry verified that, even though the human assignment is still of the highest quality, the automatic assignments are not far behind.