A cost-effective alternative to manual data labeling is weak supervision (WS), where data
samples are automatically annotated using a
predefined set of labeling functions (LFs), rulebased mechanisms that generate artificial labels for the associated classes. In this work,
we investigate noise reduction techniques for
WS based on the principle of k-fold crossvalidation. We introduce a new algorithm ULF
for Unsupervised Labeling Function correction, which denoises WS data by leveraging
models trained on all but some LFs to identify
and correct biases specific to the held-out LFs.
Specifically, ULF refines the allocation of LFs
to classes by re-estimating this assignment on
highly reliable cross-validated samples. Evaluation on multiple datasets confirms ULF’s effectiveness in enhancing WS learning without
the need for manual labeling. A large part of today’s machine learning success
rests upon large amounts of annotated training data.
However, collecting manual annotation (even in
a reduced amount, e.g., for fine-tuning large pretrained models (Devlin et al., 2019), active learning (Sun and Grishman, 2012), or semi-supervised
learning (Kozareva et al., 2008)) is tedious and
expensive. An alternative approach is weak supervision (WS), where data is labeled in an automated
process using one or multiple WS sources such
as keywords (Hedderich et al., 2021), knowledge
bases (Lin et al., 2016), and heuristics (Varma and
Ré, 2018), which are encoded as labeling functions
(LFs, Ratner et al. 2020). LFs are applied to unlabeled datasets to obtain weak training labels, which
are cheap, but often conflicting and error-prone, requiring improvement (see Table 1).
We focus on enhancing the quality of weak labels using k-fold cross-validation. Intuitively, byleaving out a portion of the data during training,
the model avoids overfitting to errors specific to
that part. Hence, a mismatch between predictions
of a model trained on a large portion of the dataset
and the labels of the held-out portion can indicate
potential noise specific to the held-out portion. Previous cross-validation-based denoising approaches
(Northcutt et al., 2021; Wang et al., 2019b) split the
data samples into folds randomly; a direct application of these methods to WS data ignores valuable
knowledge stemming from the WS process. In
our work, we leverage this knowledge by splitting
the data based on matching LFs in the samples.
The intuition is the following: a mismatch between
predictions of a model trained on a large portion
of the LFs and labels generated by held-out LFs
can indicate noise specific to the held-out LFs. By
performing cross-validation for each LF, noise associated with all LFs can be identified.
In contrast to correcting labels as in previous
work for supervised settings, we utilize the crossvalidation principle in a WS setting to adjust and
improve the LF-to-class assignment. In some cases,
an LF correctly captures some samples but mislabels others. For example, one of the LFs used to
annotate the YouTube dataset (Alberto et al., 2015)
is the keyword my, which is effective in identifying spam messages such as subscribe to my channel or check my channel out (see Table 1). However, considering this LF as solely indicative of the
spam class would be unwarranted, as numerous
non-spam messages also contain the word my (e.g.,
Sample 3). The weak labeling of such samples
can results in a tie (i.e., one vote for SPAM and
one vote for HAM), potentially leading to incorrect
label assignments through (random) tie-breaking.
To address such cases, we introduce a new
method ULF: Unsupervised Labeling Function
correction (Figure 1), which comprises both erroneous labels detection and correction by leveraging
weakly supervised knowledge. ULF re-estimates
the joint distribution between LFs and class labels during cross-validation based on highly confident class predictions and their co-occurrence with
matching LFs. Importantly, this reestimation is
performed out-of-sample, meaning it is guided by
the data itself without involving additional manual supervision. Instead of a hard assignment of
naive WS (i.e., an LF either corresponds to the
class or not), ULF performs a fine-adjusted one,
which helps to correct the label mistakes. For example, such assignment reduces the association
of the LF "my" with the SPAM class, resulting in
dominant HAM probability in Samples 2 and 3
in Figure 1. Moreover, ULF successfully labels
samples with no LFs matched, as, e.g., Sample 4,
unlike other methods that filter them out (Ratner
et al., 2020). We conduct extensive experiments using feature-based and pre-trained models to demonstrate the effectiveness of our method. To the best
of our knowledge, we are the first to adapt crossvalidation denoising methods to WS problems and
refine the LFs-to-class allocation in the WS setting.
2 Related Work
Weak supervision (WS) has been widely applied to
different tasks across various domains, such as text
classification (Zeng et al., 2022), relation extraction
(Datta and Roberts, 2023), named entity recognition (Wang et al., 2022), video analysis (Chen et al.,
2023), medical domain (Fries et al., 2021), image
classification (Yue et al., 2022). Weak annotations
are easy to obtain but prone to errors. Approaches
to improving noisy data include building a specific
model architecture (Karamanolakis et al., 2021), using additional expert annotations (Mazzetto et al.,
2021), identifying and removing or downweighting
harmful samples (Northcutt et al., 2021; Sedova
et al., 2023), or learning from manual user guidance (Boecking et al., 2021; Chatterjee et al., 2020).
ULF is compatible with any classifier and do not require any manual supervision; instead of removing
the samples, ULF corrects the labels, utilizing as
much WS data as possible. K-fold cross-validation,
a reliable method for assessing trained model quality (Wong and Yeh, 2019), is also often used to detect errors in manual annotations (Northcutt et al.,
2021; Wang et al., 2019b,a; Teljstedt et al., 2015),
but has not been applied to a WS setting. We propose WS extensions to some of these methods in
Appendix B and use cross-validation in ULF.
3 ULF: Unsupervised Labeling Function
Correction
In this section, we present the key elements of
ULF. More details can be found in Appendix A,
the pseudocode is provided in Algorithm 1.
Given a dataset X = {x1, x2, ..., xN } to be used
for K-class classifier training. In WS setting, we
do not have any gold training labels, but only a
set of LFs L = {l1, l2, ..., lL}. An LF lj matches
a sample xi
if some condition formulated in ljholds for xi
. Following Sedova et al. (2021), we
store this information in a binary matrix ZN×L,
where Zijf = 1 means that LF lj matches sample xi
. A set of LFs matched in sample xi
is denoted by Lxi
, where Lxi ⊂ L and |Lxi
| ∈ [0, |L|].
The LFs to class correspondence is stored in a binary matrix TL×K, where Tij = 1 means the
LF li corresponds to class j (i.e., li assigns the
samples to the class j)
2
. The weak training labels
Y˜ = {y˜1, y˜2, ..., y˜n}, y˜j ∈ K are obtained by multiplying Z and T, apply majority vote, and break
the ties randomly. The main goal of the ULF algorithm is to refine the T matrix. The graphical
explanation is provided in Figure 2.
First, class probabilities are predicted for each
sample using k-fold cross-validation on the training
set X and weak labels Y˜ . We propose different
ways of splitting the data into k folds f1, ..., fk,
with the most reliable method being splitting by
signatures (refer to Appendix A for details and
other possible splitting methods). The samples’
signatures, i.e., the sets of LFs matched in each
sample, are collected, split into k folds, and used
to create data folds: Xtraini = {xj |Lxj ∈/ fi},
Xouti = {xj |Lxj ∈ fi} (1). Next, k models are
separately trained on each of k−1 folds and appliedo the held-out folds, resulting in out-of-sample
predicted probabilities PN×K. The out-of-sample
label yˆi for each sample i is determined by selecting
the class with the highest probability, provided that
this highest probability exceeds the class average
threshold tj :
tj :=
P
xi∈Xy˜=j
p(˜y = j; xi
, θ)
|Xy˜=j |
(2)
That is, a sample xi
is confidently assigned to class
j if the out-of-sample probability of it belonging
to class j is higher than the average out-of-sample
probability of all samples initially assigned to this
class. If no probability exceeds the class thresholds
(e.g., all probabilities are equally small), the sample
is disregarded as unreliable for further calculations.
These assignments are used to build an LFs-toclasses confidence matrix CL×K, which estimates
the joint distribution between matched LFs and
predicted labels. For each LF li and each class
kj , the confidence matrix CL×K is populated by
counting the number of samples that have LF li
matched and confidently assigned to class kj :Subsequently, CL×K is calibrated and normalized to Qˆ
L×K in order to align with the data proportions of the Z matrix:
Qˆ
li,yˆj =

Cli,yˆj
·
X
L
m=1
Zlm,yˆj
!
/
 X
L
m=1
Clm,yˆj
!
,
(4)
where P
i∈L,
j∈K
Cli,yˆj = n,
P
K
j=1
Zlm,yˆj =
P
K
j=1
Qˆ
lm,yˆj
.
This calibration ensures that Qˆ
L×K sums up to the
total number of training samples, and the sum of
counts for each LF is the same as in the original
Z matrix; thus, Qˆ
L×K can be utilized as a crossvalidated re-estimation of T. Finally, a refined T
∗
matrix is calculated as follows:
T
∗ = p ∗ Qˆ + (1 − p) ∗ T. (5)
Here, the hyperparameter p, p ∈ [0, 1], determines
the extent to which information from the original T
matrix should be retained. The resulting T
∗ matrix
is utilized to generate improved labels for additional ULF iterations or training the final classifier.
Unlabeled samples. ULF also takes advantage
of unlabeled samples that do not have any LFs
matched. A portion of these samples, determined
by the hyperparameter λ, is randomly labeled and
included in cross-validation training, with reestimation in subsequent iterations. To leverage all
unlabeled samples in a fine-tuning-based setting,
we also include the optional Cosine self-training
step (Yu et al., 2021), which can be executed during
cross-validation and/or final classifier training.4 Experiments
Datasets and baselines. We evaluate ULF on
four WS English datasets: (1) YouTube Spam
Classification (Alberto et al., 2015); (2) Spouse
Relation Classification (Corney et al., 2016); (3)
Question Classification from TREC-6 (Li and Roth,
2002); (4) SMS Spam Classification (Almeida
et al., 2011), and two topic classification WS
African datasets: (5) Yorùbá and (6) Hausa (Hedderich et al., 2020). For all datasets, we utilize the
LFs provided by dataset authors.
We compare our results towards the (1) Gold
baseline (the only classifier which exploits gold
labels) and the most popular and recent WS baselines: (2) Majority Vote, (3) MeTaL (Ratner et al.,
2019), (4) Snorkel-DP (Ratner et al., 2020), (5) Flying Squid (Fu et al., 2020), (6) WeaSEL (Cachay
et al., 2021), and (7) FABLE (Zhang et al., 2023).
More details are provided in Appendices C and D.
Results. We run experiments using RoBERTa following Zhang et al. (2021) for English datasets and
mulitlingual BERT following Devlin et al. (2019)
for others (more implementation details are provided in Appendix F). Table 2 presents the results
of the best combination of cross-validation and
final models; each of them can be either simplefine-tuning or followed by Cosine training step (Yu
et al., 2021). On average, ULF outperforms the
baselines and achieves better results on four out
of six datasets. The weakest performance was observed on the Yorùbá dataset, which is explained
by the extremely high number of labeling functions
(19897) and the smallest training dataset size (1340
samples) when compared to the other datasets.
Results of other combinations are provided in
Table 3. Two out of the four combinations achieve
average scores of 67.6 and 67.1, demonstrating
a better performance compared to the baselines.
Although the Cosine contrastive self-training considerably improves the results, the ULF high performance does not rely solely dependent on it. This
is evident in the fact that the most effective configuration for all other datasets except TREC incorporates the use of Cosine in only one of the two
model training steps. Moreover, FT_FT setting,
which does not involve Cosine at all, also demonstrates compatible results across all datasets.
Case Study. We provide a YouTube dataset case
study. Figure 3 shows the initial and adjusted T
matrices after two ULF iterations in FT_FT setting.
Some LFs underwent minimal adjustments (such
as keyword_subscribe and regex_check_out,
which clearly corresponded to one class), while
contentious LFs (like short_comment, i.e., short
comments are non-spam) were significantly adjusted. The adjustment was slightly improved after
the second iteration of ULF; however, a single iteration was already sufficient for most of the settings, as demonstrated by our experiments (see Appendix F). Table 4 shows mislabeled samples and
their corrected labels after ULF application. In (1),
the original equal voting was changed to 1.58 for
HAM and 2.42 for SPAM after T matrix correction,
explicitly determining the label as SPAM. Similarly, labels assigned by a clear majority vote, such
as (2), were also corrected. Next, there are samples where improved and gold labels do not match
(i.e., where ULF, strictly speaking, failed). However, these samples are quite controversial: e.g., (3)
might be a spam message if the link was different
(our model does not check the link’s content), while
(4) can be interpreted differently and perceived as
a spam comment. Finally, (5), not covered by any
LFs and initially randomly assigned to the HAM
class, has been corrected to the SPAM class.
5 Conclusion & Future Work
In our work, we focused on denoising WS data by
leveraging information from LFs. Our approach
assumes that the noise specific to some LFs can be
detected by training a model that does not use those
LFs signals and then comparing its predictions to
the labels generated by the held-out LFs. This idea
is used in our method ULF, which improves the
weak labels based on the data itself, without leveraging external knowledge. Extensive experiments
validate the effectiveness of our approach and support our initial hypothesis of the significant role of
LFs in denoising WS data. Limitations
In our work, we did not focus on the task of creating
labeling functions. Rather, our primary objective
is to improve the model performance with a fixed
set of already provided labeling functions, and to
enable better generalization to new data.
All the datasets and their corresponding labeling functions used in our experiments are weakly
supervised datasets that have been extensively utilized in previous research. The provided labeling
functions for these datasets, as well as other wellknown weakly supervised datasets, are considered
reliable. ULF does not require the majority of LFs
to have high precision; however, if we consider a
significantly different setting where the majority of
labeling functions are highly unreliable (e.g., generated by a noisy automatic process), cross-validation
as done in ULF may not be as effective as in a more
standard WS setting.
In our experiments, we restricted ourselves to
NLP datasets and tasks, as creating labeling functions for weak supervision is particularly intuitive
for language-related tasks.