Many annotation tasks in natural language processing are highly subjective in that there can be different valid and justified perspectives on what is a proper label for a given example. This also applies to the judgment of argument quality, where the assignment of a single ground truth is often questionable. At the same time, there are generally accepted concepts behind argumentation that form a common ground. To best represent the interplay of individual and shared perspectives, we consider a continuum of approaches ranging from models that fully aggregate perspectives into a majority label to “share nothing”-architectures in which each annotator is considered in isolation from all other annotators. In between these extremes, inspired by models used in the field of recommender systems, we investigate the extent to which architectures that include layers to model the relations between different annotators are beneficial for predicting single-annotator labels. By means of two tasks of argument quality classification (argument concreteness and validity/novelty of conclusions), we show that recommender architectures increase the averaged annotator-individual F1-scores up to 43% over a majority-label model. Our findings indicate that approaches to subjectivity can benefit from relating individual perspectives. 1 introduction There is inherent subjectivity in many annotation tasks in natural language processing (Shahid et al., 2020; Kumar et al., 2021; Thorn Jakobsen et al., 2022). Recent work has criticized the widely adopted approach of viewing variation in human labeling behavior as “noise” (Plank, 2022), advocating against approaches that disregard the richness of human annotations and perspectives by aggregating them into a single label. In fact, it has been argued that disagreement should not be regarded
∗Authors contributed equally to this work. as a problem, but rather as a chance to end up with more user-adaptable classifiers, giving voice to minorities as well (Prabhakaran et al., 2021; Gordon et al., 2022). In this paper, we hypothesize that machine learning can best address variation in human labeling by both accounting for subjective perspectives of individuals and for more objective concepts that build a common ground between annotators. A prime example of the interplay of individual and shared perspectives is the understanding and assessment of argumentation and, in particular, of argument quality (Romberg, 2022). Given the subjectivity that many concepts of argument quality face, it has been generally shown that its annotation often results in only fair to moderate interannotator agreements (Aharoni et al., 2014; Rinott et al., 2015; Habernal and Gurevych, 2017; Shnarch et al., 2018). For example, Gretz et al. (2020) determined a global value of argument quality by asking annotators “if they would recommend a friend to use that argument as is in a speech supporting/contesting the topic”. Even in more well-defined aspects of argument quality, such as the sufficiency of an argumentative conclusion, Stab and Gurevych (2017) observed “hard cases” in which labeling depends on subjective interpretations of keywords without being able to agree on a single ground truth. However, alongside “hard cases” of irreconcilable individual perspectives, in assessing argument quality we regularly also observe uncontroversial cases such as in human annotation of the validity and novelty of conclusions (Heinisch et al., 2022) or the concreteness of arguments (Romberg et al., 2022). In order to verify our hypothesis, we investigate the impact of different architectural design choices on the ability of models to predict the labels of single annotators. To this end, we look at a continuum of model architectures between such that fully suppress annotation variation by learning
from an aggregated label and such that are specifically tailored to each annotator. Along this continuum, we focus in particular on models that include components that involve the perspectives of single annotators (Davani et al., 2022), or are able to find and relate similar annotation behavior shared among different annotators using a recommender approach (Gordon et al., 2022). Figure 1 gives a first overview of the continuum of concepts and models, which we explain in detail in Section 3. Drawing on two tasks of argument quality, namely argument concreteness and validity/novelty of conclusions, we show that architectures inspired by recommender systems perform best in learning from disagreement. We attribute this to the identification of patterns in annotation behavior across individual annotators while being faithful to single choices. Our main contributions are:
• We present a novel framework1 for comparing different architectural design choices in or-
1 RelatePerspectives-sweetspots
der to model individual label decisions along the continuum from predicting a single label for all annotators to “share nothing” architectures that model the label decisions of single individuals independently from each another. This framework is not limited to our use case of argument quality, but can be applied to any classification task whose annotation combines subjective and objective aspects, in order to find architectural sweet spots for modeling label variation. • We perform an extensive automatic evaluation tuning different model types and hyperparameters on two datasets corresponding to three tasks, involving vanilla large language models (LLM) as well as LLMs with annotator-specific classification heads and recommender models adapted to the classification tasks in argument quality. Using a recommender-based architecture, we increase the averaged annotator-individual F1-scores by up to 4 points in the classification of argument concreteness, 18 points in the classification of conclusion validity and 19 points in the classification of conclusion novelty. • We conduct a qualitative case study of the behavior of our models on controversial examples in order to shed light on the differences regarding the effect of annotator-(dis)agreement, differing amounts of annotated samples between annotators, and annotation behavior. Extending previous work that proposed to take subjectivity into account by predicting the degree of expected label variation by Romberg (2022), our study presents the first approach in the field of argument mining to learn directly from the individual human labels. Beyond the specific contributions mentioned above, our work might encourage future research in argument mining and other fields to step away from systems that suppress valid perspectives by relying on a single aggregated label, and instead moving on towards systems that cover the multi-faceted spectrum of opinions. 2 related work  2.1 subjectivity & modeling individual annotators There is a growing body of work researching subjectivity (Ovesdotter Alm, 2011; Rottger et al.,
2022), learning with disagreement (Uma et al., 2021a; Leonardelli et al., 2023; Sandri et al., 2023), diversity of perspectives (Abercrombie et al., 2022; Cabitza et al., 2023) and human label variation (Plank, 2022). Despite the varying terminology, these works overlap in concerns that aggregating labels into a single “truth” is not appropriate for many tasks (Aroyo and Welty, 2015; Uma et al., 2021b; Basile et al., 2021) and might not represent perspectives equally (Prabhakaran et al., 2021; Abercrombie et al., 2022). This line of research has produced various approaches to learning models based on individual annotations (Plank et al., 2014; Jamison and Gurevych, 2015; Akhtar et al., 2020; Fornaciari et al., 2021; Cercas Curry et al., 2021; Plepi et al., 2022). A particular way of learning from annotatorspecific labels are models which learn to predict individual annotators’ decisions. Conceptually, these models can be seen as feature-based models of annotation (see for an overview Paun et al. 2022b) in that they model how each annotator labels individual examples. However, in contrast to standard models of annotation (e.g., Hovy et al. 2013; Passonneau and Carpenter 2014; Paun et al. 2018), their goal is not to aggregate decisions to a single label before training but to learn classifiers directly from non-aggregated annotations (Paun et al., 2022a). Most work in this area (Raykar et al., 2010; Albarqouni et al., 2016; Guan et al., 2018; Rodrigues and Pereira, 2018) argues for training on non-aggregated labels as a way to deal with varying annotator reliability to better derive the correct labels. Among these, Chu et al. (2021) explore an idea similar to our emphasis of common ground in addition to individual variation, modeling annotation noise in terms of both individual and common noise. Closely related to our work, more recent studies (Davani et al., 2022; Gordon et al., 2022) focus on subjective tasks for which a single ground truth can not always be determined. Davani et al. (2022) introduce a multi-annotator model (further explored in Orlikowski et al. 2023; Vitsakis et al. 2023), in particular a variant using a multi-task architecture: For each annotator, there is a separate classification head trained on annotations from that annotator. All these annotator layers share a pre-trained language model used to encode the input. We evaluate this architecture in our experiments. Gordon et al. (2022) present a model that also predicts individual annotations and
allows a user to interactively aggregate them based on “a jury” inspired by the US judicial system. Their approach is based on a recommender architecture using “Deep & Cross Networks” (Wang et al., 2021) which we also use in our work. 2.2 subjectivity in argument mining Argumentation and, in particular, the human understanding of its quality, is often subjective, conditioned by a variety of phenomena (e.g., van der Weide et al., 2010; Esau, 2018). Only recently has the argument mining community joined other disciplines, such as social sciences and formal argumentation, in addressing the backing mechanisms for differing perspectives in argumentation. Ajjour et al. (2019) first examined the framing of arguments in order to appeal to specific audiences based on their interests, cultural backgrounds, and socialization. Putting a special focus on moral frames, Kobbe et al. (2020) and Alshomary et al. (2022) studied different moral belief systems that arguments are subject to, drawing on the moral foundations theory (Haidt and Joseph, 2004). Going into more detail on individuals’ diverse beliefs and emphasises, Kiesel et al. (2022) utilized computational methods to identify a comprehensive taxonomy of 54 human values (Searle, 2003) embedded in arguments. In addition, the effect of storytelling on individual perceptions of argumentation was addressed by Falk and Lapesa (2022). Besides the motives behind subjective reasoning, research also considered systems that include multiple perspectives in the output, such as a diversity of stances about some claim (Chen et al., 2019). A direct modeling of different perspectives as part of the machine learning process itself, on the other hand, has hardly been considered so far. The only contribution in this direction was made by Romberg (2022), who presented a methodology for integrating subjectivity information into conventional text classification workflows of argument mining. While that approach involves training a separate classifier to predict a subjectivity value, in this work we implement models that directly learn from the non-aggregated labels. 3 methodology We aim at the comparison of different paradigms that can be applied in order to model a classifier for predicting individual labeling decisions in subjective annotation tasks. To this end, we define a
model spectrum ranging from a purely annotatorspecific model design (denying that there is anything learnable which is shared among the annotators, i.e., the objective grounding) to a purely annotator-agnostic model design which only considers the majority vote (denying that there is anything learnable which is individual for an annotator, i.e., the subjective grounding). We also include models that exploit the characteristics of both sides of the coin. These hybrid models combine model components that are shared for all annotators as well as model components that are specific for each annotator. In this way we can explore different architectural design choices along the above mentioned continuum, varying the degree to which the shared labeling behavior of certain groups of users is modeled in the architecture. The proposed model spectrum complements existing taxonomies of learning from disagreement. Uma et al. (2021b) distinguish four categories of approaches, one of them being “learning directly from crowd annotations”. Within this category, our proposed spectrum allows to further differentiate how models process individual annotations. Figure 1 shows our broad bandwidth of approaches, which covers five different approaches to classification: a majority vote model, per-annotator models, and three hybrid approaches, namely a model with annotator-specific classification heads, a recommender system with a shared text encoder, and a recommender system with annotatorseparated text encoders. These will be described in more detail in the following subsections. 3.1 two poles: annotator-specific and annotator-agnostic approaches For the pure annotator-specific and annotatoragnostic approaches, we consider a pre-trained LLM with a standard classification head. The only difference between the two paradigms is the way the dataset is preprocessed. In the case of the “share nothing” annotator-specific paradigm, the dataset is split annotator-wise, i.e., one split consists of all annotated instances paired with the individual annotation of exactly one annotator. Hence, having n different annotators, we fine-tune n language models, assuming training data for each annotator. We call this variant PerAnnotator. The opposite annotator-agnostic paradigm considers the full dataset with majority-aggregated annotations, resulting in a single annotator-agnostic model that
captures the majority perspective (Majority). 3.2 Approaches between annotator-specific and annotator-agnostic approaches
For modeling both objective and subjective components, we compare two different architectures. Annotator-specific classification head The first architecture replaces the single classification head of the LLM with a set of annotator-specific classification heads. This architecture is equivalent to (multi-task) multi-annotator models introduced by Davani et al. (2022). Because of its separated classification heads, we refer to this architecture as SepHeads in the following to distinguish it from the other approaches modeling multiple annotators. Recommender-system inspired models The second approach, motivated by Gordon et al. (2022), explores recommender systems that incorporate LLMs. Such systems rely on two encoder blocks, one for the text (using a LLM) and one for the annotator. This results in two internal vector representations of the input pair of text and annotator ID. To combine these two representations, we use a neural combiner component that performs the final classification. Such recommender style architectures, by encoding annotators by their IDs, can induce representations that generalize across single annotators, thus learning commonalities in the behavior of annotators that have a similar labeling pattern. It is in this sense that the recommender architectures can relate perspectives of different annotators. In the standard recommender approach, the model contains exactly one pre-trained LLM shared among all annotators. Hence, we call this architecture ShareREC. To explore hybrid approaches emphasizing the more annotator-specific component, we model an option in which each annotator has their own seperate text encoder. This type of model is referred to as SepREC hereafter. We additionally experimented with further architectures that fit in between ShareREC and SepREC. The methodology and results of these models can be seen in Appendix C.2. 4 experiment design  4.1 datasets Due to the fact that work on modeling labeling choices of single annotators is quite recent, so far
there are not many datasets available that have released the data in a way that explicitly contains the labels of individual annotators. We base our experiments on two such datasets that examine different aspects of argument quality. CIMT Argument Concreteness Dataset (abbr. Concreteness, Romberg et al., 2022) The German-language dataset consists of argumentative text units (ATUs) extracted from public participation processes related to traffic planning. These ATUs were categorized into three levels of contentrelated concreteness: low, intermediate, and high. While ATUs of low concreteness were defined as being vague and lacking specificity, ATUs of high concreteness should provide detailed information. Each ATU was labeled by five different annotators. Released to account for individual annotation behavior, the authors applied a rigorous annotation process to ensure that discrepancies were due to subjective perceptions and not due to annotation errors. In our experiments, we use a split of the dataset that was introduced by Romberg (2022). However, in contrast to the original work, we opted not to use repeated k-fold cross-validation to minimize the use of computational resources and energy consumption. This decision was based on the reported small deviations in results between different splits. Argument Validity and Novelty Prediction Shared Task (abbr. ValNov, Heinisch et al., 2022) The shared task of the 2022 edition of the Argument Mining Workshop focused on two tasks – predicting once the validity and once the novelty of argumentative conclusions. In the corresponding dataset, consisting of English-language arguments from debatepedia.org, validity captures the extent to which a conclusion is justified given its premise. Novelty captures to what extent a conclusion contains content that is not merely a paraphrase of the premise. In both tasks, annotators had a binary choice but could abstain if they were unsure. A total of five annotators contributed to the annotation process, with each premise-conclusion pair labeled by exactly three annotators. While the original version of the dataset contains aggregated labels, we disaggregated these labels for the purpose of modeling human label variation. Although not originally designed as a dataset allowing for the study of the labels of single annotators, Heinisch et al. (2022) already emphasized the de-
gree of subjectivity in the annotation of validity and novelty. While about two-thirds of the annotation samples for validity and half of the annotation samples for novelty are non-controversial, i.e., there is complete agreement among the annotators, the labels vary more in the remaining cases, which is a manifestation of their subjectivity. In light of the careful selection of annotators and due to the comprehensive guidelines, we argue it is reasonable to assume that the variations in labels are thus due to individual perspectives. Given our interest in learning from human label variation, we required samples to be present in the training data for each of the five annotators. As this was not the case for the original split by Heinisch et al. (2022), where two annotators were only introduced in the test split, we re-partitioned the dataset. In doing so, we adhered to the original course of action by avoiding topic overlap between training and the other data, sharing eight of the total 37 topics between development and test data, and introducing seven novel topics in the test data. We also kept the proportions of the original split in terms of the premise-conclusion pairs included. As a result of the non-aggregated approach, the premise-conclusion pairs in the resulting dataset may contain fewer than three labels, as we exclude individual decisions of abstain2. Table 1 provides a general overview of the two datasets’ distribution amongst training (train), development (dev), and test set, as well as the number of classes. Going more into detail regarding annotatorindividual labels, the proportion within the splits in ValNov - unlike in the Concreteness dataset - depends on the respective annotator. For this reason, Table 2 breaks down how strongly each annotator is represented in ValNov. It shows that the distribution varies greatly, with the predominant annotator covering nearly the entire dataset, while the least represented annotator has assigned labels to only 12% of the examples. 4.2 experimental setup We use the transformers-library by Wolf et al. (2020) to implement the text processing units (pre-trained LLMs) in all models. We
2The annotator-individual split of ValNov is included in the GitHub repository: https://github.com/ phhei/RelatePerspectives-sweetspots/tree/main/ Datasets/ValNov-new_split
use the same LLM architecture (RoBERTa, Liu et al., 2019) for all models and settings. RoBERTa has proven to be effective in the prediction of different aspects related to argument quality (Gurcke et al., 2021; Heinisch et al., 2022). We use RoBERTa base variants, namely roberta-base for the English ValNov dataset and roberta-base-wechsel-german (Minixhofer et al., 2022) for the German Concreteness dataset. Thus, for all models evaluated on the same dataset, the majority of initial weights are equal. During training, we use a class-weighted cross-entropy loss. These class weights are calculated and applied for each annotator separately based on the train split (except for the Majority-model where we use the majority label for calculating the class weights). Further choices for model components, in particular for different implementation options for the recommender architectures, were determined during preliminary experiments (Appendix C). For further details on the selection of hyperparameters, the training, and used computational resources, see Appendix A. As described in Section 4.1, we use fixed data splits for both datasets (Concreteness and ValNov), having the concreteness task in a setting in which all annotators are equally represented and the two tasks in the ValNov dataset in a setting in which the annotated set of instances differs among the annotators (cf. Table 2). We perform ten runs of training and evaluation for each architecture type, using the same random seeds in the same order (see Appendix A). We report scores based on averages over individual runs. 4.3 evaluation metrics How to best evaluate models that do not learn from aggregated labels is an open problem (Basile et al., 2021; Uma et al., 2021b), and various approaches to move beyond majority labels in evaluation exist (Plank, 2022; Leonardelli et al., 2023). We follow studies on annotator-level models in evaluating against individual annotator’s decisions (Davani et al., 2022; Gordon et al., 2022; Orlikowski et al., 2023). However, instead of calculating scores over all individual annotations, we derive scores in two steps: We first calculate the macro-averaged F1 for each annotator separately (i.e., annotator-level scores). As a second step, we take the average of annotator scores as a model’s score. This is done for each run of our models, so that the final score is the average of the single-run model scores (i.e., annotator-average scores). This two-step calculation has several advantages in our setting. By first calculating per-annotator scores, the final score more appropriately reflects how well our models represent each annotator. Additionally, this method allows us to analyze model performance annotator-wise. Based on the range of these scores, we can investigate how much performance diverges between annotators. 5 results & evaluation We provide results for the five models considered along the spectrum of architectures as outlined in Figure 1. We evaluate these architectures on two levels using the previously described two-step calculation of model scores. In Section 5.1, we use the annotator-average scores in order to evaluate models across individual predictions (suitable for use cases in which a global indicator of performance is desired). To have a more fine-grained examination (suitable for use cases aiming to maximize argument quality for an individual user) we evaluate against the non-averaged annotator-level scores in Section 5.2. See Appendix B and C for full results and additional recommender configurations. 5.1 annotator-average results The annotator-average results are provided in Table 3. All models outperform a naive baseline consisting of always predicting the most frequent label in the training set: “high” for argument concreteness, “valid” for validity prediction, and “not novel” for novelty prediction, which are the same for all annotators. There are major improvements between
+24.7 and +31.69 F1-points in the concreteness task deciding between three classes and some improvements between +7.31 and +29.22 F1-points and between +1.17 and +21.38 F1-points for binary validity and novelty classification, respectively. These diminishing gains over the naive baseline reflect the increasing task difficulty from concreteness to novelty classification (Romberg et al., 2022; Heinisch et al., 2022). Looking at the different architectures along our architectural continuum, the variation regarding the F1-scores is larger with respect to validity and novelty prediction than with respect to the correctness prediction task. We find that all three tasks show similar patterns: the models at both ends of our architectural continuum (PerAnnotator and Majority) underperform in general compared to the hybrid models. The PerAnnotator-Models result in 54.57, 44.04, and 41.83 F1-points for predicting concreteness, validity, and novelty on average, respectively. Especially for the latter two tasks where the available training data for some models is reduced to minor amounts (Table 2), which are typically not sufficient to finetune a model, this approach falls apart. However, using the Majority model does not increase the performance much in validity and novelty (+3.59 and +1.64, respectively) or even reduce the performance in the concreteness task (−0.75). Using a hybrid approach increases the performance always with only one exception (SepHeads with 50.83 in the case of concreteness classification). In all other cases, we successfully relate the different perspectives by capturing the common ground of the tasks by also incorporating (and relating) the individual perspectives of the annotators. Having a medium ratio of annotator-specific parameters (Figure 1) yields superior performance. In all three tasks, ShareREC yields the best-averaged F1scores with 57.83, 65.95, and 62.04 in predicting concreteness, validity, and novelty, respectively, closely followed by the SepREC with annotatorindividual text encoders (57.77, 62.08 and 57.03, respectively). The model using separated heads yields F1-scores of 50.83 (concreteness), 53.82 (validity), and 47.04 (novelty). 5.2 minimum and maximum annotator-individual results Beyond considering only aggregated results in terms of average F1, we also investigate the ex-
tent to which the models considered can represent the perspectives not only of annotators on average, but all of them. For this, we consider the variability in F1 between the user whose perspective (subjective labeling behavior) is captured best and the user whose perspective is captured the worst (i.e., the range). A large range shows that there are large differences in how well we can represent the subjectivity across users. Table 4 shows the results per model in terms of F1 for the annotators whose behavior can be modelled best (max) and worst (min), respectively. The highest scores across annotators in terms of min and max are yielded by ShareREC (having F1-scores of between 54.01 and 62.71 for concreteness, 56.12 to 71.16 and 56.80 to 68.85 for novelty), while the other models at both ends of our architectural spectrum (PerAnnotator and Majority) show poor performance, both on the best and worst annotators. In the concreteness task, where each annotator labeled each sample, we observe that models that include annotator-specific encoders (PerAnnotator and SepREC) can successfully model the subjective text reading behavior of specific annotators. Hence, the PerAnnotator-models have a maximum F1 score of 62.88, which is only outperformed by the separated recommenders yielding the overall best single-annotator F1-score of 67.54, successfully combining the individual view on text for this annotator3 with the common ground. However, the individual annotator engagement of those models (including the SepHeads model) involves the risk of overfitting individual trends – showing a comparable high standard deviation between the prediction performances (≈ 6). Therefore, the best result regarding the minimum annotator F1-score is obtained by the more conservative ShareREC, yielding a score of 54.01. For validity and novelty, where each annotator labeled a different amount of samples, architectures featuring text encoders that are specific for single annotators fail to reliably predict the behavior of annotators having provided few samples. For example, the PerAnnotator-models have an F1-score of 34.09 and 39.15 for validity and novelty, respectively, taking the most underrepresented annotator into account. This performance is worse than the majority baseline (34.62 validity and 45.24 novelty). Especially for annotators with sparse labels,
3This annotator has the strongest correlation between label decisions and text length. Only SepREC maximizes the F1score for this annotator. it seems thus crucial to share the text encoding components of the architecture. Among the architectures that share components, the recommender systems inspired architectures perform best with respect to predicting the labeling behavior of both under- and overrepresented annotators, yielding the highest min- and max-F1 scores. Recommenders with a shared text encoder perform between 56.12 and 71.12 for validity and between 56.80 and 68.85 for novelty, yielding the best scores for these two tasks. When looking at the goal of catching contrastive views and opinions, for example, to detect “hard cases”, further insights considering the predicted agreement reveal a weakness of the ShareREC models. ShareREC models stress the common text understanding and learn to predict better-matching majorities that are appropriate for more annotators, resulting in almost no divergent predicted labels per sample. Recommenders with annotator-separate text encoders (SepRECs) model different perspectives much better, having a predicted Fleiss’ kappa inter-annotator agreement between κ = 0.73 (novelty) and κ = 0.79 (concreteness). However, modeling the individual traits of annotators is challenging, especially for complex tasks such as validity and novelty, explaining the overall worse performance of SepRECs in comparison to ShareRECs in these two tasks. 6 qualitative analysis: case study The evaluated models show different behaviors that result from how they incorporate individual annotators. These differences can best be illustrated by
discussing controversial examples. We consider in particular an example from the Concreteness dataset in which the full range of possible labels is provided by annotators. Table 5 shows such an example of an argumentative text unit about a “bike lane blocked by cars” where two annotators assigned the label “high concreteness”, two said it was of “intermediate concreteness”, and one labeled the example with “low concreteness”. Predictions are taken from models trained with the first random seed (see Section 4.2). The Majority model, by definition, predicts the same label for each annotator. The predicted value, intermediate concreteness, is plausible in this case at face value: Examining further examples from the dataset shows a tendency for short texts to be annotated as less concrete. However, for two annotators this is an example of high concreteness, despite its brevity. Thus, there is no clear majority label in this controversial case, so this prediction misses three out of five annotators. ShareREC shows the same prediction pattern as the Majority model. This is in line with the model’s discussed tendency to predict uniform labels per example. The model stresses the common ground based on its shared text representation and picks a plausible label. The PerAnnnotator and SepHeads models show more diversity in their predictions. This makes sense, as their architectures contain components for more variation isolated from other annotators via separate classification heads or models. This example underlines, however, that variation does not necessarily lead to more accurate representation of annotators overall. Both models, again, only manage to predict two out of five labels correctly. Thus, they are not more accurate than the uniform predictions. SepREC, in contrast to all other models, is very close to predicting the actual distribution of labels. For the one annotator it misses, the tendency of the label (less concrete) is correct. SepREC is thus the only model to predict the class “high concreteness” correctly for two annotators. In this example, the model picks up a peculiarity of argument concreteness in public participation related to traffic planning: while generally short argumentative units tend to be less concrete, they might be very concrete to some annotators. For example, annotators might have specific knowledge about local contexts, such as that there are certain bike lanes in a city
that are frequently blocked. This observation is in line with SepREC achieving the highest annotatorindividual scores on the Concreteness dataset (cf. Table 4). 7 conclusion & future work In this work, we have proposed a general framework to investigate the performance of different architectures on subjective annotation tasks and similar cases where different legitimate perspectives exist on the appropriate labeling decision across annotators. We have, in particular, highlighted architectures predicting labels for individual annotators along a continuum from fully annotator-specific architectures to architectures that rely only on aggregated annotations that completely disregarded individual annotators. Our main focus has been on hybrid architectures along these extreme ends of the continuum that model commonalities in annotation behavior across individual annotators. Regarding different aspects of argument quality (concreteness, conclusion validity, and conclusion novelty), we have shown that such hybrid architectures are best suited to classify the range of opinions. Our results show that recommender architectures that use an annotator-shared LLM for encoding the text excel in this setting in terms of maximizing the F1-score averaged over all individual annotators up to 43% over a majority-label model. Nevertheless, further analysis going beyond this averaged score has emphasized the importance of having an annotator-tailored text encoding to capture the different reading nuances in “hard cases”, resulting in a variance of predicted classes. Our work provides a starting point for including human label variation into subjective tasks in the field of argument mining, such as the assessment
of argument quality, without denying generally accepted objective concepts of argumentation.