Polymer simulation with both accuracy and efficiency is a challenging task. Machine learning (ML) forcefields have been developed to achieve both the accuracy of ab initio methods and the efficiency of empirical force fields. However, existing ML force fields are usually limited to single-molecule settings, and their simulations are not robust enough. In this paper, we present PolyGET, a new framework for Polymer Forcefields with Generalizable Equivariant Transformers. PolyGET is designed to capture complex quantum interactions between atoms and generalize across various polymer families, using a deep learning model called Equivariant Transformers. We propose a new training paradigm that focuses exclusively on optimizing forces, which is different from existing methods that jointly optimize forces and energy. This simple force-centric objective function avoids competing objectives between energy and forces, thereby allowing for learning a unified forcefield ML model over different polymer families. We evaluated PolyGET on a largescale dataset of 24 distinct polymer types and demonstrated state-of-the-art performance in force accuracy and robust MD simulations. Furthermore, PolyGET can simulate large polymers with high fidelity to the reference ab initio DFT method while being able to generalize to unseen polymers. 1 ar X iv :2 30 9. 00 58 5v 1 [ cs .L G ] 1 S ep 2 02 3 1 introduction Efficient and accurate forcefield computation is essential for polymer simulation, but it remains a challenging task. Although ab initio methods offer excellent accuracy, they are computationally slow. For example, Density Functional Theory (DFT)[1, 2] has a time complexity of N3e , where Ne is the number of electrons, rendering it too expensive for large polymers or long-horizon simulations. In contrast, empirical forcefields are more computationally efficient, as they simplify chemical interactions between atoms to a sum over bonded and unbonded pairwise terms[3, 4]. This approach reduces the time complexity to N2a , where Na is the number of atoms, allowing for simulating larger molecular systems. However, their accuracy is limited due to the simplification of chemical interactions, which results in suboptimal molecular dynamics (MD) simulation performance and chemical insights [5]. Machine learning (ML) forcefields combine ab initio accuracy with empirical forcefield computational efficiency. Early methods, such as kernel regression [6–8] and feedforward neural networks [9], rely on hand-crafted features, limiting flexibility and expressivity for complex molecular structures and interactions [5]. Recent Equivariant Graph Neural Networks (EGNNs)[10–13] enable automatic feature learning, achieving state-of-the-art accuracy in predicting energy and forces. However, existing EGNNs face two main limitations: (1) They are trained in a single-molecule setting, necessitating separate models for different molecules and lacking generalization to unseen molecules. In Section5.2, we will demonstrate that adapting EGNNs to the multi-molecule paradigm is nontrivial due to conflicting objectives in optimizing energy and force. (2) They are not robust enough for simulations [14], despite achieving high accuracy in static forcefield prediction. The singlemolecule training paradigm can lead to overfitting to individual molecules, compromising the model’s ability to perform robustly in dynamic simulations. We introduce a new ML forcefield approach for polymer simulation, named Polymer Forcefields with Generalizable Equivariant Transformers (PolyGET). As depicted in Figure 1, PolyGET has two key advantages compared to existing EGNN models: (1) Multi-molecule training and generalization: The model trains over a diverse set of polymer families and can generalize to previously unseen and larger polymers from smaller ones. (2) Accurate and robust simulations: The model can produce reliable and precise forces during molecular dynamics (MD) simulations. PolyGET achieves these advantages through two novel designs:
First, we train a unified multi-molecule model using Equivariant Transformer [10] as the backbone model. As a variant of the Transformer model [15] based on the attention mechanism, the Equivariant Transformer facilitates learning interactions between atoms. By incorporating multiple layers of attention blocks, the model effectively captures complex quantum interactions. Furthermore, the Equivariant Transformer learns latent atomic features that are roto-translational equivariant to atomic positions, ensuring the invariance of the learned energy, which is necessary for energy conservation. Its expressivity and capacity make it a suitable backbone for PolyGET. Second, we propose a novel training paradigm that facilitates multi-molecule training with a unified ML model. While existing deep learning approaches [10, 16] jointly optimize potential energy and forces, our training paradigm focuses solely on the optimization of forces, which we have found to be generalizable across multiple polymer families. With our training paradigm, the underlying ML model acquires robust and transferable knowledge about
quantum mechanical interactions between atoms, enabling both accurate and robust MD simulations and the capacity to generalize to unseen and larger polymers with similar quantum interactions. Although our model concentrates on force optimization, it can also output energy that is linearly correlated with ground truth energy, allowing the model to estimate ground truth energy with numerical integration. We have trained and evaluated the PolyGET model on Poly24, a novel benchmark comprising 24 polymer types and 6,552,624 conformations from four categories: cycloalkanes, lactones, ethers, and others. PolyGET demonstrates state-of-the-art force accuracy and generalizes to previously unseen large polymers. Furthermore, we have assessed the capacity of PolyGET to perform accurate MD simulations. We found that even for large 15-loop polymers not present in the training data, PolyGET can simulate polymers with high fidelity to the reference DFT data. In summary, we introduce a novel PolyGET for polymer forcefields, achieving: (1) a unified Equivariant Transformer model that can transfer across multiple polymer families for forcefield and energy prediction, (2) accurate and robust MD simulation for large unseen polymers, and (3) state-of-the-art performance in force prediction and MD simulation on a large benchmark Poly24. This paradigm can be applied to various deep learning models, enabling the development of general ML forcefields and robust MD simulation for a broader range of chemical families. 2 the polyget model  2.1 force-centric multi-molecule optimization Let x denote a molecule consisting of nx atoms. The conformations of these atoms are represented by rx ∈ Rnx×3 and their atom types are denoted by zx ∈ Rnx . The potential energy, E, is a function of the conformations and atom types. The forces exerted on the atoms are derived by F =− ∂E∂r . In molecular dynamics (MD) simulations, molecular conformations are influenced by the forces applied and the simulation thermostat. Define TDFT(x,r0,T ) as the distribution of conformations in a density functional theory (DFT)-based MD simulation for a specific thermostat T and initial conformation x0. Machine learning (ML) force fields aim to learn a model, Φθ , parameterized by θ , which approximates DFT-generated reference data. Existing ML force field models optimize model parameters θ by minimizing the following objective:
θ = argminEDFT [ ∥EML −EDFT∥22 +∥FML −FDFT∥22 ] . (1)
This optimization process is typically conducted in single-molecule settings by existing EGNN models, which learn a separate model for each molecule. As discussed in Section 1 and will be demonstrated in Section 5, models trained in a single-molecule setting not only fail to generalize to unseen molecules but also perform poorly in simulations due to their tendency to overfit the single molecules. Although it is natural to question whether a unified model can be trained to leverage DFT reference data for multiple molecules, this task is nontrivial. The challenge of training a unified model for multiple molecules can be understood by analyzing the per-atom potential energy and force distributions. As illustrated in Figure 2, although forces follow similar distributions for different types of polymers, the per-atom potential energy does not. We hypothesize that the mismatch between potential energy and force distributions is the primary cause of the optimization bottleneck and the reason for the preference for single-molecule settings in existing methods. In Section 5.2, we demonstrate that when extending existing EGNN methods for multi-molecule training— i.e., training a unified ML force field for various types of molecules—their performance can deteriorate instead of improving. In our approach, we address this challenge and learn generalizable force fields from multiple molecules by proposing a new learning objective:
Ex,r0 [ Er∼TDFT(x,r0,T ) [ ∥−∇Φθ(r,z)−F(r,z)∥22 ]] . (2)
In this objective, we minimize the differences between the predicted forces −∇Φθ (r,z) and the ground-truth forces F(r,z). The loss is optimized across multiple DFT trajectories for different types of polymers. The primary distinction between our approach and existing methods lies in the exclusive focus on forces, rather than jointly optimizing the potential energy and forces. This design choice is grounded in two key rationales: 1) forces, being a local atomic attribute, exhibit greater transferability across diverse molecules [17, 18], and 2) forces demonstrate similar distributions among different molecules, as evidenced in Figure 2. Conversely, the potential energy, a molecular-level attribute, displays considerable variation in its distributions—even after normalization—due to the intricate quantum mechanical processes
involved. By focusing on forces during training, our model can efficiently transfer information across DFT-generated data without being hindered by competing objectives between energy and force. In order to perform well across a wide array of molecules and conformation distributions, the unified model must effectively capture generic atomic interactions, rather than overfitting to a single molecule. As a result, this comprehensive training on various molecules bolsters our model’s capacity to generalize to unseen molecules and conduct more robust simulations. 2.2 energy prediction The objective function (2) provides a supervision signal to the model only on the forces. In order to predict the potential energy for molecules during simulation, we observe that by training exclusively on forces with (2), the model output Φθ is linearly correlated with the ground-truth potential energy E (Figure 3). We thus call the output energy from Φθ as uncalibrated potential energy. To predict the true potential energy E from Φθ , we can estimate Et with Et = E0 + ∫ t 0 ∂Φθ ∂r (rs,z)ds for a fixed time window t ∈ [0,T ], where E0 is the initial energy of at the start of the simulation. The integral can be approximated with numeric Taylor expansion:
Et = Et−1 − 〈
∂Φ ∂r
(rt−1),rt − rt−1 〉 . Then given uniformly sampled 0 = t0 < .. . < tm = T , we can estimate the potential energy function with
E∗(r,z) = WEΦθ (r,z)+bE ,
where (WE ,bE) = argmin m
∑ i=0 ∥(WEΦθ +bE)−E i∥. (3) forces  3 model architecture: equivariant transformer In conjunction with our new learning objective, we use Equivariant Transformer [10] as our backbone model for PolyGET. Equivariant Transformer uses the powerful self-attention mechanism [15] and the Transformer architecture for molecular data. The self-attention mechanism allows the model to capture interatomic interactions and the attraction-repulsion dynamics. Furthermore, Equivariant Transformer builds roto-translational equivariant to atomic positions — this not only allows it to learn in a more data-efficient way, but also ensures the invariance of the learned energy for the energy conservation law. As shown in Figure 4, the Equivariant Transformer architecture operates on molecular data in the form of graphs and consists of three main blocks: an embedding layer, an updated layer, and an output network. embedding layer The first component is an atom embedding layer, which transforms atoms to vector representations that encode the quantum mechanical information about the atoms and their surroundings. The atom embedding is further split to scalar embeddings h0 ∈ RD and vector embeddings v0 ∈ R3×D, where D is the embedding size. The scalar embedding is derived by integrating both the intrinsic vector, which captures information specific to the atoms themselves, and the neighborhood vector, which encapsulates the interactions within the atomic neighborhoods. The intrinsic vector, denoted as xint, is acquired by associating each atom type with a corresponding dense feature vector in RD. To compute the neighborhood embedding xnbr, we initially define the distance filter using a radial basis function. Subsequently, a continuous-filter convolution technique is employed to aggregate information from the surrounding atoms within a given neighborhood. The atom embedding x is obtained by the concatenation of intrinsic and neighborhood embeddings. The vector embedding v is initialized as zero vectors in R3×D. update layers The update layers defines the incremental change of the scalar embeddings x and vector embeddings v in each network layer. For each atom i, the update layers are defined as follows:
∇xi = f x(xi)+∑ j φ x(xi,x j)+ψxv(vi)
∇vi = f v(vi)+∑ j r j − ri ∥r j − ri∥ φ v(xi,x j)+ψvx(xi). (4)
For each atom i, the attention mechanism φ x and φ v will model its interatomic potentials to all other atoms. Based on the embeddings of two atoms and their relative distance vectors, φ x and φ v define the interaction between the two atoms. f x and f v are the residual connections [19] that prevent the network to suffer from performance degradation. ψxv and ψvx define the information exchange between x and v in each layer. ψxv is used to update the vector embedding v from the scalar embedding x, and ψvx is used to update the scalar embedding x from the vector embedding v. output network The output network computes scalar atom-wise predictions using gated equivariant blocks, which is a modified version of the gated residual network [10]. The gated equivariant blocks aggregate the atomic embeddings into a single molecular prediction. This prediction can be matched with a scalar target variable or differentiated against atomic coordinates to provide force predictions. 4 dataset Our PolyGET model is trained on a DFT-generated dataset [21] for computing ring-opening enthalpy (∆HROP), which is a crucial thermodynamic concept governing ring-opening polymerization. This process involves breaking a ring of cyclic monomers and adding the “opened” monomer to a long chain, which forms a polymer chain. The data were generated by utilizing MD simulations of multiple monomer and polymer models at a uniform level of DFT [1, 2] computations. More details about our data generation process can be found in Refs. [21, 22]. Given a cyclic monomer, several polymers models were created using Polymer Structure Predictor (PSP) package [23]. Each of them was obtained by multiplying the monomer with a small integer L, e.g., L = 3,4,5, and 6, forming a loop of size L (larger loops are better models of polymers). For each monomer or polymer models, about 10 or more maximally diversified configurations were selected as the initial configurations of the DFT-based MD simulations. As all of our models are non-periodic, we used the Γ-point version of Vienna Ab initio Simulation Package (VASP),[24, 25] employing a basis set of plane waves with kinetic energy up to 400 eV to represent the Kohn-Sham orbitals. The ion-electron interactions were computed using the projector augmented wave (PAW) method [26] while the exchangecorrelation (XC) energies were computed using the generalized gradient approximation Perdew-Burke-Ernzerhof (PBE) functional. [27]
We show the polymers used in this study in Table 1, which displays all 24 polymer types which have been broadly classified into cycloalkanes, ethers, lactones, and others, following the classification used in [21]. Averagely, the dataset includes 10 DFT trajectories for both monomers and polymers. The polymers are formed by polymerizing the monomers, resulting in a comprehensive collection of data for each polymer type. The average DFT trajectory lengths for polymers up to 6-loop are listed in Table 2. In total, we have 1311 DFT trajectories and 6,552,624 molecular conformations. 5 results In this section, we present the empirical performance of PolyGET on Poly24. First, we report on the simulation performance of PolyGET on large 15-loop unseen polymers in Section 5.1, demonstrating the accuracy, robustness, and speed of PolyGET for large polymer simulation. Then, we explore the multi-molecule training performance of PolyGET in Section 5.2. We show that our training paradigm is crucial in enhancing force prediction accuracy and the generalization capabilities of the model. 5.1 md simulation accuracy  model training and simulation setup. We have developed a 6-layer Equivariant Transformer with an embedding dimension of 128 and 8 attention heads. During training, unless specified otherwise, we trained the model with
5-loop polymers and smaller ones. Polymers of size 6 and above were unseen during training and were used as test data. We used 5% of the training data as the validation set and recorded the validation loss after every 500 training steps. The model has trained with the AdamW [28] optimizer for 3 epochs with an initial learning rate of 0.0001, and the learning rate was reduced by a factor of 0.8 whenever the validation loss increased for 30 consecutive validation steps. For testing the model’s MD simulation performance, we take the 15-loop polymerization of 4 example polymers from Table 1, CK6, OTH4, LAC2, and LAC5, and run MD simulations on them with PolyGET-generated forcefields for a maximum of 600K steps. The number of atoms for these 15-loop polymers are, respectively: 360, 360, 180, and 240. The maximum loop size is seen in the training data of PolyGET is 5-loop. Therefore, PolyGET is required to perform accurate MD simulations on unseen polymers, testing both its simulation and generalization ability. For simulation, we have integrated our method into the ASE [29] simulation environment. A diagram of the simulation is shown in Figure 4. We use Nosé–Hoover thermostat for all simulations. The velocities for molecules are initialized with Maxwell-Boltzmann distribution to simulate the desired temperature environment. Unless otherwise specified, we use a temperature of 300 Kelvin and a timestep of 0.5fs. results. Figure 5 (a)-(d) visualize the comparison results between PolyGET predicted forces and DFT reference data. For all 15 loop polymers, PolyGET obtains highly accurate forces to DFT calculations, with 0.01 eV/ MAE in energy and close to zero cosine distance. Hence, PolyGET can perform MD simulations on multiple types of polymers with uniformly high correlation with DFT references. The test 15-loop polymers contain up to 360 atoms, unseen in the training data. This shows that PolyGET can extrapolate well to large unseen polymers with known monomers. Furthermore, in practice, PolyGET only needs training data from small polymers, which are cheap to generate ab initio data and thus greatly reduce the cost of DFT data generation. In Figure 5 (e) to (h), the Root Mean Square Deviation (RMSD) and potential energy graphs are shown. RMSD curve measures how much the polymer structure deviates from the initial conformation, and the potential energy graph indicates the energy state of the simulated polymers. Both RMSD and potential energy curves indicate that the polymer in simulation converges to a stable distribution around the equilibrium. Furthermore, PolyGET can explore different possible conformation states of the polymers, as illustrated in Figure 5 (i) to (l). simulation efficiency. The PolyGET model was trained on all polymers with loop size 5 and below using two A5000 GPUs, which took approximately 30 hours. However, this is a one-time cost, as the trained model can serve as a unified model for all future simulations of different polymers. For 15-loop polymers, the model predicts atomic forces for 20-30 steps per second on average. This means that the simulations for 600K steps, as mentioned in the previous section, can be completed in 6-8 hours. In contrast, the DFT model takes hours on CPU parallelization and still 5-10 minutes on GPU parallelization for one single-step calculation — we speed up DFT simulations by at least 6000 times. Depending on the available GPU memory, the
PolyGET model can be parallelized to perform multiple simulations simultaneously.5.2 effectiveness of multi-molecule training In this subsection, we show that the multi-molecule training paradigm can crucially improve the prediction performance of PolyGET. To this end, we compare PolyGET with 3 state-of-the-art machine learning forcefields models: SchNet [16], NequIP [12], and TorchMDNet [10]. These methods feature different implementations of EGNN and have achieved accurate forcefield prediction on static DFT trajectories. Our model achieves state-of-the-art multi-molecule force prediction accuracy. To demonstrate this, we train PolyGET and the baseline models on all polymers with loop sizes below 6 (Table 1). We then evaluate the force prediction performance of different models on unseen 6-loop polymers of the same monomers. Table 6 reports the results. As shown, our model PolyGET attains 0.073 eV/Å MAE and a near-zero cosine distance compared to DFT forces, which is 6 times better than the previous state-of-the-art model TorchMDNet. Our model benefits the most from diverse training data. In Figure 7(a), we compared the performance of baseline models when trained separately or jointly on different polymers. For NequIP and TorchMDNet, joint training on all polymers brings no significant improvement and could even harm force prediction accuracy. In contrast, our model benefits from having multiple types of polymers in the training data, significantly reducing force prediction errors. It is worth noting that our model shares a similar Equivariant Transformer architecture with TorchMDNet [10]. The performance gain is mainly because of our force-centered training paradigm. As depicted in Figure 7(c), This paradigm allows the
validation MAE on forces to decrease more consistently in multi-molecule training, leading to a significantly improved model performance. Such results demonstrate the effectiveness of our force-centric multi-molecule training paradigm. The multi-molecule paradigm improves model extrapolation. In Figure 7(b), we report the baseline models’ performance on “unseen” polymers compared to “seen” polymers. “Seen” polymers refer to polymers with loop sizes below 6 present in the training data, while “unseen” polymers pertain to 6-loop polymers. In line with previous multi-molecule performance observations, our model experiences minimal performance deterioration (0.012 eV/Å to 0.013 eV/Å) when applied to unseen polymers. In contrast, NequIP’s force MAE drops from 0.080 eV/Å to 0.103 eV/Å, while TorchMDNet drops from 0.043 to 0.056. This result shows that the multi-molecule paradigm plays a vital role in enhancing the model’s extrapolation capabilities. Our model accurately predicts energy with linear transformation. Although our model does not directly optimize potential energy, it still captures the potential energy distributions for different polymers, as shown in Figure 8(a). Averaged across different polymers, PolyGET achieves an average of 0.0067 cosine distance between the model output and the DFT potential energy. After learning a linear regression on eight ground-truth conformations for each polymer, our model attains an average of 0.0006 eV/Å/atom MAE between the transformed model energy prediction and the DFT potential energy and near 0 cosine distance. We can use this adjusted model output to predict accurate potential energies for polymers, as shown in 8(b). 6 related work Machine learning forcefields have been attracting increasing attention, in order to achieve both high accuracy of ab initio methods and the computation efficiency of empirical forcefields [5, 17, 18]. Traditional machine learning approaches rely on “fingerprints”,
pre-engineered interatomic features describing atomic environments. Similar to classical forcefields, these features describe pairwise radial or angular relations between atoms, and relatively simple machine learning algorithms such as kernel regression [6–8] and neural networks (NN) [5, 17, 18] can transform these features to find the optimal statistical relations between the features and the potential energy and forces. Particularly, neural networks have the universal approximation property [30] that in theory can approximate arbitrarily complex functions, implying that with good enough features, the NN models can approximate ab initio forces with high accuracy. Of course, in practice, optimality is difficult to achieve due to problems such as overfitting, model capacity, and the quality of feature engineering. Still, neural networks have been used in calculating forces for many different types of materials [31–36]. Despite their success, traditional machine learning approaches rely on hand-crafted features that are not flexible enough to represent the complex quantum mechanical interactions between atoms. Imagine if the features are all based on pairwise distances between atoms. In such a case, no matter how complex the subsequent model is, the information it uses to make predictions would not extend beyond the pairwise relations defined in the features. This is not desirable if more complex quantum mechanics are involved. The state-of-the-art approaches in machine learning forcefield take advantage of the recent advances in Graph Neural Networks (GNN). Instead of pre-defined fingerprint features, these algorithms model directly the interaction between atoms in the latent space. Properly designed with equivariant constraints, they have theoretically universal approximation ability to equivariant and invariant physical systems [37]. By treating molecules as graphs with 3D coordinates, GNN enables the learning of quantum mechanical interactions between atoms directly with equivariant operators, such as neural message passing [38], EGNN [13], SchNet [16], DIME [39], GemNet [40], and NequIP [12], and TorchMDNet [10]. With stronger model capacity, they could achieve comparable accuracy with ab initio forcefield. However, these models typically are only trained and evaluated on given conformations from single molecules [10, 12, 16, 40] (i.e., in-distribution data), and it is nontrivial to directly extend them to train in multi-molecule settings. Furthermore, existing machine learning methods are prone to instability on out-of-distribution data, limiting their performance in MD simulations where noises are accumulated for atomic positions. The instability of existing methods is empirically analyzed by [14]. Among existing machine learning forcefields, AGNI [17, 18] and sGDML [6, 7] agree with us in focusing on forces optimization. Several different reasons are stated. [6] observes the amplification effect of the differentiation operator on noises. Combined with the inherently noisy nature of machine learning algorithms, [6] claims that models trained on potential energy would cause considerable noise in the forces predictions. [17, 18] present a learning scheme where the local forces of atoms are learned and used to simulate geometry optimizations and MD simulations, instead of the global potential energies. Adding to their observations, we find empirical evidence that the potential energy and forces are competing in the optimization of machine learning algorithms and that the forces of atoms, being localized features that can be derived from local atomic environments, have the benefit of better generalization across different molecules. 7 conclusion We introduce PolyGET, a novel polymer forcefields learning framework with Generalizable Equivariant Transformers. Focusing on force optimization exclusively, our training paradigm enables accurate molecular dynamics simulations and generalization across polymer families and unseen polymers. Our multi-molecule approach fosters robust, transferable knowledge of interatomic interactions, contrasting with existing forcefields that jointly optimize potential energy and forces. Our results on large polymer simulation datasets demonstrate PolyGET’s accuracy, robustness, and speed, showcasing its potential for material science and industry applications.