Spatio-temporal graph neural networks (STGNN) have become the most popular solution to traffic forecasting. While successful, they rely on the message passing scheme of GNNs to establish spatial dependencies between nodes, and thus inevitably inherit GNNs’ notorious inefficiency. Given these facts, in this paper, we propose an embarrassingly simple yet remarkably effective spatio-temporal learning approach, entitled SimST. Specifically, SimST approximates the efficacies of GNNs by two spatial learning techniques, which respectively model local and global spatial correlations. Moreover, SimST can be used alongside various temporal models and involves a tailored training strategy. We conduct experiments on five traffic benchmarks to assess the capability of SimST in terms of efficiency and effectiveness. Empirical results show that SimST improves the prediction throughput by up to 39 times compared to more sophisticated STGNNs while attaining comparable performance, which indicates that GNNs are not the only option for spatial modeling in traffic forecasting. 1. introduction In recent years, urban traffic forecasting has emerged as one of the most important components of Intelligent Transportation Systems. Given historical traffic observations (e.g., traffic speed, flow) collected from sensors on road networks, the task focuses on predicting future traffic trends for each sensor, which provides insights for improving urban planning and traffic management (Zheng et al., 2014). Spatio-Temporal Graph Neural Networks (STGNNs) have become the de facto most popular tool for traffic forecasting, in which sequential models such as Temporal Convolution Networks (TCNs) or Recurrent Neural Networks (RNNs)
1National University of Singapore 2University of Hong Kong 3Nanyang Technological University. Correspondence to: Xu Liu <liuxu@comp.nus.edu.sg>, Yuxuan Liang <yuxliang@outlook.com>. Preprint. Under review. are applied for modeling temporal dependencies (Yu et al., 2018; Pan et al., 2019; Wu et al., 2020; Lan et al., 2022), and Graph Neural Networks (GNNs) (Kipf & Welling, 2017; Defferrard et al., 2016) are utilized to capture spatial correlations among different locations. After revisiting the STGNNs proposed in recent years, we find that they mostly focus on enhancing the spatial learning modules (i.e., GNNs) with either complex aggregation rules or sophisticated layers to improve predictive performance. While successful, we note that they rely heavily on GNNs for performing the message passing step and thus inevitably inherit GNNs’ notorious inefficiencies, especially when the graphs are large or with dense connections (Chen et al., 2021a; Zhang et al., 2022). As a concrete example, the commonly used adaptive adjacency matrix is built by matrix multiplication of two node embedding tables, producing a fully-connected graph (Wu et al., 2019; Bai et al., 2020; Han et al., 2021). During feature aggregation, such a fully-connected structure leads to quadratic computational complexity w.r.t. the number of sensors. Consequently, the scalability challenges of GNNs hinder the deployment of STGNNs in large-scale and real-time traffic forecasting systems that are latency-bound and require fast inference. Although there are plenty of efforts in the graph domain to improve the efficiency of GNNs, such as via graph simplification (Hamilton et al., 2017; Chen et al., 2018; Chiang et al., 2019; Zeng et al., 2020; Zheng et al., 2020b), the related studies in STGNNs are still scarce to the best of our knowledge, as such simplification usually leads to information loss and performance degradation (Wu et al., 2020). Given these facts and inspired by recent progress in eliminating GNNs for node classification (Zhang et al., 2022; Tian et al., 2022), we may ask: can we remove GNNs to trim down the explosive complexity while still remaining competitive in traffic forecasting accuracy? Present Work To answer this question, we first demonstrate the functionalities of GNNs as follows. The superior performance of GNNs stems from its structure-aware exploitation of graphs: (1) for each node in the graph, a single GNN layer is used to first aggregate features from nodes’ neighbors and then transform the aggregated representations via a feed-forward network, and (2) by stacking multiple layers, the hidden representations of nodes receive messages from long-distance neighbors. ar X
iv :2
30 1. 12 60
3v 1
[ cs
.L G
] 3
0 Ja
n 20
23
In this work, we propose two spatial learning modules to approximate the above efficacies of GNNs without requiring message passing, reducing the time complexity to linear. (1) Local Proximity Modeling. We take a local view and fragment the traffic network to build an ego-graph for each node, which is constructed by incorporating historical observations of the node’s neighbors. Then an MLP is applied to transform the observations to the hidden states at each time step. (2) Global Correlation Learning. Inspired by recommender systems that learn user embeddings to reflect user behavioral similarities (He et al., 2017; Zhang et al., 2019), we propose to use sensor embeddings to represent sensors’ inherent properties and collaboratively capture spatial relationships between arbitrary sensor pairs in a data-driven manner. Compared with stacking a number of GNN layers to enlarge the receptive field and build long-range spatial dependencies, our module sets up direct connections between nodes and thereby learns the global correlations. In practice, GNNs can be used alongside various temporal models, such as GRU (Chung et al., 2014), WaveNet (Oord et al., 2016), and Transformer (Vaswani et al., 2017), for spatio-temporal learning. Analogously, we empirically find that our proposed spatial modules are agnostic to the temporal encoders and work in a plug-and-play manner. Moreover, we devise a new training strategy for SimST to further boost performance, which increases sample diversity during batch formation and enhances generalization. Integrating the above components, we propose a Simple yet effective Spatio-Temporal learning approach termed SimST, which trims the quadratic cost and performs on par with STGNNs in empirical studies. Our contributions are summarized as follows. • We for the first time demonstrate that GNNs are not the
only choice for spatial modeling in traffic forecasting, by presenting a simple yet admirable method called SimST. • Despite its simplicity, SimST achieves remarkable empirical results in terms of throughput and accuracy against sophisticated state-of-the-art STGNNs: SimST is significantly more efficient than baselines, with up to 39× higher inference throughput, while performing on par with them. • We conduct ablation and case studies to promote a better understanding of our method and motivate future research to rethink the importance of GNNs in traffic forecasting. 2. related work Traffic forecasting is a crucial application in smart city efforts. In recent years, STGNNs have become the most widely used tools for predicting traffic (Cao et al., 2020; Liu et al., 2022b; Chen et al., 2021b; 2022). Generally, they integrate GNNs with either RNNs or TCNs to capture the spatial and temporal dependencies in traffic data. For
example, DCRNN (Li et al., 2018) considered traffic flow as a diffusion process and combined a novel diffusion convolution with GRU. Other efforts (Pan et al., 2019; Fang et al., 2021; Liu et al., 2022a) were also based on RNNs. To improve training speed and enjoy parallel computation, plenty of works (Wu et al., 2019; 2020; Li & Zhu, 2021) replaced RNNs with dilated causal convolution. A fundamental problem for using GNNs in traffic modeling is how to establish a graph structure. The mainstream approach to define such structures is using either a predefined and sparse matrix constructed from the road network distances between sensors (Yu et al., 2018; Li et al., 2018; Song et al., 2020), or an adaptive and dense matrix that records the pairwise relationships between nodes (Wu et al., 2019; Bai et al., 2020; Han et al., 2021; Choi et al., 2022). However, researchers have noted that the predefined matrix is heuristic and does not reflect the genuine dependencies between nodes, which degrades model performance (Wu et al., 2019; Bai et al., 2020). Models applying the adaptive matrix generally achieve superior performance. Though successful, the adaptive matrix discards the sparsity of the graph and incurs a quadratic computational cost, making it hard to deploy for latency-constrained or large-scale traffic applications. Other methods that use attention mechanisms for spatial learning (Zheng et al., 2020a) also suffer from quadratic time complexity. In this study, we propose SimST to alleviate the inefficiency issue by eliminating the inefficient GNN component in the model. 3. preliminary Traffic Forecasting Let G = (V, E) represent a directed sensor graph with |V| nodes and |E| edges, and XT ∈ R|V|×T×F denote the features of all nodes from time step 1 to T , where F is the feature dimension. The features usually consist of a target attribute (e.g., traffic speed) and other auxiliary information, such as time of day (Wu et al., 2019). Following common settings (Li et al., 2018; Wu et al., 2019), a weighted adjacency matrix A ∈ R|V|×|V| is applied to describe the graph topology, where Aij = exp(−dist(vi,vj) 2
s2 ) if dist(vi, vj) 6 r else Aij = 0, dist(vi, vj) denotes the road network distance between sensors vi and vj , s is the standard deviation of distances, and r is a threshold for sparsity (Shuman et al., 2013). The non-zero entries in A form the set of edges in E . In traffic forecasting, we aim to learn a neural network Θ to predict the target attribute in future Tf steps based on Th historical observations over the sensor graph:
G,XTh Θ−→ Ŷ Tf (1)
where XTh ∈ R|V|×Th×F indicates the observations and Ŷ Tf ∈ R|V|×Tf×1 is the predictions. A prediction loss, e.g.,
mean absolute error, is utilized to train the neural network:
L(Θ) = 1 |V| ∑ vi∈V |Ŷ Tf vi − Y Tf vi | (2)
Graph Neural Networks Although various forms of GNNs exist, our work refers to the conventional message passing scheme (Kipf & Welling, 2017) that is widely adopted in traffic forecasting. Specifically, message passing functions in two ways: aggregating one-hop neighbors to model local correlations, and stacking layers to build longrange relationships. Formally, the node representations H l at the l-th layer of GNNs are learned by first aggregating messages from node neighborsN (v), and then transforming the representations via feed-forward propagation:
H l = σ(ÃH l−1W l) (3)
where Ã denotes the normalized adjacency matrix, W l denotes the learnable weights at the l-th layer, and σ is the activation function (e.g., ReLU). 4. methodology As a typical spatio-temporal data mining task, traffic forecasting requires modeling both spatial and temporal aspects. In this section, we start by describing how spatial correlations are set up by the proposed spatial learning modules in Section 4.1. We then introduce three temporal encoders to capture the temporal dependencies and a predictor for generating the future in Section 4.2. Finally, we present a node-based batch sampling method for training SimST in Section 4.3. Figure 1 depicts the architecture of SimST. 4.1. spatial learning modules The inefficiency of GNNs has been extensively discussed within the graph domain (Chiang et al., 2019; Chen et al., 2021a; Zhang et al., 2022). Relying on GNNs to set up spatial correlations via message passing, STGNNs also suffer from flagrant inefficiencies. In this work, we propose two simple yet effective alternatives to approximate the function-
alities of GNNs and thus allow each node to exploit spatial information without message passing. Local Proximity Modeling A single GNN layer is capable of aggregating neighborhood features in a non-Euclidean structure. To consider such a locality characteristic without using a GNN layer, we propose to first perform graph fragmentation to generate an ego-graph for each node. The features of each ego-graph are the Th steps historical time series of the center node and its one-hop neighbors. Multihop neighbors can also be incorporated if needed, but we found that incorporating more neighbors may suffer from overfitting in experiments (see Section 5.4.3). We then transform the raw features at each time step to their hidden representations via a multi-layer perceptron (MLP). Specifically, for each node v, we consider its top-k one-hop neighbors based on the weights in the normalized matrix Ã, where Ã = D̃ − 12 (A + I)D̃
− 12 and D̃ is the degree matrix of A + I . We also consider the neighbors in the reversed direction, i.e., the entries in AT , so that the model can perceive the impact from forward and backward neighbors. Let N 1f (v) and N 1b (v) denote the selected one-hop neighbors in two directions. We construct the feature matrix XThGv ∈ R Th×(2k+3) of an ego-graph Gv during period Th:
XThGv = COMBINE(X Th v , {X Th vf : vf ∈ N 1f (v)},
{XThvb : vb ∈ N 1 b (v)},X Th avgf ,XThavgb) (4)
where the last two terms are the average histories of all onehop neighbors in two directions, allowing the center node to learn more about its local context. Note that the execution of building ego-graphs can be completed in data preprocessing, so it has no influence on model training and inference. Then, we apply an MLP to transform the raw features in XThGv and generate the representations H Th Gv ∈ RTh×Dm , where Dm is the model hidden dimension. Compared with a GNN layer, our module replaces message aggregation (i.e., weighted sum) operation with ego-graph construction that can be finished in the data preprocessing stage. Global Correlation Learning Representing users with unique embeddings and learning with these embeddings to capture similarities in behavior among users, is a fundamental technique in recommender systems (He et al., 2017; Zhang et al., 2019). Along a similar line, we argue that the traffic situation of a sensor is largely influenced by its specific position on a road, e.g., on the mainline or on a ramp. This is an intrinsic, time-invariant property of the sensor: hence, we propose to represent it using static sensor location embeddings. During end-to-end training, the embedding table collaboratively learns spatial relationships between arbitrary node pairs in a data-driven manner. Apart from capturing global correlations, this module can serve as a complement to the local proximity modeling function, which is particularly useful when the connections in A are noisy (Wu et al., 2019; Bai et al., 2020). A detailed case study is conducted in Section 5.5, where we show that the learned sensor embeddings contain significant information for sensor relevance reasoning. Compared with stacking multiple GNN layers to enlarge the receptive field and establish long-range correlations, our proposed module uses embedding similarity to reflect the relationships between arbitrary sensor pairs, and thus further considers global knowledge. Moreover, the connections between sensors are built in a direct way, which keeps our module from over-squashing (Alon & Yahav, 2021). As for implementation, we randomly initialize a low-dimensional embedding for each sensor, leading to an embedding table E ∈ R|V|×Dn , where Dn is the node embedding size. To fuse the embeddings with model hidden representations, we apply an MLP to map E to H ∈ R|V|×Dm . Complexity Comparison We provide a complexity comparison between the spatial learning modules here. As GCN (Kipf & Welling, 2017) is the widely adopted form of GNNs in spatio-temporal models, we take it as a baseline. Suppose a L-layer GCN with fixed hidden features Dm is applied. For STGNNs applying the predefined adjacency matrix, the time complexity isO(L|E|Dm+L|V|D2m). When applying the adaptive adjacency matrix that boosts performance, the complexity becomes O(L|V|2Dm + L|V|D2m). In SimST, the complexity of proximity modeling is O(|V|R), where R is the average degree of nodes. The complexity of correlation learning is O(|V|DnDm). It is obvious that SimST has lower complexity and is capable of supporting forecasting applications on large-scale sensor networks. 4.2. temporal encoder & predictor There are several viable options for building the temporal dependencies of a node’s history. To demonstrate that SimST can be generalized to various temporal models, we incorporate three basic and popular backbones in this study: GRU (Chung et al., 2014) (RNN-based), WaveNet (Oord
et al., 2016) (TCN-based), and Transformer (Vaswani et al., 2017). The representations HThGv ∈ R
Th×Dm are the input of the temporal encoder. For GRU and WaveNet, they compress the temporal dimension to 1, generating the output hGv ∈ RDm , while for Transformer, the resulting representation still has the same shape as HThGv . We only take the last time step as the final output. Moreover, since the standard Transformer is aware of all input time steps during self-attention and the traffic status at the current step is in fact not conditioned on its future, we follow WaveNet to introduce causality into the attention mechanism, leading to the design of the Causal Transformer. This ensures that the model cannot violate the temporal order of inputs and such causality can be easily implemented by masking the specific entries in the attention map. Finally, we concatenate the spatio-temporal summary hGv ∈ RDm of node v with its location embedding Hv ∈ RDm , which is then fed into the predictor (implemented as an MLP) for generating the forecasting results of node v: Ŷ Tf v = MLP(hGv ||Hv). 4.3. node-based batch sampling for training simst When training an STGNN, the input data are usually a fourdimensional tensor XTh ∈ RB×|V|×Th×F , where B denotes the set batch size. That said, nodes are strictly bound together and the batch sampling unit is the entire graph. However, directly applying this graph-based sampling to SimST leads to two issues. First, since SimST is a singlenode architecture, the actual batch size B∗ for SimST is B × |V|. For example, there are 883 nodes in the PeMSD7 dataset (Song et al., 2020) and usually B is set to 64, then B∗ = 56,512. Such a large number reduces the noise in the gradient estimation and leads to a degenerated generalization (Keskar et al., 2017). Also, when |V| is large, it can easily cause GPU out-of-memory problems. Second, we argue that the graph-based sampling method significantly diminishes sample diversity when forming mini-batches. For example, the traffic flow of all nodes from 8 to 9 a.m. on January 26 will always appear in the same batch. To tackle these issues, we propose to remove the restriction on nodes and change the batch sampling unit from a graph to a single node. Note that the same node at different time periods is viewed as different instances. Consequently, the input to SimST becomes XTh ∈ RB∗×Th×F , where B∗ B × |V|. The benefits are two-fold. First, our solution provides flexibility to choose B∗, which can be disproportionate to |V| and thus allows for smaller values like 256. This property enhances generalization (Keskar et al., 2017), and allows SimST to have low GPU memory costs and to easily scale to large graphs. Second, our approach provides more sample diversity when forming mini-batches, which yields a generalization improvement (Ash et al., 2020). 5. experiments  5.1. experimental setup Datasets We conduct experiments on commonly used traffic benchmarks. The details are presented in Table 2. All traffic readings are aggregated into 5-minute windows, resulting in 288 data points per day. Note that the values in traffic flow datasets are generally much larger than those in speed-related datasets. Following (Li et al., 2018; Song et al., 2020; Choi et al., 2022), we use the 12-step historical data to predict the next 12 steps. Z-score normalization is applied to the input data for fast training. We build the adjacency matrix of each dataset by using road network distances with a thresholded Gaussian kernel (Shuman et al., 2013). The threshold r is set to 0 for PeMSD4, PeMSD7, PeMSD8, and 0.1 for LA and BAY. More information is provided in Appendix A.
Baselines We compare SimST with the following baselines. Historical Average (HA) (Pan et al., 2012) and Vector Autoregression (VAR) (Toda, 1991) are traditional methods. DCRNN (Li et al., 2018), STGCN (Yu et al., 2018) , ASTGCN (Guo et al., 2019), GWNET (Wu et al., 2019), STSGCN (Song et al., 2020), and AGCRN (Bai et al., 2020) are well-known STGNNs. We also incorporate methods that are published recently: STGODE (Fang et al., 2021), STGNCDE (Choi et al., 2022), GMSDR (Liu et al., 2022a). Implementation Details SimST is implemented with PyTorch 1.12. There are three variants of SimST based on the applied temporal encoders. We describe the specific configurations for different variants in Appendix B. The following settings are the same for all variants. We train our method via the Adam optimizer with an initial learning rate of 0.001 and a weight decay of 0.0001. We set the maximum epochs to 150 and use an early stop strategy with a patience
of 20. The batch size is 1,024. The embedding size Dn is 20. The top-k nearest neighbors are set to 3 for LA and BAY, and 0 for PeMSD4, PeMSD7, and PeMSD8 (due to limited available edges). For baselines, we run their codes based on the recommended configurations if their accuracy is not known for a dataset. If known, we use their officially reported accuracy. Experiments are repeated five times with different seeds on an NVIDIA RTX A6000 GPU. Evaluation Metrics We adopt three common metrics in forecasting tasks to evaluate the model performance, including mean absolute error (MAE), root mean squared error (RMSE), and mean absolute percentage error (MAPE). For efficiency measurement, the commonly used floating point operations per second (FLOPS) cannot reflect the “real” running speed of methods because it ignores the effects of parallelization. Hence, we apply the metric of throughput per second (TPS) in this study, which indicates the average number of samples the network can process in one second. We provide a wall-clock time comparison in Appendix D. 5.2. performance comparison In this section, we conduct a model comparison between SimST variants and state-of-the-art STGNNs on five realworld traffic benchmarks. The three variants of SimST are SimST-GRU, SimST-WaveNet (WN), and SimST-Causal Transformer (CT). According to the empirical results in
Table 1, all variants of SimST achieve competitive performance on the three evaluation metrics of all datasets, which validates the effectiveness of SimST and indicates SimST can generalize to various architectures of temporal models. For comparison among SimST variants: note that for fairness we ensure that the variants have a similar number of parameters (around 150k). SimST-GRU generally achieves comparable performance to state-of-the-art baselines, i.e., GWNET and STGNCDE. SimST-CT also attains competitive accuracy, with little difference from SimST-GRU and GWNET. We notice that while Transformers are generally considered a more powerful model, our results show that a simple GRU works well for traffic prediction. In summary, our results provide strong support for our claim that GNNs are not the only option for spatial modeling in traffic forecasting. Moreover, considering the very different design of SimST compared to other state-of-the-art methods, we hope that this will inspire follow-on studies in graph-less designs. Additionally, we find that: (1) STGNNs approaches surpass HA and VAR by a large margin due to their greater learning capacity. (2) The capability of methods that apply the adaptive matrix, such as GWNET and AGCRN, has been overlooked. They have achieved similar performance to the recently proposed approaches. 5.3. efficiency comparison In this part, we select the top-performing approaches and compare their efficiency with SimST variants. It can be seen from Table 3 that all SimST variants are significantly more efficient than baselines, thanks to their simple model architecture and linear time complexity w.r.t. |V|. Specifically, comparing TCN-based models, SimST-WN has around 45% fewer parameters than GWNET but achieves 3.3× – 5.6× higher TPS. For RNN-based methods, SimST-GRU is 3.7× – 6.1× and 19× – 26× faster than AGCRN and GMSDR, respectively. STGODE and STGNCDE are the recently published methods that are based on neural ordinary differential equations. Though achieving competitive performance, they suffer from significant inefficiency problems. For example,
SimST-WN has comparable performance to them but is 11× – 17× and 30× – 39× faster. Among the variants, SimSTWN achieves the highest TPS due to WaveNet’s superior parallelism capability. SimST-GRU also achieves good TPS results, which we attribute to an optimized implementation in the PyTorch library. See Appendix D for more results on efficiency comparison. 5.4. ablation study We have demonstrated the effectiveness and efficiency of SimST. Next, we select SimST-GRU and SimST-CT as the representatives (due to their preferable performance) and conduct a series of ablation and case studies on one flowbased dataset PeMSD4, and one speed-related dataset LA. 5.4.1. effects of spatial learning modules To study the effects of two spatial modules, we consider the following three settings for comparisons: (1) w/o Correlation Learning (CL): we turn off the spatial correlation learning module. (2) w/o Proximity Modeling (PM): for each node, we only input the histories of itself. (3) CT/GRU: we do not perform spatial learning. The results
are shown in Figure 2. First, we find that removing CL leads to significant degradation of MAE for both SimST-GRU and SimST-CT on both datasets, revealing the great importance of building global correlations under the SimST framework. Second, the benefit of PM is marginal on PeMSD4. This is because the average degree of nodes in PeMSD4 is only 1.1, so the available neighbor information is very limited. In contrast, the average degree of nodes in the LA dataset is 7.3, where PM affects performance more. Third, we notice that removing the CL module has a greater impact on the PeMSD4 dataset than on the LA dataset. The reason is that when neighbor information is scarce, the CL module takes a greater responsibility to supplement neighbor knowledge. Conversely, when there are more neighbors, the influence of CL on performance is less significant. 5.4.2. effects of node-based batch sampling We examine the effects of the node-based sampling method from two aspects. First, we show the influence of the batch size number in the first row of Figure 3. Generally, both SimST-GRU and SimST-CT reach their best performance when setting B∗ to 1,024 on the two datasets, revealing the necessity of applying a small B∗. The negative influence of
using a large batch size such as 19,648, is more significant on the PeMSD4 dataset. Besides, we find that SimST-CT only occupies around 2GB of memory on our GPU across all datasets. In contrast, the memory cost of STGNNs usually grows as |V| increases. For example, on the dataset with the lowest |V| = 170 and the highest |V| = 883, GWNET requires around 3GB and 11GB of memory, respectively. Second, we present the effects of different batch-forming approaches, i.e., graph-based sampling and node-based sampling by drawing the convergence curves in the second and third rows of Figure 3. Note that we ensure fair comparisons by setting the same B∗ in experiments. In Figure 3, it can be seen that node-based batch sampling surpasses the counterpart. Concretely, node-based sampling is only slightly better than the graph-based method when a large B∗ is applied (e.g., 19,648/13,248). This is because the sample diversity of graph-based sampling is rich enough at this time. As B∗ decreases to small values (e.g., 1,228/828), which is necessary to improve performance, the impact of sample diversity becomes significant, i.e., generalization performance is greatly improved, especially on PeMSD4. Also, we find that the curves of node-based sampling are generally more stable than the graph-based method. 5.4.3. hyperparameter study In Figure 4, first row, we show the effects of the number of top-k one-hop neighbors. We replace PeMSD4 with the BAY dataset because the average degree of nodes is only 1.1 in PeMSD4. From the figure, we observe that the performance gradually improves when k increases from 0 to 3. But the performance worsens when we set k to 4, indicating a potential overfitting issue. Next, we assess the effects of the sensor embedding dimension size in the second row of Figure 4. We find that both SimST-GRU and SimST-
CT are not sensitive to the changes in the dimension on both datasets. The results also suggest that a small dimension such as 16, is sufficient to learn correlations between nodes. 5.5. case study In this section, we explore further to understand what exactly the sensor embeddings have learned through a case study that makes connections between embedding similarities and real-world sensor locations. We apply SimSTCT on LA and BAY since only these two datasets provide sensors’ coordinates. Concretely, we first compute pairwise cosine similarities between sensor embeddings, i.e., for vi, vj ∈ V, similarity =
Evi ·Evj ‖Evi‖2‖Evj‖2 , and pairwise
geodesic distances between sensors based on their coordinates. Then we calculate the average cosine similarities within the ranges of 0–1, 1–5, 5–10, 10–20, and 20–35 kilometers of all sensors, leading to a statistical overview shown in Figure 5 (left part). It can be seen that cosine similarity becomes smaller when the geodesic distance gets larger. This result obeys Tobler’s First Law of Geography, i.e., near things are more related than distant things. Next, we look at an example by selecting sensor 62 in LA as the anchor node, which is located at the intersection of two highways. We also visualize its top similar neighbors based on cosine similarity in Figure 5 (upper right). We find that (1) sensors 13 and 58 are the nearby neighbors, which are also included in the adjacency matrix. (2) sensors 201 and 178 are distant nodes. They are considered similar sensors because they are also located at intersections, and thus share similar traffic patterns to the anchor. The situation in the
BAY dataset is similar to LA (shown in the lower right of Figure 5), so we do not elaborate further. 5.6. discussion & future work We find that SimST-GRU and SimST-CT perform worse than the state-of-the-art STGNNs (i.e., STGNCDE and GWNET) when we turn off the node-based batch sampling strategy. This result indicates that the proposed two spatial learning modules can only approximate the efficacies of GNNs, and the training strategy of SimST is an indispensable component to obtain comparable performance to the state-of-the-art. In addition, we note that when the neighbor information is relatively rich, such as with LA, SimST-CT is slightly below the state-of-the-art. 6. conclusion In this paper, we propose SimST, a simple, effective, and efficient spatio-temporal learning method for traffic forecasting. It approximates GNNs with two proposed spatial learning modules, involves a node-based batch sampling strategy, and is temporal-model-agnostic. Our study suggests that message passing is not the only effective way of modeling spatial relations in traffic forecasting; hence, we hope to spur the development of new model designs with both high efficiency and effectiveness in the community. a. datasets We conduct experiments on the traffic datasets of PeMSD41, PeMSD71, PeMSD81, LA2 and BAY2. • PEMS-04 (Song et al., 2020): The dataset refers to the traffic flow in San Francisco Bay Area. There are 307 sensors and the period ranges from Jan. 1 - Feb. 28, 2018. • PEMS-07 (Song et al., 2020): This dataset involves traffic flow readings collected from 883 sensors, and the time range is from May 1 - Aug. 6, 2017. • PEMS-08 (Song et al., 2020): The dataset contains traffic flow information collected from 170 sensors in the San Bernardino area from Jul. 1 - Aug. 31, 2016. • LA (Li et al., 2018): The dataset records the traffic speed collected from 207 loop detectors in the highway of Los Angeles County, ranging from Mar. 1 - Jun. 27, 2012. • BAY (Li et al., 2018): This dataset contains traffic speed information from 325 sensors in the Bay Area. It has 6 months of data ranging from Jan. 1 - Jun. 30, 2017. The datasets are split into three parts for training, validation, and testing with a ratio of 6:2:2 on PeMSD4, PeMSD7, and PeMSD8, and 7:1:2 on LA and BAY. The statistics of data partition are in Table 4. b. configuration of simst variants  c. additional performance comparison In Figure 6, we provide a visual comparison of throughput, accuracy, and parameter size between top-performing STGNNs and SimST. It can be seen that SimST variants are substantially more efficient and use much fewer parameters than state-of-the-art STGNNs, while achieving comparable performance. Additionally, we notice that adding an auxiliary feature – day of week can significantly improve the predictive accuracy, as shown in Table 6. The reason is that there is a significant difference in traffic patterns between weekdays and weekends. Therefore, this feature is a powerful prior knowledge for the model, and future research may consider incorporating this feature to enhance performance. d. wall-clock time comparison In Table 7, we provide the training and inference time per epoch of top-performing baselines and SimST variants, to enable straightforward efficiency comparison. We observe that SimST variants have significantly shorter inference time, thanks to their linear complexity w.r.t the number of nodes and small parameter size. As for training efficiency, although SimST-GRU applies a small actual batch size B∗ = 1, 024, which is necessary to achieve better predictive performance, its training time is still comparable to the fastest two baselines GWNET and AGCRN. Furthermore, we notice that methods based on ordinary differential equations (STGODE and STGNCDE) require significant training and inference time, which largely affects their practical applications in the real world. Moreover, since we need to perform ego-graph construction for the proximity modeling module, we provide details of the time spent here based on our current implementation: 41.9 seconds on PeMSD4, 206.5 seconds on PeMSD7, 28.8 seconds on PeMSD8, 284.1 seconds on LA, and 716.1 seconds on BAY. Note that this construction process only need to run once, because such process is deterministic and we can store the processed data and load them during training and inference. e. learning curve comparison In this section, we first scale up the parameter size of SimSTGRU from 130k to 340k (denoted as SimST-GRU-L), to ensure our method has comparable parameters to the baselines. Then in Figure 7, we compare the learning curves between top-performing baselines and SimST-GRU-L. It can be seen that SimST-GRU-L exhibits very fast convergence rate: it drops to a small validation MAE within a few epochs, and almost converges in around 20 epochs. In addition, we observe that the standard deviation of SimST is generally smaller than those in baselines, indicating the stability provided by our approach.