Natural language generation (NLG) is one of the most impactful fields in NLP, and recent years have witnessed its evolution brought about by large language models (LLMs). As the key instrument for writing assistance applications, they are generally prone to replicating or extending offensive content provided in the input. In low-resource data regime, they can also lead to repetitive outputs [1]. Usually, offensive content and repetitions are mitigated with post-hoc methods, including n-gram level blocklists, top-k and nucleus sampling. In this paper, we apply non-exact repetition suppression using token and sequence level unlikelihood loss, and further explore the framework of unlikelihood training objective in order to jointly endow the model with abilities to avoid generating offensive words and phrases from the beginning. Finally, with comprehensive experiments, we demonstrate that our proposed methods work exceptionally in controlling the repetition and content quality of LLM outputs. 1. introduction Over the years, large language models have become more impactful as they are being applied to an increasing range of features and products [2, 3]. LLMs have been widely employed in various writing assistance applications, aiding users in generating high-quality, human-like text. However, despite their immense potential, LLMs often struggle with certain limitations that prevent them from further mimicking actual human-written content. Specifically, generations from LLMs sometimes contain repetitions [1] and controversial phrases (political topics, racial topics, etc.). Repetition refers to the tendency of LLMs to produce sentences or phrases that are either identical or very similar to each other within the generated text. This can lead to a lack of diversity in the output and cause the generated content to appear unnatural, monotonous, or even nonsensical. The problem of repetition can be particularly severe in low-resource data regimes where the model has limited training data to learn from, leading to a higher likelihood of generating repetitive content. Another significant challenge associated with LLMs is the generation of controversial or offensive phrases. LLMs are trained on vast amounts of text data from the internet, and as a result, they may inadvertently learn to generate content that is politically biased, racially insensitive, or otherwise offensive. This can pose a significant risk for applications that rely on LLMs for content generation, as it may lead to the dissemination of harmful or inappropriate content. Currently, mainstream solutions for both problems are posthoc, rule-based methods, including n-gram level blocklists, top-k and nucleus sampling [1], which are suboptimal. Not only are they not effective enough - words may go through post-filters if they are not exact matches, but they also degrade the output by breaking its coherence. To overcome these limitations, in this paper we first review the effectiveness of repetition removal method based on sentence embedding, and then we adopt token and sequence level unlikelihood training objective [4] as the non-exact repetition suppression solution. Furthermore, we extend the unlikelihood training objective and customize it to jointly address a broader range of problems. Finally, we demonstrate through experiments that our approach Pareto-dominates the baseline method and can help suppress repetition and moderate content with minimum impact to model performance. 2. related work Various techniques have been proposed to tackle the repetition problem in language models. Some of these methods include modifying the decoding algorithm, such as using nucleus sampling [1], top-k sampling [5] and diverse beam
search [6]. Another approach to suppress repetition is to penalize repeated n-grams during the decoding process. Holtzman et al. [1] acknowledge the problem of repetition and propose a new decoding strategy to mitigate it. Welleck et al. [4] try to tackle this problem by customizing training objectives. Fu et al. [7] provide a theoretical framework to systematically analyze the root cause of repetition and the reason behind existing techniques. Content moderation in LLMs aims to prevent the generation of offensive or controversial content. Several methods have been proposed to mitigate this issue, including the use of blocklists, adversarial training, reinforcement learning from human feedback [8], or rule-based filters. In practice, postprocessing filters like blocklists and offensiveness classifiers are still most economic and prevalent. 3. non-exact repetition suppression Since its inception, BERT [9] has been used as a tool for encoding sentences and generating their representations. However, BERT isn’t pre-trained to be natively support sentence similarity detection. SentenceBERT [10], being a solution to this issue, makes structural changes on top of BERT for training and inference respectively with cosine-similarity in mind. In practice, we calculate the similarity score for any two sentences in a paragraph generated by the LLM, and only keep one sentence in any pair with a score above a predetermined threshold. This SentenceBERT-based method is a great representative of repetition detection & removal approaches. Figure 1 contains the architectures of SentenceBERT during training (left) and inference (right). As we discussed above in the introduction, repetition prevention is a more ideal way to address the issue than post-hoc repetition removal. We adopt the unlikelihood (UL) training objective in Welleck et al. [4]. Specifically, ordinary likelihood training objective minimizes the loss for ground truth tokens:
LtLikelihood(pθ(·|x<t)) = − log pθ(xt|x<t) (1)
Where pθ denotes the language model with parameters θ, and t is the current time step. In unlikelihood training objective, higher loss is applied to a set of tokens, called negative candidates, and thus discourages the model from generating them. Token-level UL objective defines negative candidates CtUL−token as previous tokens within the target (ground truth). LtUnlikelihood(pθ(·|x<t), Ct) = − α ∑ c∈Ct log(1− pθ(c|x<t))− log pθ(xt|x<t) (2)
Ct = CtUL−token = {x1, . . . , xt−1} \ {xt} (3)
Coefficient α controls the proportion of the unlikelihood loss term. Sequence-level UL objective follows the same principle, except that it defines negative candidates as previous duplicate n-grams within the predicted sequence. Ct = CtUL−seq = {xt} if (xt−i, . . . , xt+j) ∈ x<t−i for any i+ j + 1 = n (4) 4. content moderation The unlikelihood training objectives, with both token-level UL loss and sequence-level UL loss, are not standalone loss functions. In fact, they propose a platform with the concept of negative candidates, and having repetition-related context can be seen as a special case. Based on this idea, we further generalize the scope of negative candidates and define a new loss term called “block loss”. In block loss, the negative candidates are defined as blocklist phrases Bn that we have collected in advance. Ct = Ctblock = 10⋃ n=2 Ctblock−n (5)
Ctblock−n = {xt} if (xt−i, . . . , xt+j) ∈ Bn for any i+ j + 1 = n
(6) 5. experiments First, we evaluate the performance of SentenceBERT-based post-processing methods. Then, we compare different aspects of unlikelihood training objectives. Lastly, we show the efficacy of our novel block loss in the setting of sequencelevel UL training objective. 5.1. experimental setup We use a sequence-to-sequence text rewriting task to evaluate the approaches discussed in the sections above, with bullet points as inputs and paragraphs as outputs. In practice, this task could translate to writing assistant products or
features capable of generating fluent documents based on user-provided outlines. We use pre-trained BART model [11] as the common starting point and apply different training objectives during finetuning. The baseline model is fine-tuned until convergence with regular likelihood loss. Another model is fine-tuned from the starting point with token-level UL objective. The last model is fine-tuned with sequence-level UL objective on top of the previous model. Following Welleck et al. [4], sequence-level UL objective is applied 50% of the time during fine-tuning, with the other 50% of the time using model’s original token-level training objective. Inputs are encoded by SentencePiece tokenizer [12]. Teacher forcing [13] is applied during training to ensure a faster and more stable convergence. We use beam search as the decoding strategy with beam size 5 across all experiments. All training is done on an Azure ML Compute instance with 4 Nvidia V100 16GB GPUs. 5.2. data Our data is in a simple format, as each data sample consists of one paragraph as ground truth and its corresponding list of bullet points in pure text format. The bullet is denoted by an asterisk (∗), and the sub-bullet is denoted by double asterisk (∗∗). The size of the training set, validation set, and test set are 45k, 6.6k, 1.4k respectively. Bullet points are summarized by human annotators from paragraphs in public Word documents corpus, and unqualified samples (too long, offensive, etc.) are removed in advance. Below is one example sample of the dataset:
{"bullet_points": "Evans, CMO of Subaru of America, Inc. (SOA) * 20+yrs experience * oversaw demand generation, brand awareness * customer engagement programs * SOA revenue increase ** 40% to 65%of parent company",
"paragraph": "Evans boasts over 20 years of experience in the automotive industry, most recently as CMO of Subaru of America, Inc. (SOA). In that position, he oversaw demand generation, brand awareness and customer engagement programs that directly contributed to the company’s highest years of sales growth in its history. SOA’s revenue increased from 40% to 65% of parent company Fuji Heavy Industries’ (FHI) overall revenue during Evans’ tenure, with FHI’s stock the best performer on the Nikkei."} For the evaluation of sequence-level training objective with block loss, we use a subset of the original dataset, with 50% samples containing blocklist phrases & 50% regular samples, in order to better demonstrate the results and save computational cost. 5.3. evaluation metrics We record an extensive range of metrics during evaluation in four categories: language model metrics, repetition metrics, summarization/translation-based metrics, and grammatical metrics. Below is a list of itemized metric descriptions. Language model metrics measure the basic capabilities of language models, namely how well they can predict the next token. • Perplexity (ppl)
• Next-token prediction accuracy (acc)
Repetition metrics measure the duplicativeness and uniqueness of generation on both token level and sequence level. • Repetition (rep-l): the fraction of next-token prediction that occur in previous l tokens
• Wrong repetition (wrep-l): similar to rep-l, but only counts token repeats that are not equal to gold next token
• Portion of duplicate 1-grams (seq-rep-1)
• Portion of duplicate 4-grams (seq-rep-4)
• Number of unique next-token predictions (uniq)
• Number of unique tokens in the generated paragraph (uniq-seq)
Summarization/translation-based metrics measure the semantic consistency between model’s input and output. • ROUGE [14]: similarity between prediction & ground truth measured with overlap n-grams & longest common subsequence
• BERTScore [15]: similarity between prediction & ground truth measured with BERT representations
• BLEURT [16]: a transfer learning-based NLG metric
Grammatical metrics measure the prevalence of grammar mistakes in model outputs, and thus monitor the impact on grammatical performance. • Percentage of sentences with grammar mistakes (percgrammar-wrong)
• Number of grammar mistakes per sentence (countgrammar-wrong) 5.4. results In table 1, SentenceBERT, as a post-hoc method, is Pareto dominated by unlikelihood objectives regardless of former’s threshold choice, which justifies our transition from post processing to custom training objective. In table 2, tokenlevel UL objective significantly reduces token repetition, while additional training with sequence-level UL objective further reduces repetitive n-grams by a huge margin. Overall, unlikelihood training objectives help suppress repetition with minimum impact to performance. Table 5 lists a set of generations from different models given the same outline. Baseline output suffers from severe repetition problem whereas the outputs of models trained
with unlikelihood objectives do not, and thus provide a better text semantically and syntactically. Outline contains additional line breaks and indentations for better illustration. Ground truth (the original paragraph) is also attached for reference. In terms of training time, as shown in table 3, token-level unlikelihood objective has a similar result as regular likelihood objective, which is expected since the former can reduce to the latter when α = 0 in equation (2). In contrast, sequencelevel UL objectives are much more time-consuming. This order-of-magnitude difference between two levels can be explained by the iterative n-gram checks during the identification of sequence-level negative candidates. In table 4, sequence-level UL objective with our novel block loss reduces the number of outputs containing unwanted blocklist phrases. Similarly, a set of generations from different models as well as corresponding ground truth are listed in table 6. The word ”abortion” is in the blocklist and successfully avoided in the output of the model fine-tuned with sequence-level UL objective with block loss. 6. conclusions & future work In this paper, we explored a joint effort of non-exact repetition suppression and content moderation to address the limitations of LLMs in generating repetitive and offensive content. We first set up SentenceBERT as a baseline of repetition detection & post-processing methods. Then, we utilized multiple levels of unlikelihood training objectives to suppress repetition at the step of generation. Finally, we exploited and further generalized the unlikelihood training objective and brought it into the field of content moderation. We demonstrated that our proposed methods work exceptionally well in controlling the repetition and content quality of LLM outputs. The results showed that multi-level
unlikelihood training objectives and our novel block loss greatly reduce token repetition, repetitive n-grams and offensive content while maintaining minimum impact on model performance.