Pretrained Language Models (PLMs) have emerged as the state-of-the-art paradigm for code search tasks. The paradigm involves pretraining the model on search-irrelevant tasks such as masked language modeling, followed by the finetuning stage, which focuses on the search-relevant task. The typical finetuning method is to employ a dual-encoder architecture to encode semantic embeddings of query and code separately, and then calculate their similarity based on the embeddings. However, the typical dual-encoder architecture falls short in modeling token-level interactions between query and code, which limits the model’s capabilities. In this paper, we propose a novel approach to address this limitation, introducing a crossencoder architecture for code search that jointly encodes the semantic matching of query and code. We further introduce a Retriever-Ranker (RR) framework that cascades the dualencoder and cross-encoder to promote the efficiency of evaluation and online serving. Moreover, we present a probabilistic hard negative sampling method to improve the cross-encoder’s ability to distinguish hard negative codes, which further enhances the cascade RR framework. Experiments on four datasets using three code PLMs demonstrate the superiority of our proposed method. ii. related work In this section, we briefly review the related work from three aspects, including code search, pretrained language models for software engineering, and finetuning PLMs for code search. a. code search Code search aims to find relevant codes for given queries [14], [15]. The core of code search is to predict whether queries are relevant to codes [1], [16]. To determine the relevance score, traditional information retrieval (IR) methods mainly rely on rule-based code search, such as term matching (e.g., term frequency-inverse document frequency, TF-IDF), and manually-designed features based on the analysis of the abstract syntax tree of codes [4]–[6]. However, these methods lack the ability to understand semantic information and have some limitations: (1) The term mismatch problem limits the ability to identify potentially useful codes; (2) Manually-designed features are inadequate for capturing complex and implicit features. To overcome the limitations of IR-based methods, deep learning has been applied to the code search task to learn semantic representations of queries and codes [15]. The relevance score is typically calculated using a factorized query embedding and code embedding [2]. The query embedding is generated using natural language processing (NLP) models such as fastText [15], LSTM [1], or GRU [2]. Various methods have been employed to calculate the code embedding, including LSTM [17], MLP [1], and Graph Neural Network [3]. Code can be represented by various data structures through code static analysis [5], such as Abstract Syntax Tree (AST) [3], variable and function name [1]. To encode these different structures, a variety of models have been proposed [17]. While these techniques reduce the need for human-designed rules, they still require human effort to create effective data structures and train the models on them. b. pretrained language models (plms) a) PLMs for NLP: Recent years have seen significant success in the field of natural language processing (NLP) thanks to pre-trained language models (PLMs), which have become the primary paradigm in this area [18]. These models are transformer-based and trained with self-supervised learning using a large set of unlabeled data. The most commonly used pre-training task involves the “mask then predict” strategy, where tokens in a sequence are masked and predicted based on the surrounding tokens. For instance, BERT [18] masks tokens randomly and predicts them based on the left tokens, while GPT [19] masks the latter tokens in a sequence and predicts them based on former tokens. Pre-trained models can then be fine-tuned on many downstream tasks such as sentiment classification and summarization to transfer PLM knowledge, leading to state-of-the-art performance in these tasks. b) PLMs for Software Engineering: Motivated by the huge succees of pretrained language models (PLMs) in the NLP field, many researchers introduce PLMs to software engineering field [10], [11], [20]–[22]. Various code PLMs have been trained from different perspectives, with some based on language modeling [8], [23], and others based on coderelevant tasks such as link prediction on abstract syntax tree (AST) and data flow graph (DFG) [7], [11], or contrastive learning with regard to multi-modals about code [9]. Following pretraining, code PLMs can be finetuned on code downstream tasks, such as code search and code generation, significantly outperforming previous models. While much of the research on code PLMs focuses on designing different pretraining tasks to improve their general ability, little attention has been given to improving the finetuning stage for specific downstream tasks. This suggests that the potential of PLMs for downstream tasks has not been fully exploited. In this paper, we focus on improving the finetuning stage of PLMs for the code search task, exploring ways to make the models more suitable for this task. c. finetuning plms for code search When using pre-trained language models (PLMs) for code search, the most commonly employed approach is the dualencoder architecture [7], [9], [21]. In this method, the PLM serves as an encoder to extract query and code embeddings, which are then used to compute the relevance score. Notably, the dual-encoder architecture for fine-tuning code PLMs is similar to prior deep learning approaches for training models on code search tasks [1]. The key difference lies in the choice of encoder. While previous work utilized encoders such as LSTM and GNN, the PLM based models relies on the pretrained Transformer encoder. To optimize the model, various loss functions have been adopted for the code search task. One approach is to treat the task as a binary classification problem, where the objective is to determine whether a query is relevant to a code or not. In this case, a binary classification loss can be utilized to optimize the code search model, as proposed in [24]. However, the code search task requires ranking all codes
according to their relevance scores with a query, is actually a ranking task. Therefore, the binary classification loss is not entirely consistent with the ranking setting. To address this, some previous works have proposed using the pairwise loss to optimize the code search model. This approach involves maximizing the relevance score margin between a query and its relevant code compared to its irrelevant code [1], [3], [17], [25]. Furthermore, other studies have proposed optimizing the model by maximizing the relevance scores of a query with its relevant code and simultaneously minimizing the relevance scores of this query with many irrelevant codes [7], [20], [26]. iii. method In this section, we present a retriever and ranker framework with a probabilistic hard negative sampling method for code search, which is a flexible and universal patch and can be applied to many code PLMs to improve the performance of previous work for the code search task. We begin by introducing the code search task and the dual-encoder architecture which is typically used for finetune PLMs for code search. We then introduce the cross-encoder architecture and the retrieval and ranker framework for the code search task. We then delve into the details of the probabilistic hard negative sampling method, which is used to train the cross-encoder within the RR framework. Finally, we provide an in-depth analysis of RR/R2PS, highlighting its superior model capabilities and complexity. a. task formulation and dual-encoder Assuming we have a large code corpus containing various code snippets ci, where i = 1, 2, · · · , N , each implementing a specific function. Given a user query q, described in natural language, the objective of code search is to quickly identify and present a small amount of relevant codes to the user based on relevance scores (also known as similarities). The core of code search lies in two perspectives: (1) Precision: precisely estimate the relevance scores of queries and codes; (2) Efficiency: rapidly estimate the relevance scores of the query and all codes in the code corpus. The definitions for used notations in this paper are shown in Table I. As depicted in Figure 1(a), the dual-encoder architecture is commonly used to predict relevance scores for PLM-based code search. Firstly, the query and code are tokenized to token sequence. Then, the PLM encoder maps the query and code token sequence to query embedding and code embedding, respectively, Finally, the relevance score of the query and code is calculate by the dot product of the two embeddings. The dual-encoder architecture can be formulated as:
sdual(q, c) =< E(q), E(c) >, (1)
where E() denotes the PLM encoder, <,> denotes the dot product operation, q and c denote the token sequence of query and code, respectively. The PLM is essentially a transformerbased model, with self-attention of all tokens in the sequence to learn interactions among the tokens [27]. b. cross-encoder and rr framework a) Motivation: Trasnformer-based models [28] are powerful thanks to their self-attention mechanism that enables them to model full token interactions within a sequence. In the Transformer architecture, self-attention can be formulated as follows:
H ′ = softmax( QKT√ d )V , (2)
where Q = HW1, K = HW2, V = HW3 are linearly projected by the hidden representation H , H denotes the representation matrix of all tokens in the sequence, with each row corresponding to a specific token, H ′ denotes the representation matrix after self-attention operation, √ d denotes the scaling factor, softmax(QK T
√ d ) is attention matrix of Transformer. From element perspective, the i-th token in the sequence can be formualted as:
h ′
i = l∑ j=1 st(< qi,kj >)vj , (3)
where h ′
i, qi, kj , vj denote the representation vectors of i/jth token in the corresponding matrix, < qi,kj > denotes dotproduct of vector qi and kj which can be regarded as the interaction of i-th and j-th tokens, st() denotes the scaling operation and softmax operation about j = 1, 2, · · · , l. Thus, the embedding of each token, h ′
i, is obtained by modeling its interactions with all tokens in the sequence (< qi,kj >, j = 1, 2, · · · , l) and fusing all tokens information according to the interactions. Therefore, the dual-encoder for code search can model the token-level interactions within both query tokens and code tokens with Transformer-based encoder E(). Specifically, E(q) models all interactions in the query token sequence q = (sq1, s q 2, · · · , s q l ), including all pairs (s q i , s q j), where i, j = 1, 2, · · · , l; Similarly, E(c) models all interactions in the code token sequence c = (sc1, s c 2, · · · , scm), including all pairs (sci , s c j), where i, j = 1, 2, · · · ,m. However, the model
cannot directly model token-level interactions between query and code tokens (sqi , s c j) where i = 1, 2, · · · , l and j = 1, 2, · · · ,m, This limitation restricts the model’s capability and effectiveness in code search. b) Cross-encoder: To improve the model capability of PLMs for code search, we propose using a cross-encoder to model the token-level interactions between query and code. Specifically, we concatenate the query token sequence and code token sequence to a unified sequence, then use the PLM encoder to map the unified sequence into an embedding, and finally use a neural network head to map the embedding to a scalar that represents the relevance score of the query and code. The cross-encoder architecture can be represented as:
scross(q, c) = NN(E([q, c])), (4)
where [q, c] = (sq1, s q 2, · · · , s q l , s c 1, s c 2, · · · , scm) denotes the concatenation of query and code, E() denotes the PLM encoder, and NN() denotes the neural network head. In this way, the query tokens and code tokens are inputed into the Transformer model together, and thus all token-level interactions can be effectively modeled, including not only interactions within query (sqi , s q j) and code (s c i , s c j), but also those between query and code (sqi , s c j), (s c j , s q i ). As a result of its ability to learn the cross-interactions between query and code, we refer to this model as the cross-encoder framework. Compared to the dual-encoder, the model capability of the cross-encoder is stronger, making it more precise for code search. c) Retriever and Ranker Framework: During the inference stage, the cross-encoder must encode all possible concatenations of each query and all codes in the codebase using the PLM encoder. This enables the cross-encoder to retrieve the relevant code snippets based on their respective relevance scores. However, given that the codebase is typically vast and comprises millions of codes, this process demands a significant amount of computing resources. As a result, the cross-encoder can be slow to provide results when serving online, which can impact its efficiency. To make the cross-encoder practical for code search, we introduce a Retriever and Ranker framework (RR) for code search, as illustrated in Figure 2. The RR framework comprises two cascaded modules: (1) A dual-encoder module is employed as the retriever to identify k codes with the highest relevance scores for a given user query; (2) A cross-encoder module is used as the ranker to rank the k codes further. We explain the rationality behind the efficiency of the RR framework for code search as follows: In the dual-encoder model, we can compute the code embeddings of all the codes in the codebase during pre-processing and store them as E(ci), where i = 1, 2, · · · , N . Therefore, during evaluation and online serving, the retriever only needs to compute the embedding of the given query, E(q), and match it with the pre-calculated code embeddings to calculate the relevance scores. The cross-encoder model then determines the relevance scores between the query and the retrieved k codes, and ranks them accordingly. As a result, the online encoding time for
a query only involves one forward propagation of the dualencoder for the query and k forward propagations of the crossencoder for the concatenations of the query and k codes. This computational process is independent of the number of codes in the codebase. By increasing k, the dual-encoder finds more relevant codes for the cross-encoder to rank, which results in better performance. However, with a smaller value of k, the cross-encoder performs fewer propagations, which reduces the computing cost. Thus, choosing the appropriate value of k can balance the performance and computing cost. c. training method The cascaded RR framework consists of two modules that perform distinct functions. The dual-encoder module retrieves data from the codebase, which contains all codes, and its ability to predict the relevance scores of all codes is crucial. On the other hand, the cross-encoder module ranks a small number of retrieved codes that have high relevance scores, as determined by the dual-encoder. Thus, its ability to predict the relevance scores of these potentially relevant codes is of utmost importance. Therefore, it is essential to design different training goals for the two modules to optimize their performance. The goal of training a code search model is to learn a relevance estimation function that assigns higher scores to relevant codes for queries compared to those that are irrelevant. To achieve this, we sample some irrelevant codes as negative samples and aim to maximize the relevance scores of the relevant codes while minimizing the relevance scores of the sampled codes. Our method employs the InfoNCE loss [29], [30] as loss function, which can be formulated as:
Lq = −log es(q,c
+)/τ es(q,c+)/τ + ∑m i=1 e s(q,c−i )/τ , (5)
where q is a query, c+ is a relevant code of the query q, and c−i is the set of sampled negative codes, τ denotes the temperature hyper-parameter, s(q, c) denotes the relevance score of the query-code pair estimated by the PLM encoders. Minimizing this loss will result in increasing the relevance score of the relevant query-code pair s(q, c+) and decreasing the relevance scores of the query with irrelevant codes s(q, c−i ). As mentioned before, it is crucial for the dual-encoder to predict the relevance scores of queries with all codes in the codebase. Therefore, our primary objective is to train the model to enhance its ability to retrieve potentially relevant
codes from the entire codebase. To achieve this, we utilize a method of negative sampling wherein we randomly select codes from the entire codebase as negative samples to train the model. In practice, we employ the in-batch negative sampling technique, which treats the codes in the sampled batch as negative samples. Specifically, given a batch data {(qj , cj)}bj=1, where code cj is relevant to query qj , all other codes ci, where i 6= j are deemed as negative samples of query qj . Since the batch data is randomly sampled from the training set, we can consider other codes in the batch as randomly sampled from the training set. On the other side, it is crucial for the cross-encoder to predict the relevance scores the query with possibly relevant codes retrieved by the dual-encoder. To achieve this, we should train the model to improve its ability to identify relevant codes from those retrieved by the dual-encoder. In pursuit of this objective, we utilize a set of codes predicted to be possibly relevant by the dual-encoder as negative samples for the cross-encoder. The dual-encoder is capable of predicting the relevance scores of each query to all codes, and we can select the codes with high relevance scores as the potentially relevant ones. However, we must exercise caution, as the small set of codes with the highest relevance scores may contain false negatives, which are relevant but not labeled. To prevent the inclusion of such false negatives, we exclude the codes with the highest relevance scores predicted by the dual-encoder from our sampling process. To implement above idea, we propose a probabilistic hard negative sampling method, as shown in Figure 3. The specific process contains the following steps. We first calculate the relevance scores of the query q with all codes ci in the training set using the well-trained dual-encoder. We then eliminate two types of codes from the sampling candidates: (1) the (probably) irrelevant codes whose relevance scores are below a threshold ρ1, and (2) the (probably) false negative codes whose relevance scores are above a threshold ρ2. Since the number of irrelevant codes is much greater than the number of false negative codes, the remaining codes are still relatively hard negative samples. We refer to these remaining codes as {c∗i }Li=1, which satisfy the condition ρ1 < sdual(q, c∗i ) < ρ2. Next, we rescale the relevance scores of left code candidates
as a probability distribution using the following formula:
pi = esdual(q,c
∗ i )/τ ′∑L j=1 e sdual(q,c∗j )/τ ′ , i = 1, 2, · · · , L, (6)
and then sample m negative samples according to this probability distribution as negative samples. The parameter τ ′ adjusts the smoothness of the distribution; larger τ ′ results in a smoother distribution, allowing us to sample more codes with higher relevance scores according to the dual-encoder. After sampling m negative codes, we can calculate the InfoNCE loss function and optimize the cross-encoder model. Through the use of a probabilistic hard negative sampling strategy, the cross-encoder is able to be more effectively trained to serve as the ranker module within the cascaded RR framework. This framework is referred to as R2PS. Additionally, the cross-encoder can be optimized by implementing an in-batch negative sampling method as the dual-encoder, which we refer to as RR in our paper. d. model analysis We conduct some analysis to exhibit the superiority of the RR/R2PS method. First, we demonstrate the model capability of the cross-encoder and provide rationalization for its effectiveness in comparison to the dual-encoder. We then present a complexity analysis of the dual-encoder, crossencoder, and RR framework to demonstrate the efficiency of the RR framework for evaluation and online serving. Finally, we discuss the process of fine-tuning CodeBERT, which is the work most closely related to our paper. a) Model Capability: The self-attention mechanism for the query tokens in the dual-encoder can be formualted as:
Hq = AqqV c, (7)
where Aqq ∈ Rl∗l denotes the attention matrix to model the interactions between all query tokens, V q,Hq ∈ Rl∗d is the representation matrix of all query tokens. Similarly, the selfattention mechanism for the code tokens in the dual-encoder can be formualted as:
Hc = AccV c, (8)
where Acc ∈ Rm∗m denotes the attention matrix to model the interactions between all token tokens, V c,Hc ∈ Rm∗d is the representation matrix of all code tokens. These two equations can be combined into one as follows:(
Hq Hc
) = ( Aqq 0 0 Acc )( V q V c ) . (9)
The self-attention mechanism of all tokens in the crossencoder can be formualted as:(
Hq Hc
) = ( Aqq Aqc
Acq Acc
)( V q
V c
) , (10)
where Aqc ∈ Rl∗m,Acq ∈ Rm∗l denote the interactions between query and code tokens. Comparing Equation (9) and Equation (10), we can find that the dual-encoder is actually a special case of the cross-encoder
with Aqc = 0 and Acq = 0. Hence, it is apparent that the cross-encoder possesses a stronger model capability than the dual-encoder. In Figure 4, we demonstrate the importance of query-code interactions modeled by Aqc and Acq using a case study. To accurately predict the relevance of a query and code, it is essential to examine the various matching relationships between them in detail, as depicted by the same colors in the figure. The matching query and code tokens facilitate strong interactions in Aqc and Acq , which result in that their representations are enhanced by fusing information within these matching tokens each other with Aqc and Acq . This approach enables the model to learn the matching relationships between query and code tokens more effectively. b) Complexity Analysis: Assuming that we have M queries to provide them with relevant codes from a codebase containing N codes, we analyze the complexity about the evaluation with different framework. The search process entails computing the relevance scores of all queries against all codes. These relevance scores are represented by a matrix S ∈ RM∗N , where each row corresponds to a query and each column represents a code. With the dual-encoder architecture, S can be factorized by multiplying the embedding matrix of all queries and codes, as expressed in the following equation:
S = QCT , (11)
where Q ∈ RM∗d and C ∈ RN∗d, d denotes the embedding dimension. Each row in Q and C is the output of PLM encoder for corresponding query and code. Therefore, M rounds of PLM propagation for queries and N rounds of PLM propagation for codes are sufficient. As a result, the dualencoder approach offers a complexity of O(M +N). On the contrary, the dual-encoder cannot factorize S as the dual-encoder. Each element in S must be calculated by propagation of the PLM encoder. S consists of M ∗N elements, making the complexity of the cross-encoder O(M ∗N). The RR framework comprises of two cascade modules. Firstly, a dual-encoder is used to retrieve relevant codes from the entire codebase, which has a complexity of O(M + N). Secondly, a cross-encoder is employed to identify relevant codes from the k retrieved codes. For every query, the crossencoder needs to conduct forward propagation k times, re-
sulting in a complexity of O(M ∗ k) for the cross-encoder in the RR framework. Therefore, the total complexity of the RR framework can be expressed as O(M ∗ (1 + k) +N). Table II summarizes the complexities of the three frameworks discussed above. The dual-encoder’s complexity of O(M +N) makes it highly efficient to implement for evaluation. However, the cross-encoder’s complexity of O(M ∗N) makes it infeasible to implement, as the number of queries and codes are often immense. In contrast, the complexity of the RR framework, given that k is always a small number, is of the same magnitude as that of the dual-encoder but much lower than that of the cross-encoder. Therefore, the RR framework is also highly efficient to implement. c) Relation with Finetuning CodeBERT: CodeBERT [8] was the pioneering work that leveraged PLM for the code search task. It utilizes the cross-encoder framework for finetuning and trains the search model using binary classification loss, which suffers from the efficiency problem and is not practical for evaluation purposes on all codes in the codebase. The official code of CodeBERT to evaluate their model by ranking the codes from a batch of 1000 codes rather than the full codebase, which is inconsistent with real scenario and may lead to inaccuracies due to high variance. To address the issue of inconsistency between evaluation and real-world application, the research team behind CodeBERT adopted a different approach in their subsequent work [7], [9]. They abandoned the previous method and instead used a dual-encoder framework for code search. Furthermore, based on the experimental results reported in their later work, it appears that they also re-implemented CodeBERT using the dual-encoder framework and reported the results of finetuning CodeBERT with the dual-encoder framework in their subsequent work. In this paper, we have followed their approach and re-implemented the finetune settings for CodeBERT accordingly. Even compared to the original CodeBERT implementation which uses a cross-encoder in the official code [8], our method differs in three key ways. First, we use the RR framework to efficiently cascade the dual-encoder and cross-encoder components. Second, we train our model using rank loss and leverage a probabilistic hard negative method to enhance the training of the cross-encoder. This rank loss differs from the classification loss used in the original CodeBERT implementation and is better suited for a code search setting. Finally, we evaluate our model using all codes in the codebase as candidate codes, whereas CodeBERT only considers a batch of 1000 codes for evaluation. iv. experiments In this section, we conduct experiments on four datasets to evaluate our proposed method. The experiments are designed to address the following research questions: • RQ1: Can our proposed RR/R2PS patch boost the perfor-
mance of PLMs for the code search task? • RQ2: Is the cross-encoder inefficient for online serving? If
so, can our RR framework overcome this efficiency issue? • RQ3: How does the number of retrieved code snippets
impact the performance and efficiency tradeoff? • RQ4: Does our R2PS method provide a more reasonable
ranking distribution of relevant code snippets? a. experimental setup Our proposed RR/R2PS is actually a patch during the finetuning stage for the code search task, which can be applied to various pretrained language models designed for code. To fully validate the effectiveness and universality of our proposed method, we use three different code PLMs as backbone models of our RR/R2PS patch, including CodeBERT [8], GraphCodeBERT [7], and UniXcoder [9]. a) Datasets: We evaluate using four code search benchmark datasets: CodeSearchNet (CSN) [7], [31], AdvTest [32], StaQC [33], and CosQA [24]. CSN is collected from GitHub and uses the function comments as queries and the rest as codes. It includes six separate datasets with various programming languages including Ruby, JavaScript, Go, Python, Java, and PHP. AdvTest is a challenging variant of CSNPython that anonymizes function and variable names. StaQC is obtained from StackOverFlow with question descriptions as queries and code snippets in answers as codes. CosQA is a human-annotated dataset with queries collected from Bing search engine and candidate codes generated from a finetuend CodeBERT encoder. Table III summarizes the dataset statistics, including the number of relevant query-code pairs in the training, validation, and test set, and the number of codes in the codebase used for evaluation. The codebase for AdvTest is comprised of all codes in the validation and test sets respectively. b) Evaluation Metrics: To evaluate the performance of code search, we use Mean Reciprocal Rank (MRR), a commonly used metric for code search in previous research [9]. MRR calculates the average reciprocal rank of the relevant codes for all queries. It is defined as:
MRR = 1 |D| ∑
(q,c) inD
1
rank (q) c
, (12)
where rank(q)c is the rank of the relevant code c among all code in the codebase for the query q, and |D| is the total number in the dataset. c) Baselines: The compared methods include: (1) CodeBERT [8], which was pretrained on the masked language modeling and replaced token detection task; (2) GraphCodeBERT [7], which was pretrained on additional graph relevant tasks such as link prediction of data flow graph; (3) UniXcoder [9], which was pretrained on addtional language modeling task; (4) SyncoBERT [34], which was pretrained on syntax-enhanced contrastive learning task; (5) CodeRetriever [20], which was pretrained on NL-code contrastive learning task. The reported results of CodeBERT, GraphCodeBERT, and UniXcoder are reproduced by us by re-run their official released code, and the performance is nearly the same as their reported results in their paper. d) Implementation Details: During the finetune stage, both the query encoder and the code encoder in the dualencoder share parameters, with the same code pre-trained language model (PLM) used to initialize the two encoders. The cross-encoder is also finetuned based on the corresponding code PLMs. However, it does not share parameters with the previous query and code encoders in the dual-encoder framework. In the RR approach, we train the dual-encoder and the cross-encoder together using in-batch negative sampling strategy. On the other hand, the R2PS approach involves a two-step training process: first, we train the dual-encoder with in-batch negative sampling; then, we train the cross-encoder with negative sampling based on relevance scores calculated by the previous well-trained dual-encoder. We follow the same experimental settings as the series work of CodeBERT, GraphCodeBERT, and UniXcoder, including 10 training epochs, a learning rate of 2e-5 with linear decay, and 0 weight decay. We set the temperature hyper-parameter τ in the InfoNCE loss function to 0.05 for CSN and CosQA datasets, and 0.025 for AdvTest and StaQC. The number of retrieved codes in the RR architecture is 10. For the CosQA dataset, we use the average of the dual-encoder and crossencoder as the ranker module. In the InfoNCE loss, we set the number of negative samples to 31 to fully utilize the GPU memory. In probabilistic hard negative sampling, we set ρ1 and ρ2 to keep only a small set of samples beginning from rank 1 by the dual-encoder for CSN, CosQA, and AdvTest without tuning, and a small set of samples beginning from the top 0.4% rank for StaQC. We set the sampling hyper-parameter 1/τ ′ as 0 without tuning. In fact, the default setting without tuning for most hyper-parameters can achieve satisfying performances in our method. However, the hyper-parameters also keep the flexibility to perform well for some rare data distributions. All experiments are conducted in a server consisting of 8 NVIDIA
A100-SXM4-80GB GPUs. We will release our code and welltrained model when our paper is published. b. performance comparison(rq1) The performance of compared methods in terms of MRR is shown in Table IV. The RR patch was applied to CodeBERT, GraphCodeBERT, and UniXcoder’s backbone models, while the R2PS patch was applied only to UniXcoder, due to computing resource limitation. From this table, we observe the following: (1) Our proposed RR patch significantly improves the performance of all three backbone models, with an average improvement of 4.7 on CodeBERT, 4.1 on GraphCodeBERT, and 2.8 on UniXcoder across the four datasets. Furthermore, the R2PS patch leads to a substantial improvement of 4.1 MRR for UniXcoder. These results demonstrate the effectiveness of our proposed RR/R2PS method for code search. (2) The performance enhancement of UniXcoder with the RR patch is more significant than GraphCodeBERT and CodeBERT, consistent with their performance without the RR patch. This finding suggests that better-performing code PLMs can achieve even stronger performance when used in conjunction with the RR patch. (3) Overall, UniXcoder with R2PS outperforms all other methods, including the latest CodeRetriever, providing further evidence for the superiority of our proposed method. c. efficiency evaluation (rq2) We conducted a comparison of the computing costs of the dual-encoder, cross-encoder, and our RR framework during the online serving stage. To simulate incoming queries in online serving, we simulate the response processing time when the server receives a user query and infers relevant codes before returning them to the user. The code embeddings were precalculated in the pre-processing stage, and the response time did not include the time required to embed codes for the dualencoder. We conducted experiments with codebase scales of 1,000, 10,000, and 100,000 codes, and calculated the average response time for 100 query requests, using an A100 GPU. Our results, presented in Table V, show that: (1) The dual-encoder and the RR achieve response times of no more than 100ms for a codebase with 100,000 codes, which is efficient for online serving. In contrast, the cross-encoder takes approximately 8 minutes to respond for a codebase of 100,000 codes, which is too long for online serving. (2) The response time of the RR is longer than that of the dual-encoder due to the ranker module in the RR framework. However, the extra time is justified considering the significant performance improvement. (3) As the codebase scale increased by 10 times, the computing time for the cross-encoder increases by about 10 times, while the computing time for the dual-encoder and RR increases slowly (much less than 10 times). This is because the cross-encoder requires the encoding of the concatenation of the given queries and all codes in the codebase during online serving, while the dual-encoder and RR framework calculate the embeddings of all codes in the codebase during the data pre-processing stage. d. hyper-parameter analysis (rq3) We conducted experiments to investigate the impact of varying the number of retrieved codes, k, on both the performance and user satisfaction of our framework. Specifically, we measured user satisfaction by evaluating their tolerance for the response time of the system. To this end, we defined the user satisfaction score, S, as follows:
S = 100
t+ 50 , (13)
where t denotes the response time of the model in milliseconds, and we added 50ms to simulate additional system costs such as network delay. For instance, if t equals 50ms, the
total waiting time would be 100ms, resulting in S = 1, which indicates high user satisfaction. Conversely, if t is greater than 150ms, the total waiting time would be over 200ms, and S would be less than 0.5, indicating low user satisfaction. From the result shown in Figure 5. we can observe that: (1) In general, the MRR of RR/R2PS increases with the larger number of retrieved codes k, but the rate of increase slows down with higher k values. This suggests that retrieving more codes can enhance performance, but the improvement becomes less significant as more codes are retrieved. (2) The performance of R2PS is better than R2 in most cases with different k and different datasets, demonstrating the effectiveness of our proposed probabilistic hard negative sampling strategy for code search. This strategy helps in selecting better codes for training the model, resulting in improved performance. (3) User satisfaction decreases as k increases due to the longer forward propagation time required by the cross-encoder. To achieve higher user satisfaction, it is recommended to use a smaller value of k. (4) The performance of R2PS declines slightly in the CSN-python, CSN-Java, and AdvTest datasets with large k values. With fewer retrieved codes, the candidate codes for the cross-encoder are more difficult, whereas with more retrieved codes, the candidate codes are easier. While hard negative sampling in R2PS enhances the model’s ability to handle hard negative samples, its ability to distinguish easier codes may decrease. This explains the decrease in R2PS performance with large k values. (5) Setting k less than 10 causes a significant decline in performance, while k greater than 20 causes a substantial decrease in user satisfaction. Therefore, we recommend setting k between 10 and 20 in our RR/P2PS method. e. rank distribution analysis (rq4) We conduct experiments to show whether our RR/R2PS patch can help the PLMs to rank the relevant codes to higher position. Specifically, we figure out the distribution of ranks of relevant codes in the test set, as shown in Figure 6. We can observe: Compared to UniXcoder, our RR/R2PS patch can rank a larger number of relevant codes to top-1 position, and R2PS patch performs better than RR from this point. Larger number of relevant codes in the top-1 position means user can find useful information within top-1 result in the list. Thus, our RR/R2PS can help the PLMs become the more effective code search tool. (2) We find that the rank distributions of the two CSN datasets (Python and Java) are similar, while are different from StaQC and AdvTest. The steady improvements of our RR/R2PS patch on these different datasets further validate the universality of our proposed method. v. conclusion In this paper, our aim is to enhance the performance of PLMs for the code search task during the finetune stage. To achieve this, we introduce several novel approaches. Firstly, we propose a cross-encoder for code search, which enhances the model’s capabilities compared to the typical dual-encoder. Next, we propose a retriever and ranker framework for code
search that balances both effectiveness and efficiency. Finally, we propose a probabilistic negative sampling method to further improve the retriever and ranker framework’s effectiveness. We conducted thorough experiments and found that our R2PS system significantly improves performance while incurring an acceptable extra computing cost.