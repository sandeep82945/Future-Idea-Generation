Due to the prevalence of scale variance in nature images, we propose to use image scale as a self-supervised signal for Masked Image Modeling (MIM). Our method involves selecting random patches from the input image and downsampling them to a low-resolution format. Our framework utilizes the latest advances in superresolution (SR) to design the prediction head, which reconstructs the input from low-resolution clues and other patches. After 400 epochs of pre-training, our Super Resolution Masked Autoencoders (SRMAE) get an accuracy of 82.1% on the ImageNet-1K task. Image scale signal also allows our SRMAE to capture scale invariance representation. For the very low resolution (VLR) recognition task, our model achieves the best performance, surpassing DeriveNet by 1.3%. Our method also achieves an accuracy of 74.84% on the task of recognizing low-resolution facial expressions, surpassing the current state-of-the-art FMD by 9.48%. 1 introduction Masked Image Modeling (MIM)[5, 23, 71] self-supervisely learns deep representations by masking a portion of input signals and predicting these masked signals. MIM has shown has shown great success in downstream tasks such as image classification [13], object detection [40], semantic segmentation [77], and video classification [19]. The MIM method is a form of self-supervised learning that utilizes signals during the learning process. Researchers are currently exploring the use of multiple self-supervised signals for learning through a detailed analysis of the MIM method. MAE consider the original pixel intensity as a self-supervised signal by reconstructing it based on partial observation. MaskFeat demonstrated that masked features like graident histogram is a powerful self-supervised signal. CAEv2 utilizes CLIP as a self-supervised signal. In our study, we propose using scale variance as a self-supervised signal. Scale variance is prevalent in nature images due to the continuous variation of the projection size of an object relative to the human eye or camera sensor as its distance changes [31]. This scale variance poses numerous challenges for modern neural networks [3]. A minute variation in size that is imperceptible to the human eye can cause a significant change in the output. Thus, scale invariance is an integral aspect of the advancement in computer vision[41]. Recent studies have demonstrated that resolution/scale is an efficient self-supervised signal that enables the network to generalize its object detection capability to low-resolution (LR) images. We introduce scale as a self-supervised signal in our proposed novel MIM algorithm called Super Resolution Masked Autoencoders (SRMAE). Fig. 1 illustrates that the input image is divided into a set of patches, some of which are downsampled into low-resolution (LR) patches. On the other hand, the remaining patches are called high-resolution (HR) ones, similar to the MAE [23], where our
Preprint. Under review. ar X
iv :2
30 8. 08 88
4v 1
[ cs
.C V
] 1
7 A
ug 2
SRMAE also encodes the HR patches. In contrast to MAE, which solely reconstructs the input image from these HR patches, we use both LR and encoded HR patches to reconstruct the original signal. The empty mask token of MAE is substituted with the LR signal in the prediction head. Additionally, we employ a High Preserving Block (HPB) module [42] to extract features from LR patches before reconstructing the original signal with a lightweight ViT (Vision Transformers) structure to benefit from the super-resolution (SR) technique. The original image is termed as high-resolution (HR) image. Similar to the ViT [17], the highresolution image and the low-resolution image are divided into two sets of regular non-overlapping patches. We sample a subset of patches from the high-resolution patches and take the low-resolution patches that are relative to the unsampled high-resolution patches as the subsequent prediction head input. The encoder of SRMAE, similar to the encoder of MAE [23], processes only these sampled high-resolution patches. Nevertheless, unlike the prediction head of MAE, we preserve the lowresolution signals of patches that have not been encoded. The complete set of tokens comprises encoded high-resolution patches and selected low-resolution patches that act as the input to the SRMAE prediction head. In contrast, the full set of tokens for MAE consists of encoded visible patches and mask tokens. We add positional embeddings to all full set tokens, and the prediction head restores the resolution for the selected low-resolution patches. Rather than reconstructing pixels like MAE, we need to recover the resolution. To achieve this goal, we use an HPB module [42] that is appropriate for extracting features before the lightweight ViT structure. In the fine-tuning stage of ImageNet-1K, SRMAE with ViT-B achieved a top-1 accuracy of 82.1%, which is 1% lower than MAE with an equal number of training epochs. The use of self-supervision with the scale signal makes our SRMAE capable of capturing scaleinvariant representations. (Very) low resolution (VLR/LR) recognition is particularly challenging as neuron networks have to recognise under various scales. We thus evaluated the performance of our SRMAE on several VLR/LR tasks. In particular, we conducted a VLR digit classification experiment on the SVHN dataset using images of resolution 8×8. We conducted LR facial expression classification experiments using images of resolution 100×100 on the CK+, RAF-DB, and ExpW datasets. Our SRMAE shows strong scale-invariant ability when compared with state-of-the-art (SOTA) methods such as DeriveNet[57], which obtained a top-1 classification accuracy of 87.85% on the SVHN dataset, and FMD[28], which achieved 65.37% top-1 accuracy on the ExpW dataset. Our SRMAE has achieved SOTA performance on some VLR/LR recognition datasets. On the SVHN dataset, our SRMAE achieved a top-1 accuracy of 89.14%, which is 1.3% higher than the previous state-of-the-art (87.85%[57]). On the ExpW dataset, our SRMAE achieved a top-1 accuracy of 74.84%, which is a significant improvement of 9.5% over the previous state-of-the-art top-1 accuracy of 65.37%. In this paper, we mainly make the following contributions:
(i) For the first time, we propose to utilise scale as a self-supervised signal for Masked Image Modeling (MIM). Our SRMAE is a simple and easy-to-implement framework preserving scale invariance. (ii) Our framework could leverage the latest super-resolution (SR) architecture advance to design the prediction head that predicts masked original signals from low-resolution clues. (iii) SRMAE achieves close to SOTA results in different resolution visual tasks, such as finetuning on ImageNet-1K, VLR digit classification on SVHN dataset and LR facial expression classification on ExpW dataset. 2 related work Masked Image Modeling Masked Image Modeling (MIM) has become a popular pretext task for visual representation learning, inspired by BERT[14] for Masked Language Modeling[5, 23, 71]. The context encoder approach[51] is a groundbreaking work in this field, which masks a rectangular section of the original images and predicts the missing pixels. iGPT[9], ViT[17], and BEiT[5] were the first notable works that recognized the MIM learning strategy on modern vision Transformers. Several target signals have been developed for the mask-prediction pretext task in MIM, such as normalized pixels[23, 71], discrete tokens[5, 16], HOG features[69], deep features[4, 78], and CLIP[74]. A major bottleneck for the industrial applications of MIM is that these models often require large computational resources and substantial pre-training time. As a result, some works have accelerated the process by employing the asymmetric encoder-decoder strategy[23, 27] or reducing the input patches[8, 37]. In this work, we leverage scale as a self-supervised signal to learn scale-invariant representation. Super Resolution Compared to traditional image super resolution methods [21, 63, 62, 46, 22] which are generally model-based, learning based methods have become more popular due to their impressive performance. The convolutional neural network (CNN)-based super-resolution model holds an important position in super-resolution work, with SRCNN[15] being the first model to utilize CNN in SISR. Since then, many works[39, 75, 12, 30, 29, 44] have continuously improved the CNN-based super-resolution model. With the increasing use of Vision Transformer in the field of vision [17], IPT [7] has utilized a transformer-based network as a pre-trained model. SwinIR [38] introduced the Swin Transformer to SISR, while ESRT [42] uses a lightweight CNN backbone to extract deep features and obtain long-range dependency relationships through a lightweight Transformer, combining the CNN and Transformer frameworks. In this study, the HPB module in ESRT [42] was adopted to extract features for resolution recovery since the HPB module aligns well with our prediction head in predicting the original resolution signal from chosen LR patches and remaining HR clues. VLR/LR recognition VLR/LR images (or regions of interest) often contain less information content, rendering ineffective feature extraction and classification, thus can intuitively demonstrate the model’s learning ability for the features of low-resolution images. In practical applications, we often face lowquality images, which may be low-resolution, noisy, blurry, or have low dynamic ranges. According to the conclusion in [35], even when the degradation in image quality is imperceptible to the naked eye, the performance of deep neural networks will significantly decline. At the same time, the paper also indicates that super-resolution can effectively alleviate the decrease in classification accuracy caused by low resolution. In existing VLR/LR recognition technology, HR information is usually used to improve the classification model, which can be divided into image-level, feature-level, and classifier-level. In terms of image-level technology, Grm et al. [20] proposed a cascaded superresolution network and a set of face recognition models as priors, while Kazemi et al. completed the same task using a multi-scale generative adversarial network [32]. Researchers have also proposed VLR/LR algorithms that merge HR information at the feature or classifier level [48, 1, 67, 18]. In the work of [57], effective class boundaries were learned through joint training of two losses, and a new data augmentation based on a multi-resolution pyramid was used for training. In this work, the combination of super-resolution and the MIM model has made our model more robust for low-resolution image tasks. Therefore, we demonstrate the scale invariance ability of our model through the VLR/LR recognition task. 3 method  3.1 a revisit of mim The MIM framework involves masking parts of the input image and then predicting their information based on the unmasked observation. It consists of four main components: Masking strategy, encoder architecture, prediction head, and prediction target. The masking strategy determines how to select and mask an area. The encoder architecture extracts a latent feature representation for the masked image, which is then used to predict the original signal within the masked area. The prediction head is applied to the latent feature representation to generate one form of the original signals within the masked area. The prediction target defines the form of the original signals to be predicted. Both our SRMAE and MAE belong to the MIM framework. Our SRMAE is a modification of MAE’s structure, so based on the MIM framework, we will first highlight the similarities and differences between our approach and the well-known MAE. MAE treats the original pixel intensity as the prediction target by reconstructing it, while we use scale as our target by recovering the resolution. The masking strategy in MAE samples a subset of patches and remove the remaining ones, sending the remaining visible patches to the encoder for learning latent representation. Our SRMAE gains low-resolution signals through downsampling and utilizes these removed patches in MAE, which will be explained in more detail later. The encoder used in MAE is a ViT and applied only on sampled patches, which is also the structure we adopted. The prediction head in MAE is a lightweight decoder used for pre-training to perform the image reconstruction task. Our prediction head design differs from that of MAE, and we will elaborate on this in the following section. 3.2 srmae Directly applying the original pixel intesity as self-surpervised signal by reconstructing would not be suitable for images with varying scales. We introduce scale as self-supervised signal to utilize the image information at different resolutions. The overall pipeline of SRMAE is shown in Figure 1. Applying the original pixel intensity as a self-supervised signal by reconstruction would be unsuitable for images with varying scales. To account for such images, we introduce scale as a self-supervised signal to utilize the information at different resolutions. The overall pipeline of our SRMAE is illustrated in Figure 1. Instantiations Our work leverages this process to train a novel MIM model, using scale as the self-supervision signal. An example will illustrate the main structure and training process of our model. Given an input image x, we first perform the encoder stage. We divide x into non-overlapping image patches, represented by xp. At the same time, we downsample and patchify x to obtain a low-resolution xl. We then perform a masking operation, selecting a random mask M and replacing the corresponding parts of the patches with [MASK], obtaining xph = x
p⊙M+[MASK]⊙(1−M). xph is a learnable embedding indicating masked patches. We then add positional information to x p h and learn the image’s latent representation xe through the Vision Transformer. Then, in the prediction head, we concatenate xl and xe based on their corresponding positional information. This replaces the patches marked with [MASK] with the corresponding patches from xl. We preliminarily extract information that favors super-resolution from the concatenated patches sequence using a replaceable super-resolution module. Subsequently, we apply a lightweight Vision Transformer[17] that recovers the resolution, resulting in yp. The recovery loss is obtained by calculating the mean squared error (MSE) between yp and the original image patches xp, Lrec = ||(yp − xp)⊙ (1−M)||22. Only the loss generated by the parts replaced by xl is calculated[23]. During the pre-training stage, we use scale as the self-supervised learning signal, combined with information from input images at different scales to learn the deep cross-resolution representation of images. After pre-training, we abandon the prediction head and only use the encoder for downstream visual tasks under different resolution conditions. Downsampling To utilize scale as a self-supervised signal for training, one must obtain images with resolutions that differ from the original image. Additionally, in order to maintain the positional information of images, it is required to stitch together images with different resolutions to their initial positions once resolution changes have been made. As such, the downsampling process involves two steps: image downsampling and dimensions adaptation. As show in bottom part of 1. The first step is to perform a downsampling process. In order to fully utilize the latest advanced super-resolution structure subsequently when restoring the image’s resolution, we referred to some papers on super-resolution[15, 61, 42, 7] where the frequently used scaling factors are 2, 3, and 4. Therefore, in this study, we applied these ubiquitous scaling factors to take full advantage of this advanced technology, which is widely recognized in the literature for its effectiveness. For example, when using a scaling factor of 4, we downscaled the original 224× 224 image to 1/4 of its resolution, resulting in a 56 × 56 image. In the second step, we concatenated the high-resolution patches with the low-resolution patches. Since the dimensions of HR and LR images do not match, we used the nearest neighbor interpolation method to resize the LR image to 224× 224 dimensions. After that, we divided this interpolated image into non-overlapping patches of size 16× 16. Encoder Only visible patches xp = {xip|M i = 1} are fed to the encoder following[23] and mapped to the high resolution patches xe across a stack of transformer blocks. The operation of encoder is based on self-attention. In this paper, we utilize ViT-Base to form the encoder. Prediction head Our study aimed to create a prediction head that could predict resolution recovery in conjunction with super-resolution technology. In contrast to the prediction head design of MAE, our SRMAE model’s input for the prediction head consists of a patch sequence composed of highresolution patches xe = {xie|M i = 1} output by the encoder phase and the low-resolution patches xl = {xil|M i = 0}. We altered the input of the prediction head to enable resolution restoration instead of image reconstruction by using low-resolution image patches in place of [MASK] training patches utilized in MAE. The prediction head, illustrated in 1, consists of a High Preserving Block (HPB) module and a lightweight Vision Transformer (ViT). To enhance the resolution recovery ability of the prediction head for super-resolution tasks, we extracted applicable modules from advanced super-resolution works. The HPB module proposed in ESRT captures image texture details in its own super-resolution model structure, making it suitable for feature extraction from both high-resolution and low-resolution patches. As such, we used it as the first processing module in our prediction head. The implementation of a transformer in IPT for super-resolution tasks demonstrates that the lightweight ViT used in our prediction head also possesses super-resolution ability. As a result, by combining effective modules from super-resolution works with the ViT in the prediction head, we achieved resolution recovery of low-resolution patches. 4 experiments The SRMAE framework is designed for learning scale-invariant representations. In this section, we conducted extensive experiments to evaluate the effectiveness of the SRMAE framework in learning
scale-invariant representations. We evaluated the ability of SRMAE to maintain scale invariance for high-resolution images by conducting experiments on the ImageNet-1K dataset [13]. We fine-tuned the entire set of parameters of the classification task using a fine-tuning protocol and reported the results of transfer learning to assess the quality of scale invariance for high-resolution images. In addition, we conducted numeric classification tasks on the SVHN dataset[47] at very low resolutions and low-resolution object recognition tasks on the CK+[43], RAF-DB[36], and ExpW[76] datasets to assess the ability of SRMAE in preserving scale invariance for low-resolution images. 4.1 high-quality images tasks Settings The imageNet-1K(IN-1K)[13] dataset contains 1.3 million images categorized into 1000 classes for image classification and is divided into training and validation sets. The evaluation protocol is pre-training followed by end-to-end fine-tuning. We utilized vanilla ViT[17] base models without modification and pre-trained our models without labels on the IN-1K training set at a resolution of 2242. To minimize data augmentation, we used random resized cropping and horizontal flipping. We randomly mask out 75% of total image patches following MAE[23]. For downsampling the images during pre-training, we employed the commonly-used four-fold downsampling strategy used in super-resolution experimentation[15, 42]. We follow the default finetuneing parameters of the MAE[23]. For finetuning, we report the classification accuracy on the IN-1K validation set of the finetuned SRMAE encoders. Classification on ImageNet-1K We present the accuracy of SRMAE on Table 1 and conduct comparisions with previous mask autoencoding methods. For fair comparison, we estimate the pre-training duration of each model on the same machine with one Tesla V100-32G GPU. We report the running time on single GPU, denoted as ’Pretraining Time’. We pre-trained SRMAE for 400 epochs, obtaining a top-1 accuracy of 82.1%, which is 0.3% higher than from-scratch. Compared with the MAE pretrained for 400 epochs, our finetuning accuracy is 1% lower with the same number of pretraining epochs. Our finetuning accuracy is 1.2% lower than that of the MAE pretrained for 800 epochs, despite the close pretraining time. CCKD[52] is a comparative study in subsequent LR recognition experiments, and it also reports the top-1 accuracy on ImageNet-1K. Compared with CCKD, our top-1 accuracy improved by 14.4%, far exceeding their results. We believe that the use of scale signal leads to s subpar from-scratch and fine-tune performance, and leave further investigations of this phenomenon to future work. We note, however, that our method still improves over these handling low-resolution models by as large a margin as other methods. Our method still achieves significant improvement compared to previously used models (CCKD[52], AT[34]) for low-resolution tasks. Semantic segmentation on ADE20K We utilized UperNet [70] and followed the semantic segmentation code of [5, 23]. Our SRMAE’s mIoU, as presented in Figure 3, was 14.55 at the beginning of training and improved to 35.5 at the end, a rise of 20.95 points. To compare our results, we replicated
MAE’s semantic segmentation experiment, which yielded an initial mIoU score of 28.05, which improved to 46.1 at the end, a rise of 18.05 points. Although our transfer learning ability was inferior to MAE, our training improvement was 2.9 points higher. We evaluated the loss results and observed that the loss values were consistently within similar levels, as shown on the right side of Figure 3. However, on the left-hand side, a noticeable gap in mIoU persisted. Because we based our code on MAE’s implementation, we speculate that our model modifications made the initialization of other segments irregular, leading to unsatisfactory mIoU scores. 4.2 low-quality images tasks Datasets The SRMAE has been evaluated on two different tasks: VLR digit classification and LR facial expression classification. The first task utilized one dataset, whereas the second task used three datasets to evaluate the scale invariance ability of SRMAE. Details for each benchmark datasets ars as below:
(i) SVHN[47]: We employed the Street View House Numbers (SVHN) dataset to assess the efficacy of SRMAE in classifying very low-resolution digits. This dataset comprises 0-9 digit images that were captured from natural scenes in the real world, at a 32×32 resolution. It consists of 73,257 images for training and 26,031 images for testing. Consistent with established protocols set forth by Wang, et al. [67], the test dataset was created via subsampling the images by a factor of 2, resulting in an 8×8 resolution that represents the lowest resolution among the four datasets. (ii) CK+[43]: The CK+ dataset contains 593 image sequences, each describing the transition process from a neutral expression to an emotional peak. We followed the protocol of [11] and used six prototype expressions (anger, disgust, fear, happiness, sadness, and surprise) in the training and testing process. In 327 image sequences, we selected the first frame as the neutral expression. Therefore, we obtained a total of seven expressions, with 1254 facial expression images, including the background, at a resolution of 640 × 490 pixels. In the experiment, we converted the images to 100 × 100 pixels for low-resolution testing. (iii) RAF-DB[36]: The RAF-DB database contains 12,271 training images and 3,068 testing images, with each picture being 100 × 100 pixels and using seven emotion labels including anger, disgust, fear, happiness, sadness, surprise, and neutral. (iv) ExpW[76]: The ExpW database is one of the largest facial expression databases, containing 91,793 network images. The facial area of these images ranges from 23×23 pixels to 180×180 pixels. We use the dlib library to extract the facial area from these images. To meet the experimental requirements, images with a facial area smaller than 80×80 pixels were filtered out, while those
Algorithm CK+[43] RAF-DB [36] ExpW[76]Extra Data Top-1 Acc. Extra Data Top-1 Acc. Extra Data Top-1 Acc. equal to or greater than 80×80 pixels were resized to 100×100 pixels. In the end, a total of 31,127 processed images were obtained. VLR digit classification on SVHN dataset As the SVHN dataset contains sufficient data, we pretrain our SRMAE on SVHN dataset without label followed by end-to-end fine-tuning on SVHN dataset. Table 2 presents the top-1 and top-5 classification accuracies on the SVHN dataset. The SRMAE model achieved a top-1 classification accuracy of 89.14% and a top-5 classification accuracy of 98.40%, demonstrating an improvement of over 1% from the state-of-the-art result.We used MAE to conduct the same experiment, and the final results showed that the top-1 classification accuracy was 89.10%, while the top-5 classification accuracy was 98.38%. Our results show an improvement compared with those reported. The improved performance demonstrates that using scale as a self-supervised signal can enhance the ability of the model to recognize extremely low-resolution images(e.g., digits) and achieve advanced levels. LR facial expression classification on CK+ dataset: In the experimental setup, we discovered that the CK+ dataset had limited data. Thus, training the model using CK+ dataset alone may not produce sophisticated results. We employed the model that was pre-trained for 400 epochs on the ImageNet-1K[13] dataset and fine-tuned it on the CK+ dataset. According to the results presented in 3, our model attained a top-1 accuracy of 94.21% on this dataset, which is within 1.4% of the state-of-the-art performance. We conducted an unfair comparison with MAE, due to the difference in the pre-training process. MAE employed a ViT-B model that was pre-trained for 1600 epochs and fine-tuned it for the downstream task, resulting in a top-1 accuracy of 95.04%. LR Facial Expression Classification on RAF-DB dataset: The size of the RAF-DB dataset is still relatively small. We also use the model that was pre-trained for 400 epochs on the ImageNet-1K dataset. We fine-tuned it on the RAF-DB dataset. According to the results presented in 3, the model’s performance on the RAF-DB dataset was found to be 5% lower than that of the state-of-the-art models. We also conducted an unfair comparison with MAE dut to MAE employed a model that was pre-trained for 1600 epochs. The results in 3 demonstrated that MAE yielded good results on the RAF-DB dataset. Therefore, we attribute our relatively lower performance to the inadequate pre-training of our model. If more comprehensive pre-training is undertaken, we anticipate that our results may significantly improve. LR Facial Expression Classification on ExpW dataset: The ExpW dataset is one of the largest databases consisting of facial expressions. Hence, we forewent the use of supplementary datasets [13] and exclusively employed the ExpW dataset for pre-training, followed by fine-tuning during downstream tasks. Our model yielded a top-1 classification accuracy of 74.84%, as per the results presented in 3. This accuracy is 9.48% higher than that of the previous state-of-the-art model, indicative of a significant improvement in our methodology. Additionally, we conducted a fair comparison with MAE on this dataset, without using any extra pre-training datasets. Our experimental results revealed that our model achieved a top-1 accuracy of 74.53% on this dataset, which is marginally below that of SRMAE by 0.3%. SRMAE exhibits strong robustness in low-resolution tasks, particularly in larger datasets where the advantages of self-supervised learning can be fully utilized, leading to excellent performance. Moreover, SRMAE improves the MIM model’s learning of low-resolution image features under similar experimental conditions by using scale as the self-supervisory signal, which renders it more suitable for low-resolution tasks compared to MAE. 5 conclusion and discussion We present a robust and effective MIM framework, named SRMAE, that is capable of learning scale-invariant deep representation and apply it to visual tasks of different scales. TOur method leverages scale as a self-supervised signal to facilitate resolution restoration, which replaces image reconstruction and adapts easily to scale-varied tasks. We have achieved extensive results in tasks such as high-resolution image classification and low-resolution object recognition. To the best of our knowledge, this is the first model that achieves close to SOTA results for high-resolution image classification and low-resolution object recognition.