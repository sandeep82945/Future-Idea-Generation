Automated program repair is a crucial task for improving the efficiency of software developers. Recently, neuralbased techniques have demonstrated significant promise in generating correct patches for buggy code snippets. However, most existing approaches arbitrarily treat the buggy context without any analysis to capture the semantic relationship between the buggy statement and its context. Additionally, we observe that existing neural models may output an unaltered patch consistent with the input buggy code snippet, which fails to be the correct human-written one for fixing the given bug. To address the aforementioned limitations, we present in this paper a novel neural program repair framework called REPEATNPR, which adapts the general pre-trained language model for fixing singleline Java bugs. We make the first attempt to use program slicing to extract contextual information directly related to the given buggy statement as repair ingredients from the corresponding program dependence graph and eliminate unaltered patches using an intuitive but effective filter mechanism. We demonstrate the effectiveness of REPEATNPR on five benchmarks when compared with state-of-the-art baselines. ii. related work As a promising research topic, APR has received significant attention from both the SE and AI communities. According to a living APR review [24], researchers have proposed a bunch of APR approaches in the last decade, which can be categorized into two mainstreams: search-based [6], [7], [25]– [30] and semantics-based [31]–[35]. With the rapid development of DL-based techniques, researchers have begun to pay more attention to NPR approaches [36], [37], which have demonstrated remarkable potential for improving program repair performance. In contrast to traditional APR approaches,
learning-based techniques can automatically capture the semantic relationships between parallel bug-fixing pairs. Tufano et al. [11] harness the power of NMT to translate buggy code snippets into fixed ones. In particular, they abstract the identifiers and literals in the source code to reduce the vocabulary size during the data preprocessing process. SEQUENCER [15] is an end-to-end approach based on sequenceto-sequence (Seq2Seq) learning that employs the long shortterm memory (LSTM) encoder-decoder architecture with copy mechanism to overcome the out-of-vocabulary issue. In addition, SEQUENCER considers the class-level abstract context surrounding the buggy statement as input to capture the longrange dependencies required for patch generation. Chakraborty et al. [38] present a two-stage approach called CODIT to learn code changes for fixing the buggy statements by using an LSTM-based NMT model. DLFix [39] parses the source code to AST and uses a tree-based LSTM to encode the code structures surrounding the bug-fixing changes as contextual information for learning code transformations. Lutellier et al. [16] propose an ensemble approach called CoCoNut that combines convolutional neural networks (CNNs) and contextaware NMT models to fix bugs in multiple programming languages. To better capture the diversity of bug-fixing patterns, CoCoNut introduces a novel context-aware NMT architecture that takes the buggy statement and its surrounding method context as two separate inputs. Different from the above NMTor Seq2Seq learning-based models, Ding et al. [12] implement an edit-based model that performs token-level insertion and deletion operations on the buggy code instead of generating raw tokens of the fixed code. Zhu et al. [40] design Recoder, a syntax-guided edit decoder with a novel provider/decider architecture. Recoder receives the buggy statement with its context, the partial AST, and a tree path from the root node to
a non-terminal node as inputs and embeds them using different encoders to generate edits on the AST of the buggy methods. Inspired by the success of pre-trained models achieved in the field of natural language processing (NLP), an emerging trend is to build language models pre-trained on large source code corpus to boost the performance of program repair. Mastropaolo et al. [41] empirically investigate the performance of the text-to-text transfer transformer (T5) model on four coderelated tasks (including program repair). The experimental results indicate that the T5 model can substantially boost the program repair performance. In a follow-up study of CoCoNut [16], Jiang et al. [42] propose CURE that modifies the NMTbased architecture by using a pre-trained GPT module to learn contextual embedding. Mashhadi et al. [43] apply a pretrained model CodeBERT [44] to fix simple Java bugs on the ManySStuBs4J dataset [45]. Specifically, REPEATNPR utilizes the pre-trained language model to automatically learn bug-fixing patterns from sliced context without necessitating professional domain knowledge. Furthermore, existing NPR models simply consider the context arbitrarily. In contrast, REPEATNPR employs the program dependence analysis technique to enhance the NPR task by extracting slicing-based contextual information from PDG. At last, REPEATNPR eliminates the unaltered patches via an effective filter mechanism that aims to take advantage of the ensemble performance of different NPR models. iii. methodology To address the limitations mentioned in Section I, we present the detailed design (as shown in Figure 2) of our proposed neural APR framework REPEATNPR in this section, which mainly consists of three stages: input processing, model training, and patch filtering. We first employ static program
slicing on the PDG to extract the relevant contextual information of the corresponding buggy statement. Then, we leverage a general pre-trained language model as the model skeleton to gain the powerful representation learning ability and finetune it using the slicing-based contextual inputs. Finally, we integrate different NPR models via an intuitive but effective filter mechanism to filter out the generated unaltered patches. A. Input Processing
Existing NPR approaches [11], [16], [17], [42], [46] usually treat the context of a buggy statement in an arbitrary manner, such as by directly considering the whole buggy method or a limited number of lines of code surrounding the buggy statement, which neither captures the semantic relationship between the buggy statement and its context nor includes sufficient contextual information as repair ingredients. Thus, we propose to utilize program slicing techniques to extract contextual information from the graphical representations of source code (i.e., PDGs). Unlike existing approaches, we consider the control and data dependencies of the corresponding buggy statement from the buggy method, which is reported to help capture the bug-fixing patterns [22], [47]. 1) Program Dependence Analysis: The graphical representations of source code are a fairly used data structure in the field of program analysis [48]. In this paper, we leverage PDG for program analysis to examine the buggy statement and all the ingredients (e.g., variable usages and method invocations). The PDG explicitly represents the dependencies among statements and predicates, which consists of two subgraphs: the data dependence graph (DDG) and the control dependence graph (CDG). The directed edges in DDG denote a data flow from the source statement to the destination statement, reflecting the influence of one variable on another (i.e., variable definition, usage, or modification). Similarly, each vertex in CDG has a control dependence on its parent vertex. Thus, the control dependency edges reflect the influence of predicates
on the values of variables. The upper left corner of Fig.3 corresponds to a buggy code snippet example, in which Line 2 of the method parse in the EditCommandParser class is a buggy statement that needs to be fixed. To illustrate, the definition of argsTokenizer in statement S2 can lead to its use in statement S6. In this case, a data dependence edge S2 99K S6 is present in the PDG (shown on the right side of Fig.3) of the buggy method. 2) Dependency Context Extraction: To determine all contexts that affect the buggy statement, we introduce an algorithm for extracting dependency context based on the def-use analysis of variables within the given node Nbuggy. Lines 2 to 19 in the introduced algorithm detailed illustrate the steps of extracting contextual statements by leveraging program slicing (both backward and forward) and ingredient matching. Specifically, we first extract a set of variables that are accessed in Nbuggy. For each variable var used in Nbuggy, we slice a set of nodes Ncontext from PDGbuggy, including both the nodes that have effect on var and those affected by var. Then, we examine all the ingredients in Ncontext and store them in varUsageSet and methodInvocationSet respectively. In addition, we collect all the public fields fieldSet and the signatures of other public methods methodSignatureSet in the buggy class from ASTbuggy. If the elements in fieldSet or methodSignatureSet can match with any of the ingredients in Ncontext, the corresponding AST node of such element is also added to Ncontext as contextual information. The rationale that motivated this inclusion is that generating a correct patch may require inspecting the status of the used public fields or the argument list of the invoked public methods. Finally, the algorithm outputs the sliced context statements concerning the slicing criterion (i.e., the buggy statement) as repair ingredients to generate the inputs for REPEATNPR. In the illustrated example of Fig.3, given the buggy statement S2, the algorithm extracts the sliced statements {S1, S3, S6, S7} as intra-procedural context and the public method
parseTagsForEdit invoked in S7 as global context. The extracted dependency context contains the repair ingredient "PREFIX_DEADLINE", thus making it more likely to generate the correct patch for fixing the buggy statement. 3) Context Statement Tokenization: At this stage, REPEATNPR prepares the collected context statements in such a manner that they can be directly fed into the DL models based on an encoder-decoder architecture. The input of REPEATNPR is composed of three parts: the buggy statement, the intra-procedural context, and the global context. Existing approaches [16], [42] attempt to encode different parts of the input separately and then fuse the encoding representations. However, it is still a challenge to effectively eliminate the semantic gaps between different encoders and merge the separated encoding vectors. Recently, the empirical results [49] indicate that the best strategy is to encode all of the different input parts using a single encoder. In light of this, REPEATNPR employs a consolidated format to combine the buggy statement and its context into one sequence with special isolated tokens. As shown in the lower left corner of Fig.3, REPEATNPR first separates each content token by a single whitespace. Then, different parts of the input are isolated by different abbreviate tokens (e.g., ‘‘<BOL>’’ denotes the beginning of a buggy statement). REPEATNPR also inserts a special token ‘‘<SEP>’’ for each sliced statement in the intra-procedural context to inform the end-of-line information. In addition, REPEATNPR utilizes the sentence-piece tokenizer [50] to alleviate the open-vocabulary problem [51]. In this manner, the low-frequency tokens can be synthesized more easily, thus making the program repair task more tractable. b. model training As illustrated in the right side of Fig.2, REPEATNPR takes the tokenized contextual sequences as input and works on three
different pieces of content: 1) the global context that contains information of public fields and method signatures; 2) the intra-procedural context sliced from the buggy method; 3) the buggy statement. We begin with the pre-trained model CodeT5 [18] serving as the reference architecture for the proposed REPEATNPR framework. Then, we perform the fine-tuning on the task of program repair. At the inference step, the decoder sequentially predicts the candidate patches. 1) Model Architecture: CodeT5 is a Seq2Seq language model pre-trained on the colossal clean crawled corpus, which consists of an encoder and a decoder. In the context of NPR, an encoder takes a sequence of code tokens as input to map the buggy context code X = [x1, . . . , xm] into a fixedlength hidden state, while the decoder generates the output sequence of tokens Y = [y1, . . . , yn] by taking the hidden state vector as input. Given an input sequence, CodeT5 first obtains the contextualized vector representations for input tokens by projecting them into an embedded vector space through the embedding and positional encoding layer. Then, the vector is fed into the encoder to capture the long-term dependencies from different perspectives of the input sequences. The encoder comprises a stack of Transformer layers, each of which contains a multi-head self-attention layer followed by a position-wise fully connected feed-forward network. The decoder in CodeT5 is a Transformer-based sequential left-toright decoder that generates one new token at a time until a special stop token is reached. The decoder is similar in structure to the encoder except for the usage of the mask mechanism in multi-head attention, which forces to attend only to past tokens and avoids distraction and information leakage of the subsequent tokens in training. 2) Fine-Tuning: To learn generic bug-fixing representations, we leverage the pre-trained model CodeT5, which has achieved remarkable performance on a variety of NLP and code-related SE tasks, as the starting point to train REPEATNPR. We then fine-tune the pre-trained model for the task of program repair. Fine-tuning techniques can optimize the pretrained parameters to make them more suitable for downstream tasks. Specifically, we represent the program repair task in a “text-to-text” format, where the input is the tokenized buggy contextual sequence, and the output is the expected humanwritten patch. The fine-tuning process is performed using the training corpus of BFP dataset D, and each instance within D can be formally represented as a bug-fixing pair di = {b, f}, where b = (c,m, s) comprises the global context obtained from the buggy class c, the intra-procedural context sliced from the buggy method m and the buggy statement s, and f denotes the human-written patch. The fine-tuning objective is to minimize the cross-entropy loss by learning the mapping b → f as a conditional probability p(f |b). 3) Patch Inference: After fine-tuning CodeT5 for the program repair task, REPEATNPR can learn latent bug-fixing patterns and generate correct patches. During inference, REPEATNPR represents the given buggy statement with three pieces of token sequence: one for the given statement, the other two for its context. REPEATNPR follows the typical NPR
process and employs beam search to generate the candidate patches sequentially. Once the decoder reaches the stop token, REPEATNPR outputs the top-ranked patches for the given buggy statement, where the number of generated candidate patches is configured as beam width. c. patch filtering The lower left corner of Fig.2 shows the procedure of patch filtering. To avoid the unaltered patching issue existing in the NPR task, REPEATNPR proposes an intuitive mechanism to filter out the unaltered patches generated by the pre-trained model CodeT5 for a given bug and then integrates a different NPR model into REPEATNPR to re-generate patches for the bug. As discussed in Section I, the unaltered patching issue frequently occurs when using DL-based models for the APR task since the buggy and fixed code snippets are highly similar in the context of fixing single-line bugs. Such an unsupervised phenomenon can be determined by simply comparing the model-generated patches with the input buggy code snippets. In another word, we can directly filter out a portion of the incorrect patches without knowing the human-written ground truth. The goal of patch filtering is to filter out the unaltered patches from the generated candidate patches of the former model and utilize a new NPR model to work on them. Specifically, given a buggy input, the candidate patches generated by CodeT5 in the first stage can be divided into three categories: correct patches (denoted as CPCodeT5), unaltered patches (denoted as UPCodeT5), and other incorrect patches that are inconsistent with the input buggy code snippet (denoted as IPCodeT5). Then, REPEATNPR filters out the UPCodeT5 and feeds them into another NPR model in the next stage, whereas the CPCodeT5 and IPCodeT5 remain as the final suggested patches. At last, the new NPR model will continue to fix the buggy code snippets among UPCodeT5 with the newlygenerated correct patches CPnewNPR. Thus, the correct patches produced by REPEATNPR can be formulated as follow:
CPRepeatNPR = CPCodeT5 + UPCodeT5 ∩ CPnewNPR iv. experimental setup  a. experimental subjects To evaluate REPEATNPR, we require a large-scale corpus of real-world bug-fixing pairs to fine-tune CodeT5. In this paper, we select the BFP dataset [11] as our original data source, which consists of 787k bug-fixing commits extracted from the GitHub repositories. Each instance is composed of both the buggy and fixed versions of a Java method. We follow the steps described in Section III-A to construct our adapted dataset. First, we utilize PROGEX [52], a crossplatform tool for extracting well-known graphical program representations from source code, to parse each Java method into a corresponding PDG for dependency context extraction. In this step, we filter out the instances that failed to be parsed by PROGEX, meanwhile, we truncate the instances whose length is longer than 512 after subword tokenization. Next, we further split the BFP dataset into training, validation, and
testing sets by the ratio of 8:1:1. The dataset split is performed carefully by taking into account possible data leakage. To be specific, any two instances belonging to the same GitHub repository cannot be put in two different sets (e.g., one in training and the other in testing). To enrich the diversity of benchmarks for evaluation, we add four benchmarks widely used in the APR task to our testing set and perform the same procedure of input processing. The detailed statistics for the established benchmarks are listed in Table I. In total, we collect 141195 bug-fixing pairs in the training set, 13523 in the validation set, and 13635 in the testing set. Note that each instance represents a single-line bug that can be fixed by using a single-line patch within one Java method. b. experimental design In this section, we introduce the selected baselines, implementation details, and how to assess the generated patches. 1) Baseline: To evaluate the performance of our proposed framework, we compare REPEATNPR with the following ten baselines that are related to our work: • CODIT [38]: a two-stage approach that learns code
changes for bug fixing by using a tree-based NMT model. • Edits [12]: an edit-based model that performs token-level
insertion and deletion operations on buggy code. • Tufano [11]: an RNN-based model that can translate
buggy code snippets into fixed ones. • Recoder [40]: a syntax-guided decoder to generate edits
on the AST of the buggy method. • CoCoNut [16]: an ensemble approach for fixing bugs
in multiple programming languages that combines CNNs and context-aware NMT models. • SEQUENCER [15]: a Seq2Seq learning-based approach for end-to-end program repair that employs the LSTM encoder-decoder architecture with a copy mechanism. • RoBERTa [57]: a Transformer-based model pre-trained on a large corpus of natural language texts in a selfsupervised fashion. • CodeBERT [44]: a bimodal pre-trained model for both programming and natural language that learns generalpurpose representations to support code-related SE tasks. • GraphCodeBERT [58]: a graph-based pre-trained model for the programming language that considers data-flow information along with code sequences. • CodeT5 [18]: a pre-trained encoder-decoder model that better leverages the code semantics conveyed from the developer-assigned identifiers. 2) Implementation: We implement REPEATNPR with the open-source framework PyTorch2 and initialize REPEATNPR with the pre-trained CodeT5-small checkpoint3 from the Huggingface’s website. We adopt the same architecture as T5 [59] model, consisting of 8-headed attention and 6 layers in both the encoder and decoder. We set the maximum source and target sequence lengths both to 512 and the batch size to 32. For the implementation of the selected baseline models, we use the publicly available source codes provided by the authors or the publicly released checkpoints. During the fine-tuning step, we train REPEATNPR for a maximum of 20 epochs. After each epoch, we compute the loss on the validation set and save the model with the minimum validation loss. To avoid over-fitting and save computation costs, we perform early stopping if the validation performance does not improve for five consecutive epochs. During the inference stage, we set both the beam size and candidate number to 10, which means that REPEATNPR will generate 10 top-ranked candidate patches for each perfectly fault-localized instance in the testing set for validation. Prior studies usually choose a larger candidate number for evaluation (e.g., Recoder [40] generates 100 candidate patches). However, recent work [60] shows that most developers are only willing to review up to 10 patches. Thus, we also report the evaluation results within the top-10 candidates generated by each baseline in this paper to draw fair conclusions. During the patch filtering step, we integrate SEQUENCER, which is considered the best NPR model in a recent empirical study [23], into REPEATNPR with our filter mechanism for patch re-generation. We conduct experiments on 4 Nvidia GTX 1080Ti GPUs of 12 GB memory. 3) Patch Assessment: Existing APR approaches typically use test suites for patch validation, which run the humanwritten test suite against each candidate patch to find plausible patches that can pass all the test cases. Due to the overfitting issue, such plausible patches need further manual assessment to confirm their correctness. However, the empirical results [61] indicate that manual patch assessment 1) needs expertise to understand the semantics of the program under repair, 2) may introduce biases to some extent, and 3) can be undoable when the scale is large. To avoid human bias and reduce manual effort, we use a more objective way in our experiment to assess the correctness of each generated candidate patch by checking if it is identical to the human-written one, that is, we use the exact match metric to evaluate the model performance on the testing set in this paper. v. results and analysis In this section, we present the experimental results for measuring the performance of our proposed framework and answering the following three research questions (RQs): • RQ1: How does REPEATNPR perform compared with
the state-of-the-art baselines? • RQ2: How does each component in REPEATNPR impact
the performance of bug fixing? 2https://pytorch.org 3https://huggingface.co/Salesforce/codet5-small
• RQ3: What is the quality of the candidate patches generated by REPEATNPR? a. answering rq1 To answer this question, we compare REPEATNPR with ten state-of-the-art baselines on five commonly used benchmarks. To make a fair comparison, we uniformly use the training set of BFP to train or fine-tune the baselines with corresponding input representations via the same training strategy and hyperparameter settings described in Section IV-B2. 1) Experimental Metric Evaluation: Table II reports the number of correct patches that are identical to the humanwritten ground truth generated by REPEATNPR and the ten baselines. The best result for each benchmark is marked in bold. As shown in Table II, REPEATNPR substantially outperforms the ten baselines on all five benchmarks. Specifically, REPEATNPR produces more correct patches than the best baseline model CodeT5 by 15.9% in the BFP benchmark, 12.0% in the Bugs.jar benchmark, 22.2% in the Defects4J benchmark, 62.5% in the Bears benchmark, 7.1% in the QuixBugs benchmark. As exact match is a strict metric, such improvements prove the superiority of REPEATNPR in the NPR task. We also observe that the pre-trained models are more promising than those trained from scratch. For instance, CodeT5 improves SEQUENCER by 14.3% in the BFP benchmark and 51.5% in the Bugs.jar benchmark. Since the two models have a similar architecture and a comparable amount of parameters, such improvements demonstrate that using pre-training techniques is advantageous for learning bugfixing patterns for patch generation. Additionally, the Seq2Seq learning-based models (i.e., REPEATNPR, CodeT5, and SEQUENCER) can obtain better results than other baselines, which illustrates the equal importance of correctly understanding the buggy input and generating the candidate patches. 2) The Impact of Fixing Rate under Different Candidate Numbers: We further investigate the fixing rates of each corresponding model under different candidate numbers on the BFP benchmark (12224 bugs). We set the candidate numbers from 1 to 10, and the fixing rate denotes the ratio of exact match predictions. Note that fixing one bug under candidate number k indicates that at least one of the k model-generated candidate patches is identical to the human-written ground truth. As it
can be seen in Fig.4, REPEATNPR consistently outperforms the ten baselines under all candidate numbers on the BFP benchmark. As the candidate number increases, we notice that the performance gains obtained by different NPR models may be inconsistent, even if they are trained with the unified dataset. For example, when the candidate number is limited to 1, Recoder relatively outperforms SEQUENCER by 21.4% in terms of the fixing rate. But when the candidate number reaches 10, the number of correct patches that SEQUENCER can generate is almost twice that of Recoder. 3) Answer to RQ1: In summary, REPEATNPR significantly outperforms the baselines in terms of the exact match metric. Such improvements demonstrate the effectiveness of REPEATNPR in the APR task. Our observations also indicate that REPEATNPR is capable of generating more correct patches under different candidate numbers than the baselines. b. answering rq2 To answer this question, we evaluate the impact of different components (i.e., program dependence analysis and filter mechanism) in the design of REPEATNPR by conducting ablation experiments. For a fair comparison, the training strategy and the hyper-parameter settings are consistent with those described in Section IV-B2. 1) Ablation Study: Table III lists the results with each row representing one model and the number of correct patches that such a model can generate for each benchmark. The best result is marked in bold. To show how each component improves the number of correctly generated patches, we start with the basic pre-trained model CodeT5, which is fine-tuned on the original BFP dataset without using program dependence analysis A and filter mechanism F . When we augment CodeT5 with component F , the ablated model CodeT5F respectively repairs 88, 6, 1, 3, and 0 more bugs for the five benchmarks. When we add component A to CodeT5, the number of correct patches generated by the ablated model CodeT5A for the five benchmarks is respectively 240, 8, 6, 7, and 1 more than that of CodeT5. Generally, the correct patches generated by the three ablated models are fewer than REPEATNPR, which demonstrates the necessity of each component. Figure 5
depicts a bug-fixing example from BFP only correctly patched by REPEATNPR. In this case, the predicate of the buggy statement is redundant according to its context. Thus, a correct patch needs to remove the entire body of the if statement. We can observe that the two ablated models without program dependence analysis (i.e., CodeT5 and CodeT5F ) generate the same incorrect patch that is semantically equivalent with the buggy statement, whereas CodeT5A generates an unaltered patch. Therefore, by further applying component F on CodeT5A, REPEATNPR can successfully produce the correct patch that is identical to the human-written one. 2) The Impact of Program Dependence Analysis on PreTrained Models: We further investigate the fixing rate of each
pre-trained model when using dependency context as input on the BFP benchmark. Table IV presents the comparison results with each pre-trained model comprising two lines of experimental results, in which the first line shows the results of the model that is fine-tuned without using the contextual information based on the program dependence analysis, and the second line shows the results of using such information. The Fix@k metric denotes that a correct patch for a given bug should be among the top-k generated ones. As shown in Table IV, we can observe that providing the dependency context as repair ingredients contributes to improving the bugfixing performance of all the pre-trained models. Among the four pre-trained models, CodeT5 gains the most performance improvements from the input contextual information. This is because CodeT5 is an encoder-decoder model that can leverage the code semantics conveyed from the developer-assigned identifiers to better derive generic representations, whereas the encoder-only pre-trained models (e.g., CodeBERT) treat the source code in the same way as natural language, neglecting the special characteristics of code. Figure 6 demonstrates a bug that can only be fixed by CodeT5A. As shown in Fig.6, the sliced global context provides a hint that variable stepNumber is used to track the current step, thus guiding CodeT5A to generate the correct patch. 3) The Impact of the Order and Quantity of Models on Filter Mechanism: We first analyze the impact of models’ order on the filter mechanism by comparing FCodeT5A+SRA and FSRA+CodeT5A (SR is short for SEQUENCER). Then, we further integrate a new NPR model GraphCodeBERTA (short as GCBA) into REPEATNPR to evaluate to what extent the models’ quantity impacts the performance of the filter mechanism. As it can be seen in Table V, the fixing rates decrease
simultaneously when exchanging the models’ order, which indicates that the order change may affect the performance of bug fixing. As for the models’ quantity, by comparing the results of CodeT5A, FCodeT5A+SRA and FCodeT5A+SRA+GCBA , we can see that the fixing rates can be improved or remain the same by integrating new NPR models into REPEATNPR. Nevertheless, the performance improvement has decreased with the increase in the models’ quantity. A possible reason may be that there is an overlap between the correct patches generated by each model. We will discuss this phenomenon in Section V-C2. 4) Answer to RQ2: To sum up, all components of REPEATNPR can contribute to performance improvement. Specifically, collecting the dependency context as repair ingredients can better exploit the powerful representation learning ability of the pre-trained model. Meanwhile, the proposed filter mechanism can further increase the number of correct patches by making use of the ensemble performance of different NPR models to eliminate the generated unaltered patches. c. answering rq3 To answer this question, we manually inspect the generated candidate patches for further evaluation. The evaluation is split into two aspects: 1) analyzing the bug types fixed in the generated correct patches; 2) discussing the overlapping phenomenon among the generated correct patches. 1) Bug Types Evaluation: We first compute the difference between two ASTs generated by Spoon [62] using the Gumtree algorithm [63]. In total, we classify the bugs into four types, that is Simple Delete, Simple Insert, Simple Replace, and Mixed, according to the edit operations needed to transform one buggy statement into its fixed version. Table VI lists the comparison results with each row representing one model and the number of correct patches that such a model can generate for each bug type on the BFP benchmark. The best result is marked in bold. The statistic results shown in Table VI indicate that existing NPR models do well in fixing bugs that only need deletion edit operations, but are weak in fixing the more complex ones. That makes sense because the insert and replace edit operations require the model to search for additional tokens to fix the given bug. Specifically, Recoder performs the best on generating code-removal patches, while REPEATNPR outperforms all the baselines in fixing the other
three types of bugs. Overall, REPEATNPR can achieve better or comparable performance than the baseline models in fixing both simple and complex bugs. 2) Overlapping Phenomenon Evaluation: As illustrated in Fig.7, each row lists the overlapping ratio of correct patches generated for the BFP benchmark between one NPR model and the other models, while the diagonal indicates the number of unique correct patches generated by each model. For instance, 52% of the correct patches generated by REPEATNPR (row 12) can also be fixed by SEQUENCER (column 7). And there are 390 bugs (row 12, column 12) that can only be fixed by REPEATNPR. According to the results in Fig.7, we observed that the better the repair performance of a model, the higher the overlapping patching rate between it and other models. Regarding the results in Table II, we can find that our proposed framework REPEATNPR, CodeT5, and SEQUENCER are the top three models evaluated on the BFP benchmark. Relatively, the overlapping rate of other models with the three ones is much higher. The possible reason for this phenomenon may be that the DL-based approaches mostly adopt similar network architectures and inference paradigms. Furthermore, the number of unique correct patches generated by REPEATNPR is larger than that of other baselines (except Recoder). As is discussed in Section II, the possible reason is that Recoder is designed to generate edits on the AST of the
buggy methods, which is different from the NMT- or Seq2Seq learning-based approaches. Nevertheless, regarding the results we obtained in Table VI, Recoder tends to fix bugs of a specific type (i.e., Simple Delete). vi. threats to validity In this section, we illustrate the main threats to the validity of our approach, which are listed as follows:
• External threat: The quality of the selected experimental subjects and the generalizability of REPEATNPR are the principal threats to external validity in this paper. First, we use the mainstream dataset BFP for fine-tuning as previous studies [11], [18], [23], [64]. We remove all duplicate instances between the training and testing sets to avoid the data leakage issue. Second, REPEATNPR has been evaluated in Java bugs. Besides, the designed components in REPEATNPR are language-agnostic and can be applied to other programming languages. • Internal threat: It is widely known that DL-based models are sensitive to hyper-parameters. Thus, using a suboptimal hyper-parameter can pose an internal threat to the validity of REPEATNPR. Due to the limitation of computational resources, we cannot thoroughly explore optimal hyper-parameters in this paper. Since Raffel et al. [59] have explored effective settings of hyper-parameters through extensive experiments in previous work, we use the same hyper-parameters described in their paper. We acknowledge that there might be room for further improvement through additional tuning. • Construct threat: In this paper, the experimental metric used for model evaluation is referred to as the construct threat. We adopt the exact match metric to assess the correctness of the generated candidate patches. Although such a metric does not represent human judgment, it is a strict and objective metric that can be used to quickly and quantitatively evaluate the model performance. In the future, we will conduct more human evaluations. vii. conclusion and future work In this paper, we propose a novel NPR framework called REPEATNPR that adapts a state-of-the-art encoder-decoder language model for fixing single-line Java bugs. To accurately capture the semantic relationship between the buggy statement and its context, we make the first attempt to leverage program dependence analysis to improve the NPR task by extracting slicing-based contextual information as repair ingredients from PDG. Additionally, we propose an intuitive but effective filter mechanism to eliminate the unaltered patches by taking advantage of the ensemble performance of different NPR models. Empirical results demonstrate that REPEATNPR outperforms the state-of-the-art approaches on five widely used benchmarks in terms of the exact match metric. Furthermore, we will design a better filter mechanism that can determine more incorrect patches (except the unaltered ones) without comparing them with the humanwritten ground truth.