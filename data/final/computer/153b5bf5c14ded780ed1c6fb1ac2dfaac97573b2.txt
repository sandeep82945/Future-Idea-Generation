Individual preference (IP) stability, introduced by Ahmadi et al. (ICML 2022), is a natural clustering objective inspired by stability and fairness constraints. A clustering is α-IP stable if the average distance of every data point to its own cluster is at most α times the average distance to any other cluster. Unfortunately, determining if a dataset admits a 1-IP stable clustering is NP-Hard. Moreover, before this work, it was unknown if an o(n)-IP stable clustering always exists, as the prior state of the art only guaranteed an O(n)-IP stable clustering. We close this gap in understanding and show that an O(1)-IP stable clustering always exists for general metrics, and we give an efficient algorithm which outputs such a clustering. We also introduce generalizations of IP stability beyond average distance and give efficient, near-optimal algorithms in the cases where we consider the maximum and minimum distances within and between clusters. Supported by DFF-International Postdoc Grant 0164-00022B from the Independent Research Fund Denmark. Supported in part by an NSF Graduate Research Fellowship and a Hertz Fellowship Significant part of works was done while P.S. was a Ph.D. candidate at Northwestern University 1 ar X iv :2 30 9. 16 84 0v 1 [ cs .D S] 2 8 Se p 20 23 1 introduction In applications involving and affecting people, socioeconomic concepts such as game theory, stability, and fairness are important considerations in algorithm design. Within this context, Ahmadi et al. [AAK+22] introduced the notion of individual preference stability (IP stability) for clustering. At a high-level, a clustering of an input dataset is called IP stable if, for each individual point, its average distance to any other cluster is larger than the average distance to its own cluster. Intuitively, each individual prefers its own cluster to any other, and so the clustering is stable. There are plenty of applications of clustering in which the utility of each individual in any cluster is determined according to the other individuals who belong to the same cluster. For example, in designing personalized medicine, the more similar the individuals in each cluster are, the more effective medical decisions, interventions, and treatments can be made for each group of patients. Stability guarantees can also be used in personalized learning environments or marketing campaigns to ensure that no individual wants to deviate from their assigned cluster. Furthermore, the focus on individual utility in IP stability (a clustering is only stable if every individual is “happy”) enforces a sort of individual fairness in clustering. In addition to its natural connections to cluster stability, algorithmic fairness, and Nash equilibria, IP stability is also algorithmically interesting in its own right. While clustering is well-studied with respect to global objective functions (e.g. the objectives of centroid-based clustering such as k-means or correlation/hierarchical clustering), less is known when the goal is to partition the dataset such that every point in the dataset is individually satisfied with the solution. Thus, IP stability also serves as a natural and motivated clustering framework with a non-global objective. 1.1 problem statement and preliminaries The main objective of our clustering algorithms is to achieve IP stability given a set P of n points lying in a metric space (M,d) and k, the number of clusters. Definition 1.1 (Individual Preference (IP) Stability [AAK+22]). The goal is to find a disjoint k-clustering C = (C1, · · · , Ck) of P such that every point, on average, is closer to the points of its own cluster than to the points in any other cluster. Formally, for all v ∈ P , let C(v) denote the cluster that contains v. We say that v ∈ P is IP stable with respect to C if either C(v) = {v} or for every C ′ ∈ C with C ′ ̸= C(v),
1 |C(v)| − 1 ∑
u∈C(v)
d(v, u) ≤ 1 |C ′| ∑ u∈C′ d(v, u). (1)
The clustering C is 1-IP stable (or simply IP stable) if and only if every v ∈ P is stable with respect to C.
Ahmadi et al. [AAK+22] showed that an arbitrary dataset may not admit an IP stable clustering. This can be the case even when n = 4. Furthermore, they proved that it is NP-hard to decide whether a given a set of points have an IP stable k-clustering, even for k = 2. This naturally motivates the study of the relaxations of IP stability. Definition 1.2 (Approximate IP Stability). A k-clustering C = (C1, · · · , Ck) of P is α-approximate IP stable, or simply α-IP stable, if for every point v ∈ P , the following holds: either C(v) = {v} or
for every C ′ ∈ C and C ′ ̸= C,
1 |C(v)| − 1 ∑
u∈C(v)
d(v, u) ≤ α |C ′| ∑ u∈C′ d(v, u). (2)
The work of [AAK+22] proposed algorithms to outputting IP stable clusterings on the onedimensional line for any value of k and on tree metrics for k = 2. The first result implies an O(n)-IP stable clustering for general metrics, by applying a standard O(n)-distortion embedding to one-dimensional Euclidean space. In addition, they give a bicriteria approximation that discards an ε-fraction of the input points and outputs an O ( log2 n
ε
) -IP stable clustering for the remaining
points. Given the prior results, it is natural to ask if the O(n) factor for IP stable clustering given in [AAK+22] can be improved. 1.2 our results New Approximations. Improving on the O(n)-IP stable algorithm in [AAK+22], we present a deterministic algorithm which for general metrics obtains an O(1)-IP stable k-clustering, for any value of k. Note that given the existence of instances without 1-IP stable clusterings, our approximation factor is optimal up to a constant factor. Theorem 1.3. (Informal; see Theorem 3.1) Given a set P of n points in a metric space (M,d) and a number of desired clusters k ≤ n, there exists an algorithm that computes an O(1)-IP stable k-clustering of P in polynomial time. Our algorithm outputs a clustering with an even stronger guarantee that we call uniform (approximate) IP stability. Specifically, for some global parameter r and for every point v ∈ P , the average distance from v to points in its own cluster is upper bounded by O(r) and the average distance from v to points in any other cluster is lower bounded by Ω(r). Note that the general condition of O(1)-IP stability would allow for a different value of r for each v.
We again emphasize that Theorem 1.3 implies that an O(1)-IP stable clustering always exists, where prior to this work, only the O(n) bound from [AAK+22] was known for general metrics. Additional k-Center Clustering Guarantee. The clustering outputted by our algorithm satisfies additional desirable properties beyond O(1)-IP stability. In the k-center problem, we are given n points in a metric space, and our goal is to pick k centers as to minimize the maximal distance of any point to the nearest center. The clustering outputted by our algorithm from Theorem 1.3 has the added benefit of being a constant factor approximation to the k-center problem in the sense that if the optimal k-center solution has value r0, then the diameter of each cluster outputted by the algorithm is O(r0). In fact, we argue that IP stability is more meaningful when we also seek a solution that optimizes some clustering objective. If we only ask for IP stability, there are instances where it is easy to obtain O(1)-IP stable clusterings, but where such clusterings do not provide insightful information in a typical clustering application. Indeed, as we will show in Appendix B, randomly k-coloring the nodes of an unweighted, undirected graph (where the distance between two nodes is the number of edges on the shortest path between them), gives an O(1)-IP stable clustering when k ≤ O ( √ n
logn
) . Our result on trees demonstrates the idiosyncrasies of individual objectives thus
our work raises further interesting questions about studying standard global clustering objectives under the restriction that the solutions are also (approximately) IP stable. Max and Min-IP Stability. Lastly, we introduce a notion of f -IP stability, generalizing IP stability. Definition 1.4 (f -IP Stability). Let (M,d) be a metric space, P a set of n points of M , and k the desired number of partitions. Let f : P × 2P → R≥0 be a function which takes in a point v ∈ P , a subset C of P , and outputs a non-negative real number. we say that a k-clustering C = (C1, · · · , Ck) of P is f -IP stable if for every point v ∈ P , the following holds: either C(v) = {v} or for every C ′ ∈ C and C ′ ̸= C,
f (v, C(v) \ {v}) ≤ f ( v, C ′ ) . (3)
Note that the standard setting of IP stability given in Definition 1.1 corresponds to the case where f(v, C) = (1/|C|)× ∑ v′∈C d(v, v
′). The formulation of f -IP stability, therefore, extends IP stability beyond average distances and allows for alternative objectives that may be more desirable in certain settings. For instance, in hierarchical clustering, average, minimum, and maximum distance measures are well-studied. In particular, we focus on max-distance and min-distance in the definition of f -IP stable clustering in addition to average distance (which is just Definition 1.1), where f(v, C) = maxv′∈C d(v, v′) and f(v, C) = minv′∈C d(v, v
′). We show that in both the max and min distance formulations, we can solve the corresponding f -IP stable clustering (nearly) optimally in polynomial time. We provide the following result:
Theorem 1.5 (Informal; see Theorem 4.1 and Theorem 4.2). In any metric space, Min-IP stable clustering can be solved optimally and Max-IP stable clustering can be solved approximately within a factor of 3, in polynomial time. We show that the standard greedy algorithm of k-center, a.k.a, the Gonzalez’s algorithm [Gon85], yields a 3-approximate Max-IP stable clustering. Moreover, we present a conceptually clean algorithm which is motivated by considering the minimum spanning tree (MST) to output a Min-IP stable clustering. This implies that unlike the average distance formulation of IP stable clustering, a Min-IP stable clustering always exists. Both algorithms work in general metrics. Empirical Evaluations. We experimentally evaluate our O(1)-IP stable clustering algorithm against k-means++, which is the empirically best-known algorithm in [AAK+22]. We also compare k-means++ with our optimal algorithm for Min-IP stability. We run experiments on the Adult
data set1 used by [AAK+22]. For IP stability, we also use four more datasets from UCI ML repositoriy [DG19] and a synthetic data set designed to be a hard instance for k-means++. On the Adult data set, our algorithm performs slightly worse than k-means++ for IP stability. This is consistent with the empirical results of [AAK+22]. On the hard instance2, our algorithm performs better than k-means++, demonstrating that the algorithm proposed in this paper is more robust than k-means++. Furthermore for Min-IP stability, we empirically demonstrate that k-means++ can have an approximation factors which are up to a factor of 5x worse than our algorithm. We refer to Section 5 and Appendix C for more details. 1.3 technical overview The main contribution is our O(1)-approximation algorithm for IP stable clustering for general metrics. We discuss the proof technique used to obtain this result. Our algorithm comprises two steps. We first show that for any radius r, we can find a clustering C = (C1, . . . , Ct) such that (a) each cluster has diameter O(r), and (b) the average distance from a point in a cluster to the points of any other cluster is Ω(r). Conditions (a) and (b) are achieved through a ball carving technique, where we iteratively pick centers qi of distance > 6r to previous centers such that the radius r ball B(qi, r) centered at qi contains a maximal number of points, say si. For each of these balls, we initialize a cluster Di containing the si points of B(qi, r). We next consider the annulus B(qi, 3r)\B(qi, 2r). If this annulus contains less than si points, we include all points from B(qi, 3r) in Di. Otherwise, we include any si points in Di from the annulus. We assign each unassigned point to the first center picked by our algorithm and is within distance O(r) to the point. This is a subtle but crucial component of the algorithm as the more natural “assign to the closest center” approach fails to obtain O(1)-IP stability. One issue remains. With this approach, we have no guarantee on the number of clusters. We solve this by merging some of these clusters while still maintaining that the final clusters have radius O(r). This may not be possible for any choice of r. Thus the second step is to find the right choice of r. We first run the greedy algorithm of k-center and let r0 be the minimal distance between centers we can run the ball carving algorithm r = cr0 for a sufficiently small constant c < 1. Then if we assign each cluster of C to its nearest center among those returned by the greedy algorithm k-center, we do indeed maintain the property that all clusters have diameter O(r), and since c is a small enough constant, all the clusters will be non-empty. The final number of clusters will therefore be k. As an added benefit of using the greedy algorithm for k-center as a subroutine, we obtain that the diameter of each cluster is also O(r0), namely the output clustering is a constant factor approximation to k-center. 1.4 related work Fair Clustering. One of the main motivations of IP stable clustering is its interpretation as a notion of individual fairness for clustering [AAK+22]. Individual fairness was first introduced by [DHP+12] for the classification task, where, at high-level, the authors aim for a classifier that gives “similar predictions” for “similar” data points. Recently, other formulations of individual fairness have been studied for clustering [JKL20, ABDL20, BCD+20, CDE+22, KKM+23], too. [JKL20]
1https://archive.ics.uci.edu/ml/datasets/adult; see [Koh96]. 2The construction of this hard instance is available in the appendix of [AAK+22]. proposed a notion of fairness for centroid-based clustering: given a set of n points P and the number of clusters k, for each point, a center must be picked among its (n/k)-th closest neighbors. The optimization variant of it was later studied by [MV20, NC21, VY22]. [BCD+20] studied a pairwise notion of fairness in which data points represent people who gain some benefit from being clustered together. In a subsequent work, [BCD+21] introduced a stochastic variant of this notion. [ABDL20] studied the setting in which the output is a distribution over centers and “similar” points are required to have “similar” centers distributions. Stability in Clustering. Designing efficient clustering algorithms under notions of stability is a well-studied problem3. Among the various notion of stability, average stability is the most relevant to our model [BBV08]. In particular, they showed that if there is a ground-truth clustering satisfying the requirement of Equation (1) with an additive gap of γ > 0, then it is possible to recover the solution in the list model where the list size is exponential in 1/γ. Similar types of guarantees are shown in the work by [DLS12]. While this line of research mainly focuses on presenting faster algorithms utilizing the strong stability conditions, the focus of IP stable clustering is whether we can recover such stability properties in general instances, either exactly or approximately. Hedonic Games. Another game-theoretic study of clustering is hedonic games [DG80, BJ02, EFF20]. In a hedonic game, players choose to form coalitions (i.e., clusters) based on their utility. Our work differs from theirs, since we do not model the data points as selfish players. In a related work, [SP18] proposes another utility measure for hedonic clustering games on graphs. In particular, they define a closeness utility, where the utility of node i in cluster C is the ratio between the number of nodes in C adjacent to i and the sum of distances from i to other nodes in C. This measure is incomparable to IP stability. In addition, their work focuses only on clustering in graphs while we consider general metrics. 2 preliminaries and notations We let (M,d) denote a metric space, where d is the underlying distance function. We let P denote a fixed set of points of M . Here P may contain multiple copies of the same point. For a given point x ∈ P and radius r ≥ 0, we denote by B(x, r) = {y ∈ P | d(x, y) ≤ r}, the ball of radius r centered at x. For two subsets X,Y ⊆ P , we denote by d(X,Y ) = infx∈X,y∈Y d(x, y). Throughout the paper, X and Y will always be finite and then the infimum can be replaced by a minimum. For x ∈ P and Y ⊆ P , we simply write d(x, Y ) for d({x}, Y ). Finally, for X ⊆ P , we denote by diam(X) = supx,y∈X d(x, y), the diameter of the set X. Again, X will always be finite, so the supremum can be replaced by a maximum. 3 constant-factor ip stable clustering In this section, we prove our main result: For a set P = {x1, . . . , xn} of n points with a metric d and every k ≤ n, there exists a k-clustering C = (C1, . . . , Ck) of P which is O(1)-approximate IP stable. Moreover, such a clustering can be found in time Õ(n2T ), where T is an upper bound on the time it takes to compute the distance between two points of P . 3For a comprehensive survey on this topic, refer to [AB14]. Algorithm Our algorithm uses a subroutine, Algorithm 1, which takes as input P and a radius r ∈ R and returns a t-clustering D = (D1, . . . , Dt) of P with the properties that (1) for any 1 ≤ i ≤ t, the maximum distance between any two points of Di is O(r), and (2) for any x ∈ P and any i such that x /∈ Di, the average distance from x to points of Di is Ω(r). These two properties ensure that D is O(1)-approximate IP stable. However, we have no control on the number of clusters t that the algorithm produces. To remedy this, we first run a greedy k-center algorithm on P to obtain a set of centers {c1, . . . , ck} and let r0 denote the maximum distance from a point of P to the nearest center. We then run Algorithm 1 with input radius r = cr0 for some small constant c. This gives a clustering D = (D1, . . . , Dt) where t ≥ k. Moreover, we show that if we assign each cluster of D to the nearest center in {c1, . . . , ck} (in terms of the minimum distance from a point of the cluster to the center), we obtain a k-clustering C = (C1, . . . , Ck) which is O(1)-approximate IP stable. The combined algorithm is Algorithm 2. Algorithm 1 Ball-Carving 1: Input: A set P = {x1, . . . , xn} of n points with a metric d and a radius r > 0. 2: Output: Clustering D = (D1, . . . , Dt) of P . 3: Q← ∅, i← 1 4: while there exists x ∈ P with d(x,Q) > 6r do 5: qi ← argmaxx∈P :d(x,Q)>6r |B(x, r)| 6: Q← Q ∪ {qi}, si ← |B(qi, r)|, Ai ← B(qi, 3r) \B(qi, 2r) 7: if |Ai| ≥ si 8: Si ← any set of si points from Ai 9: Di ← B(qi, r) ∪ Si
10: else Di ← B(qi, 3ri) 11: i← i+ 1 12: end while 13: for x ∈ P assigned to no Di do 14: j ← min{i | d(x, qi) ≤ 7r} 15: Dj ← Dj ∪ {x} 16: end for 17: t← |Q| 18: return D = (D1, . . . , Dt)
We now describe the details of Algorithm 1. The algorithm takes as input n points x1, . . . , xn of a metric space (M,d) and a radius r. It first initializes a set Q = ∅ and then iteratively adds points x from P to Q that are of distance greater than 6r from points already in Q such that |B(x, r)|, the number of points of P within radius r of x, is maximized. This is line 5–6 of the algorithm. Whenever a point qi is added to Q, we define the annulus Ai := B(qi, 3r) \B(qi, 2r). We further let si = |B(qi, r)|. At this point the algorithm splits into two cases. • If |Ai| ≥ si, we initialize a cluster Di which consists of the si points in B(x, r) and any arbitrarily chosen si points in Ai. This is line 8–9 of the algorithm. • If, on the other hand, |Ai| < s, we define Di := B(qi, 3r), namely Di contains all points of P within distance 3r from qi. This is line 10 of the algorithm. Algorithm 2 IP-Clustering 1: Input: Set P = {x1, . . . , xn} of n points with a metric d and integer k with 2 ≤ k ≤ n. 2: Output: k-clustering C = (C1, . . . , Ck) of P . 3: S ← ∅ 4: for i = 1, . . . , k do 5: ci ← argmaxx∈P {d(x, S)} 6: S ← S ∪ {ci}, Ci ← {ci} 7: end for 8: r0 ← min{d(ci, cj) | 1 ≤ i < j ≤ k} 9: D ← Ball-Carving(P, r0/15)
10: for D ∈ D do 11: j ← argmini{d(ci, D)} 12: Cj ← Cj ∪D 13: end for 14: return C = (C1, . . . , Ck)
After iteratively picking the points qi and initializing the clusters Di, we assign the remaining points as follows. For any point x ∈ P \ ⋃ iDi, we find the minimum i such that d(x, qi) ≤ 7r and assign x to Di. This is line 13–16 of the algorithm. We finally return the clustering D = (D1, . . . , Dt). We next describe the details of Algorithm 2. The algorithm iteratively pick k centers c1, . . . , ck from P for each center maximizing the minimum distance to previously chosen centers. For each center ci, it initializes a cluster, starting with Ci = {ci}. This is line 4–7 of the algorithm. Letting r0 be the minimum distance between pairs of distinct centers, the algorithm runs Algorithm 1 on P with input radius r = r0/15 (line 8–9). This produces a clustering D. In the final step, we iterate over the clusters D of D, assigning D to the Ci for which d(ci, D) is minimized (line 11–13). We finally return the clustering (C1, . . . , Ck). Analysis We now analyze our algorithm and provide its main guarantees. Theorem 3.1. Algorithm 2 returns an O(1)-approximate IP stable k clustering in time O(n2T + n2 log n). Furthermore, the solution is also a constant factor approximation to the k-center problem. In order to prove this theorem, we require the following lemma on Algorithm 1. Lemma 3.2. Let (D1, . . . , Dt) be the clustering output by Algorithm 1. For each i ∈ [t], the diameter of Di is at most 14r. Further, for x ∈ Di and j ̸= i, the average distance from x to points of Dj is at least r4 . Given Lemma 3.2, we can prove the the main result. Proof of Theorem 3.1. We first argue correctness. As each ci was chosen to maximize the minimal distance to points cj already in S, for any x ∈ P , it holds that min{d(x, ci) | i ∈ [k]} ≤ r0. By Lemma 3.2, in the clustering D output by Ball-Carving(P, r0/15) each cluster has diameter at most 1415r0 < r0, and thus, for each i ∈ [k], the cluster D ∈ D which contains ci will be included in Ci in the final clustering. Indeed, in line 11 of Algorithm 2, d(ci, D) = 0 whereas d(cj , D) ≥ 115r0 for all j ≠ i. Thus, each cluster in (C1, . . . , Ck) is non-empty. Secondly, the diameter of each cluster
is at most 4r0, namely, for each two points x, x′ ∈ Ci, they are both within distance r0 + 1415r0 < 2r0 of ci. Finally, by Lemma 3.2, for x ∈ Di and j ̸= i, the average distance from x to points of Dj is at least r060 . Since, C is a coarsening of D, i.e., each cluster of C is the disjoint union of some of the clusters in D, it is straightforward to check that the same property holds for the clustering C. Thus C is O(1)-approximate IP stable. We now analyze the running time. We claim that Algorithm 2 can be implemented to run in O(n2T + n2 log n) time, where T is the time to compute the distance between any two points in the metric space. First, we can query all pairs to form the n× n distance matrix A. Then we sort A along every row to form the matrix A′. Given A and A′, we easily implement our algorithms as follows. First, we argue about the greedy k-center steps of Algorithm 2, namely, the for loop on line 4. The most straightforward implementation computes the distance from every point to new chosen centers. At the end, we have computed at most nk distances from points to centers which can be looked up in A in time O(nk) = O(n2) as k ≤ n. In line 8, we only look at every entry of A at most once so the total time is also O(n2). The same reasoning also holds for the for loop on line 10. It remains to analyze the runtime. Given r, Algorithm 1 can be implemented as follows. First, we calculate the size of |B(x, r)| for every point x in our dataset. This can easily be done by binary searching on the value of r along each of the (sorted) rows of A′, which takes O(n log n) time in total. We can similarly calculate the sizes of |B(x, 2r)| and |B(x, 3r)|, and thus the number of points in the annulus |B(x, 3r) \B(x, 2r)| in the same time to initialize the clusters Di. Similar to the k-center reasoning above, we can also pick the centers in Algorithm 1 which are > 6r apart iteratively by just calculating the distances from points to the chosen centers so far. This costs at most O(n2) time, since there are at most n centers. After initializing the clusters Di, we finally need to assign the remaining unassigned points (line 13–16). This can easily be done in time O(n) per point, namely for each unassigned point x, we calculate its distance to each qi assigning it to Di where i is minimal such that d(x, qi) ≤ 7r. The total time for this is then O(n2). The k-center guarantees follow from our choice of r0 and Lemma 3.2. Remark 3.3. We note that the runtime can possibly be improved if we assume special structure about the metric space (e.g., Euclidean metric). See Appendix A for a discussion. We now prove Lemma 3.2. Proof of Lemma 3.2. The upper bound on the diameter of each cluster follows from the fact that for any cluster Di in the final clustering D = {D1, . . . , Dt}, and any x ∈ Di, it holds that d(x, qi) ≤ 7r. The main challenge is to prove the lower bound on the average distance from x ∈ Di to Dj where j ̸= i. Suppose for contradiction that, there exists i, j with i ̸= j and x ∈ Di such that the average distance from x to Dj is smaller than r/4, i.e., 1|Dj | ∑ y∈Dj d(x, y) < r/4. Then, it in particular holds that |B(x, r/2) ∩Dj | > |Dj |/2, namely the ball of radius r/2 centered at x contains more than half the points of Dj . We split the analysis into two cases corresponding to the if-else statements in line 7–10 of the algorithm. Case 1: |Aj | ≥ sj: In this case, cluster Dj consists of at least 2sj points, namely the sj points in B(qj , r) and the set Sj of sj points in Aj assigned to Dj in line 8–9 of the algorithm. It follows from
the preceding paragraph that, |B(x, r/2)∩Dj | > sj . Now, when qj was added to Q, it was chosen as to maximize the number of points in B(qj , r) under the constraint that qj had distance greater than 6r to previously chosen points of Q. Since |B(x, r)| ≥ |B(x, r/2)| > |B(qj , r)|, at the point where qj was chosen, Q already contained some point qj0 (with j0 < j) of distance at most 6r to x and thus of distance at most 7r to any point of B(x, r/2). It follows that B(x, r/2) ∩Dj contains no point assigned during line 13– 16 of the algorithm. Indeed, by the assignment rule, such a point y would have been assigned to either Dj0 or potentially an even earlier initialized cluster of distance at most 7r to y. Thus, B(x, r/2) ∩Dj is contained in the set B(qj , r) ∪ Sj . However, |B(qj , r)| = |Sj | = sj and moreover, for (y1, y2) ∈ B(qj , r)× Sj , it holds that d(y1, y2) > r. In particular, no ball of radius r/2 can contain more than sj points of B(qj , r)∪Sj . As |B(x, r/2)∩Dj | > sj , this is a contradiction. Case 2: |Aj | < sj: In this case, Dj includes all points in B(qj , 3r). As x /∈ Dj , we must have that x /∈ B(qj , 3r) and in particular, the ball B(x, r/2) does not intersect B(qj , r). Thus,
|Dj | ≥ |B(x, r/2) ∩Dj |+ |B(qj , r) ∩Dj | > |Dj |/2 + sj ,
so |Dj | > 2sj , and finally, |B(x, r/2) ∩ Dj | > |Dj |/2 > sj . Similarly to case 1, B(x, r/2) ∩ Dj contains no points assigned during line 13– 16 of the algorithm. Moreover, B(x, r/2)∩B(qj , 3r) ⊆ Aj . In particular, B(x, r/2) ∩Dj ⊆ Sj , a contradiction as |Sj | = sj but |B(x, r/2) ∩Dj | > sj . 4 min and max-ip stable clustering The Min-IP stable clustering aims to ensure that for any point x, the minimum distance to a point in the cluster of x is at most the minimum distance to a point in any other cluster. We show that a Min-IP stable k-clustering always exists for any value of k ∈ [n] and moreover, can be found by a simple algorithm (Algorithm 3). Algorithm 3 Min-IP-Clustering 1: Input: Pointset P = {x1, . . . , xn} from a metric space (M,d) and integer k with 2 ≤ k ≤ n. 2: Output: k-clustering C = (C1, . . . , Ck) of P . 3: L← {(xi, xj)}1≤i<j≤n sorted according to d(xi, xj) 4: E ← ∅ 5: while G = (P,E) has > k connected components do 6: e← an edge e = (x, y) in L with d(x, y) minimal. 7: L← L \ {e} 8: if e connects different connected components of G then E ← E ∪ {e} 9: end while
10: return the connected components (C1, . . . , Ck) of G.
The algorithm is identical to Kruskal’s algorithm for finding a minimum spanning tree except that it stops as soon as it has constructed a forest with k connected components. First, it initializes a graph G = (V,E) with V = P and E = ∅. Next, it computes all distances d(xi, xj) between pairs of points (xi, xj) of P and sorts the pairs (xi, xj) according to these distances. Finally, it goes through this sorted list adding each edge (xi, xj) to E if it connects different connected components of G. After computing the distances, it is well known that this algorithm can be made to run in
time O(n2 log n), so the total running time is O(n2(T + log n)) where T is the time to compute the distance between a single pair of points. Theorem 4.1. The k-clustering output by Algorithm 3 is a Min-IP stable clustering. Proof. Let C be the clustering output by the algorithm. Conditions (1) and (2) in the definition of a min-stable clustering are trivially satisfied. To prove that (3) holds, let C ∈ C with |C| ≥ 2 and x ∈ C. Let y0 ̸= x be a point in C such that (x, y0) ∈ E (such an edge exists because C is the connected component of G containing x) and let y1 be the closest point to x in P \ C. When the algorithm added (x, y0) to E, (x, y1) was also a candidate choice of an edge between connected components of G. Since the algorithm chose the edge of minimal length with this property, d(x, y0) ≤ d(x, y1). Thus, we get the desired bound:
min y∈C\{x} d(x, y) ≤ d(x, y0) ≤ d(x, y1) = min y∈P\C d(x, y). Theorem 4.2. The solution output by the greedy algorithm of k-center is a 3-approximate Max-IP stable clustering. Proof. To recall, the greedy algorithm of k-center (aka Gonzalez algorithm [Gon85]) starts with an arbitrary point as the first center and then goes through k− 1 iterations. In each iteration, it picks a new point as a center which is furthest from all previously picked centers. Let c1, · · · , ck denote the selected centers and let r := maxv∈P d(v, {c1, · · · , ck}). Then, each point is assigned to the cluster of its closest center. We denote the constructed clusters as C1, · · · , Ck. Now, for every i ̸= j ∈ [k] and each point v ∈ Ci, we consider two cases:
• d(v, ci) ≤ r/2. Then
max ui∈Ci
d(v, ui) ≤ d(v, ci) + d(ui, ci) ≤ 3r/2,
max uj∈Cj
d(v, uj) ≥ d(v, cj) ≥ d(ci, cj)− d(v, ci) ≥ r/2. • d(v, ci) > r/2. Then
max ui∈Ci
d(v, ui) ≤ d(v, ci) + d(ui, ci) ≤ 3d(v, ci),
max uj∈Cj
d(v, uj) ≥ d(v, cj) ≥ d(v, ci). In both cases, maxui∈Ci d(v, ui) ≤ 3maxuj∈Cj d(v, uj). 5 experiments While the goal and the main contributions of our paper are mainly theoretical, we also implement our optimal Min-IP clustering algorithm as well as extend the experimental results for IP stable clustering given in [AAK+22]. Our experiments demonstrate that our optimal Min-IP stable clustering algorithm is superior to k-means++, the strongest baseline in [AAK+22], and show that our IP clustering algorithm for average distances is practical on real world datasets and is competitive to k-means++ (which fails to find good stable clusterings in the worst case [AAK+22]). We give our experimental results for Min-IP stability and defer the rest of the empirical evaluations to Section C. All experiments were performed in Python 3. The results shown below are an average of 10 runs for k-means++. Metrics We measure the quality of a clustering using the same metrics used in [AAK+22] for standardization. Considering the question of f -IP stability (Definition 1.4), let the violation of a point x be defined as Vi(x) = maxCi ̸=C(x) f(x,C(x)\{x}) f(x,Ci) . For example, setting f(x,C) = ∑
y∈C d(x, y)/|C| corresponds to the standard IP stability objective and f(x,C) = miny∈C d(x, y) is the Min-IP formulation. Note point x is stable iff Vi(x) ≤ 1. We measure the extent to which a k-clustering C = (C1, . . . , Ck) of P is (un)stable by computing MaxVi = maxx∈P Vi(x) (maximum violation) and MeanVi = ∑ x∈P Vi(x)/|P | (mean violation). Results For Min-IP stability, we have an optimal algorithm; it always returns a stable clustering for all k. We see in Figures 1 that for the max and mean violation metrics, our algorithm outperforms k-means++ by up to a factor of 5x, consistently across various values of k. k-means ++ can return a much worse clustering under Min-IP stability on real data, motivating the use of our theoretically-optimal algorithm in practice. 6 conclusion We presented a deterministic polynomial time algorithm which provides an O(1)-approximate IP stable clustering of n points in a general metric space, improving on prior works which only guaranteed an O(n)-approximate IP stable clustering. We also generalized IP stability to f -stability and provided an algorithm which finds an exact Min-IP stable clustering and a 3-approximation for Max-IP stability, both of which hold for all k and in general metric spaces. Is there an efficient algorithm which returns an O(α∗)-IP stable clustering? • For what specific metrics (other than the line or tree metrics with k = 2) can we get 1-IP stable clusterings efficiently? a discussion on the run-time of algorithm 2 We remark that the runtime of our O(1)-approximate IP-stable clustering algorithm can potentially be improved if we assume special structure about the metric space, such as a tree or Euclidean metric. In special cases, we can improve the running time by appealing to particular properties of the metric which allow us to either calculate distances or implement our subroutines faster. For example for tree metrics, all distances can be calculated in O(n2) time, even though T = O(n). Likewise for the Euclidean case, we can utilize specialized algorithms for computing the all pairs distance matrix, which obtain speedups over the naive methods [IS22], or use geometric point location data structures to quickly compute quantities such as |B(x, r)| [Sno04]. Our presentation is optimized for simplicity and generality so detailed discussions of specific metric spaces are beyond the scope of the work. b random clustering in unweighted graphs In this appendix, we show that for unweigthed, undirected, graphs (where the distance d(u, v) between two vertices u and v is the length of the shortest path between them), randomly k-coloring the nodes gives an O(1)-approximate IP-stable clustering whenever k = O(n1/2/ log n). We start with the following lemma. Lemma B.1. Let γ = O(1) be a constant. There exists a constant c > 0 (depending on γ) such that the following holds: Let T = (V,E) be an unweighted tree on n nodes rooted at vertex r. Suppose that we randomly k-color the nodes of T . Let Vi ⊆ V be the nodes of color i, let Xi = ∑ v∈Vi d(r, v), and
let X = ∑ v∈V d(r, v). If k ≤ c √ n logn , then with probability 1−O(n −γ), it holds that X/2 ≤ kXi ≤ 2X for all i ∈ [k]. Proof. We will fix i, and prove that the bound X/2 ≤ Xi ≤ 2X holds with probability 1−O(n−γ−1). Union bounding over all i then gives the desired result. Let ∆ = maxv∈V d(r, v) be the maximum distance from the root to any vertex of the tree. We may assume that ∆ ≥ 5 as otherwise the result follows directly from a simple Chernoff bound. Since the tree is unweighted and there exists a node v of distance ∆ to r, there must also exist nodes of distances 1, 2, . . . ,∆− 1 to r, namely the nodes on the path from r to v. For the remaining nodes, we know that the distance is at least 1. Therefore,∑
v∈V d(r, v) ≥ (n−∆− 1) + ∆∑ j=1 j = n+ ( ∆ 2 ) − 1 ≥ n+ ∆ 2 3 ,
and so µi = E[Xi] ≥ n+∆ 2/3
k . Since the variables (d(r, v)[v ∈ Vi])v∈V sum to Xi and are independent and bounded by ∆, it follows by a Chernoff bound that for any 0 ≤ δ ≤ 1,
Pr [|Xi − µi| ≥ δµi] ≤ 2 exp ( −δ
2µi 3∆
) . By the AM-GM inequality, µi ∆ ≥ 1 k
( n
∆ +
∆
3 ) ≥ 2 √ n√ 3k . Putting δ = 1/2, the bound above thus becomes Pr [ |Xi − µi| ≥
µi 3
] ≤ 2 exp ( − √ n
6 √ 3k
) ≤ 2 exp ( − √ n
11k
) ≤ 2n− 1 11c ,
where the last bound uses the assumption on the magnitude of k in the lemma. Choosing c = 111(γ+1) , the desired result follows. Next, we state our result on the O(1)-approximate IP-stability for randomly colored graphs. Theorem B.2. Let γ = O(1) and k ≤ c √ n
logn for a sufficiently small constant c. Let G = (V,E) be an unweighted, undirected graph on n nodes, and suppose that we k-color the vertices of G randomly. Let Vi denote the nodes of color i. With probability at least 1− n−γ, (V1, . . . , Vk) forms an O(1)-approximate IP-clustering. Proof. Consider a node u and let Xi = ∑
v∈Vi\{u} d(u, v). Node that the distances d(u, v) are exactly the distances in a breath first search tree rooted at v. Thus, by Lemma B.1, the Xi’s are all within a constant factor of each other with probability 1−O(n−γ−1). Moreover, a simple Chernoff bound
shows that with the same high probability, |Vi| = nk + O (√ n logn k ) = Θ ( n k ) for all i ∈ [k]. In
particular, the values Yi = Xi|Vi\{u}| for i ∈ [k] also all lie within a constant factor of each other which implies that u is O(1)-stable in the clustering (V1, . . . , Vk). Union bounding over all nodes u, we find that with probability 1−O(n−γ), (V1, . . . , Vk) is an O(1)-approximate IP-clustering. Remark B.3. The assumed upper bound on k in Theorem B.2 is necessary (even in terms of log n). Indeed, consider a tree T which is a star S on n−k log k vertices along with a path P of length k log k having one endpoint at the center v of the star. With probability Ω(1), some color does not appear on P . We refer to this color as color 1. Now consider the color of the star center. With probability at least 9/10, say, this color is different from 1 and appears Ω(log k) times on P with average distance Ω(k log k) to the star center v. Let the star center have color 2. With high probability, each color appears Θ(n/k) times in S. Combining these bounds, we find that with constant probability, the average distance from v to vertices of color 1 is O(1), whereas the average distance from v to vertices of color 2 is Ω ( 1 + k 2(log k)2
n
) . In particular for the algorithm to give an O(1)-approximate IP-stable
clustering, we need to assume that k = O ( √
n logn
) . c additional empirical evaluations We implement our O(1)-approximation algorithm for IP-clustering. These experiments extend those of [AAK+22] and confirm their experimental findings: k-means++ is a strong baseline for IP-stable clustering. Nevertheless, our algorithm is competitive with it while guaranteeing robustness against worst-case datasets, a property which k-means++ does not posses. Our datasets are the following. There are three datasets from [DG19] used in [AAK+22], namely, Adult, Drug [FMM+17], and IndianLiver. We also add two additional datasets from UCI Machine Learning Repository [DG19], namely, BreastCancer and Car. For IP-clustering, we also consider a synthetic dataset which is the hard instance for k-means++ given in [AAK+22]. Our goal is to show that our IP-clustering algorithm is practical and in real world datasets, is competitive with respect to k-means++, which was the best algorithm in the experiments in [AAK+22]. Furthermore, our algorithm is robust and outperform k-means++ for worst case datasets. As before, all experiments were performed in Python 3. We use the k-means++ implementation of Scikit-learn package [PVG+11]. We note that in the default implementation in Scikit-learn, k-means++ is initiated many times with different centroid seeds. The output is the best of 10 runs
by default. As we want to have control of this behavior, we set the parameter n_init=1 and then compute the average of many different runs. Additionally to the metrics used in the main experimental section, we also compute the number of unstable points, defined as the size of the set U = {x ∈ M : x is not stable}. In terms of clustering qualities, we additionally measure three quantities. First, we measure “cost”, which is the average within-cluster distances. Formally, Cost = ∑k i=1 1
(|Ci|2 )
∑ x,y∈Ci,x ̸=y d(x, y). We then
measure k-center costs, defined as the maximum distances from any point to its center. Here, centers are given naturally from k-means++ and our algorithm. Finally, k-means costs, defined as k-means-cost = ∑k i=1 1 |Ci| ∑ x,y∈Ci,x ̸=y d(x, y) 2. C.1 Hard Instance for k-means++ for IP-Stability
We briefly describe the hard instance for k-means++ for the standard IP-stability formulation given in [AAK+22]; see their paper for full details. The hard instance consists of a gadget of size 4. In the seed-finding phase of k-means++, if it incorrectly picks two centers in the gadget, then the final clustering is not β-approximate IP-stable, where β is a configurable parameter. The instance for k-clustering is produced by concatenating these gadgets together. In such an instance, with a constant probability, the clustering returned by k-means++ is not β-approximate IP-stable and in particular. We remark that the proof of Theorem 2 in [AAK+22] easily implies that k-means++ cannot have an approximation factor better than nc for some absolute constant c > 0, i.e., we can insure β = Ω(nc). Here, we test both our algorithm and k-means++ in an instance with 8,000 points (for k = 2, 000 clusters). IP-Stability results We first discuss five real dataset. We tested the algorithms for the range of k up to 25. The result in Figures 2 and 3 is consistent with the experiments in the previous paper as we see that k-means++ is a very competitive algorithm for these datasets. For small number of clusters, our algorithm sometimes outperforms k-means++. We hypothesize that on these datasets, especially for large k, clusters which have low k-means cost separate the points well and therefore are good clusters for IP-stability. Next we discuss the k-means++ hard instance. The instance used in Figure 3 was constructed with β = 50. We vary k but omit the results for higher k values since the outputs from both algorithms are stable. We remark that the empirical results with different β gave qualitatively similar results. For maximum and mean violation, our algorithm outperforms k-means++ (Figure 3).