We propose a novel image registration method based on implicit neural representations that addresses the challenging problem of registering a pair of brain images with similar anatomical structures, but where one image contains additional features or artifacts that are not present in the other image. To demonstrate its effectiveness, we use 2D microscopy in situ hybridization gene expression images of the marmoset brain. Accurately quantifying gene expression requires image registration to a brain template, which is difficult due to the diversity of patterns causing variations in visible anatomical brain structures. Our approach uses implicit networks in combination with an image exclusion loss to jointly perform the registration and decompose the image into a support and residual image. The support image aligns well with the template, while the residual image captures individual image characteristics that diverge from the template. In experiments, our method provided excellent results and outperformed other registration techniques. 1 introduction Image registration is a crucial prerequisite for image comparison, data integration, and group studies in contemporary medical and neuroscience research. In research and clinical settings, pairs of images often show similar anatomical structures but may contain additional features or artifacts, such as specific staining, electrodes, or lesions, that are not present in the other image. This difficulty of finding corresponding structures for automatically aligning images complicates image registration. In this work, we address the challenging problem of the gene expression image registration in the marmoset brain. Brain atlases of gene
⋆ Corresponding author. ar X
iv :2
30 8. 04 03
9v 1
[ cs
.C V
] 8
expression, created using images of brain tissue processed through in situ hybridization (ISH), offer single-cell resolution of spatial gene expression patterns across the entire brain [2,7]. However, accurately quantifying gene expression requires brain image registration to spatially align ISH images to a common atlas space. The diversity of gene expression patterns in ISH images causes variations in visible anatomical brain structures with respect to the template image. ISH microscopy images are also susceptible to tissue processing artifacts, resulting in non-specific staining and tissue deformations. Traditional pair-wise image registration methods use optimization algorithms to find the deformation field that maximizes the similarity between a pair of images. While several deep learning methods based on convolutional neural networks (CNNs) have been proposed for calculating the deformation field between two images [3], such models typically require large training sets and may suffer from generalization issues when applied to images presenting texture patterns that diverge from the training data. Therefore, classic algorithms, such as Advanced Normalization Tools (ANTs) [1], are still preferred as off-the-shelf tools for image registration in neuroscience due to scarce experimental data and the diversity of data acquisition protocols and registration tasks. Recently, implicit neural representations (INRs) have been utilized for image registration in MRI and CT [14,16], offering a hybrid approach that connects modern deep learning techniques with per-case optimization as used in classical approaches. INRs are defined on continuous coordinate spaces, making them suitable for registration of images that differ in geometry. In this work, we propose a novel INR-based framework well-suited to address the challenging problem of gene expression brain image registration. We associate the registration problem with an image decomposition task. We utilize implicit neural networks to decompose the ISH image into two separate images: a support image and a residual image. The support image corresponds to the part of the ISH image that is well-aligned with the registration template image in respect to the texture. On the contrary, the residual image presents features of the ISH image, such as artifacts or texture patterns (e.g. gene expression), which presumably undermine the registration procedure. The support image is used to improve the deformation field calculations. We also introduce an exclusion loss to encourage clearer separation of the support and residual images. The usefulness of the proposed method is demonstrated using 2D ISH gene expression images of the marmoset brain. 2 methods  2.1 registration with implicit networks The goal of the pairwise image registration is to determine a spatial transformation that maximizes the similarity between the moving image M and the target fixed template image F . INRs serve as a continuous, coordinate based approximation of the deformation field obtained through a fully connected neural network. In this study, as the backbone for our method, we utilized the standard
approach to registration with INRs, as described in [14,16]. We used a single implicit deformation network D to map 2D spatial coordinates x̄ ∈ [−1, 1]2 of the moving image M to a displacement vector ∆x̄ ∈ R2. Next, the transformation field was determined as Φ(x̄) = x̄+∆x̄ and the bilinear interpolation algorithm was applied to obtain the corresponding moved image TΦ(M). To train the deformation network, the following loss function based on correlation coefficients was applied to assess the similarity between the moved image TΦ(M) and the fixed template image F :
Lcc(F, TΦ(M)) = 1
2N ∑ x̄ ( NCC(F, TΦ(M)) + LNCC(F, TΦ(M)) ) , (1)
where NCC and LNCC stand for the normalized cross-correlation and local normalized cross-correlation based loss functions averaged over the entire image domain consisting of N elements. NCC was used to stabilize the training of the network, while LNCC ensured good local registration results. Additionally, following the standard approach to INR based registration, we regularized the deformation field based on the Jacobian matrix determinant |JΦ(x̄)| using following equation [16]:
Lreg(Φ(x̄)) = 1
N ∑ x̄ |1− |JΦ(x̄)||. (2) 2.2 registration guided image decomposition Our aim is to improve the registration performance associated with the implicit deformation network D. The proposed framework is presented in Fig. 1. We assume that the moving image M can be decomposed with separate implicit networks, S and R, into two images: the support image MS and the residual image MR. Ideally, the support image should correspond to the part of the moving image that contributes to the registration performance. On the contrary, we expect the residual image to include image artifacts and texture patterns (e.g. ISH gene expression patterns) that diverge from the fixed template image and undermine the registration procedure. We impose the following condition based on the mean squared error loss function for the decomposition of the moving image:
Lrec(M,MS +MR) = 1
N ∑ x̄ (M −MS −MR)2, (3)
stating that the support MS and residual MR images should sum up to the moving image M . To ensure that the support image MS contributes to the registration with respect to the fixed image F , we utilize the cross-correlation based loss function Lcc(F, TΦ(MS)) (eq. 1), where TΦ(MS) stands for the transformed support image MS . Therefore, the deformation network is trained to provide the transformation field Φ(x) both for the moving image and the support image
using two cross-correlation based loss functions. This way the training of the deformation network is guided to provide a more detailed transformation field for the contents of the moving image that actually correspond to the fixed template image. Moving image texture patterns that do not correspond to the fixed image have lower impact on the training of the deformation network. In practice, it might be beneficial, following INR based methods for obstruction and rain removal, to additionally constrain the image decomposition procedure to obtain more clearly separated support MS and residual MR images [10]. For this, we utilize the following exclusion loss to encourage the gradient structure of the implicit networks S and R to be decorrelated [4]:
Lexcl(MS ,MR) = 1
N ∑ x̄ ∑ i,j |Γ (JS(x̄), JR(x̄))| (4)
where Γ (JMS (x̄), JMR(x̄)) = tanh(JS(x̄)) ⊗ tanh(JR(x̄)), ⊗ indicates elementwise multiplication and indices i, j go over all elements of the matrix Γ . In our framework, we jointly optimize all three implicit networks (D, S and R) using the following composite loss function:
Loss = α1Lcc(F, TΦ(M)) + α2Lcc(F, TΦ(MS)) + α3Lreg(Φ(x̄)) + α4Lrec(M,MS +MR) + α5Lexcl(MS ,MR). (5)
The first row of eq. 5 can be perceived as a standard registration loss, while the second row stands for a regularized image reconstruction loss. 2.3 evaluation We designed the proposed method with the aim to address the problem of ISH gene expression image registration. For the evaluation, we used neonate marmoset brain ISH images collected at the Laboratory for Molecular Mechanisms of Brain Development, RIKEN Center for Brain Science, Wako, Japan (geneatlas.brainminds.jp) [6,12]. We prepared manual annotations for 2D images from 50 gene expression datasets. Atlas template images were created using ANTs [1], based on semi-automatically aligned sets of 2D ISH images from 1942 gene expression datasets. ISH images used to generate the template were converted to gray-scale to meet ANTs requirements and better highlight brain tissue interfaces. Performance of the proposed approach was compared to the SynthMorph network and the ANTs SyN registration algorithm based on mutual information metric, as these two methods do not require pre-training and can serve as off-the-shelf registration tools for neuroscience [1,5]. We conducted an ablation study to assess the effectiveness of the proposed representation decomposition approach with and without the exclusion loss. Registration methods were evaluated quantitatively based on Dice scores using manual 2D segmentations prepared for the following five brain structures ranging in size and shape complexity: aqueduct (AQ, 95 masks), hippocampus area (HA, 570 masks), dorsal lateral geniculate (DLG, 370 masks), inferior colliculus (IC, 70 masks) and visual cortex area (VCA, 68 masks). Segmentations were outlined both for the template and ISH 2D images, resulting in 1114 image pairs corresponding to the same brain regions. We also calculated the percentage of the non-positive Jacobian determinant values to assess the deformation field folding. Moreover, we determined the structural similarity index (SSIM) between the moved images and the template fixed images. 2.4 implementation We utilized sinusoidal representation networks to determine the implicit representations [13]. Each network contained five fully connected hidden layers with 256 neurons. We used the Fourier mapping with six frequencies to encode the input coordinates [15]. The coordinates and the encoded coordinates were additionally concatenated within the middle layer of the network. Weights of the networks were initialized following the original paper except for the last linear layer of the deformation network D, for which we uniformly sampled the weights from [-0.0001, 0.0001] interval to ensure small deformations at initial epochs. Additional details about the network architecture can be found in the supplementary materials. Networks were trained for 1000 epochs using AdamW optimizer with learning rate of 0.0001 on a server equipped with several NVIDIA A100 GPUs [8]. ISH images of size 360x420 were downsampled to 256x256. Each epoch corresponded to a batch of all image pixel coordinates [13]. After some initial experiments, we set the composite loss function weights (eq. 5) to α1=α2=α3=α5=1 and α4=100, partially following the previous studies on INRs
[10,14,16]. The window size for the LNCC loss was set to [32, 32]. Our PyTorch implementation of the proposed INR based registration method is available at https://github.com/BrainImageAnalysis/ImpRegDec. 3 results  3.1 qualitative results Support and residual images generated with the proposed method are shown in Fig.2. The support images retain the main style and content of the fixed template image, while the residual images include the remaining image contents, along with gene expression patterns not present in the template image. Utilization of the exclusion loss resulted in a clearer and more visually plausible separation between the support and residual images, particularly for gene expression patterns. Fig. 3 further highlights the usefulness of the proposed registration guided image decomposition technique. First, our method can be applied to extract microscopy image artifacts, and therefore mitigate their impact on the registration. Second, the proposed method is general and can also be applied to register an ISH gene expression image to a Nissl image. In this case, the color distribution of the support image corresponds to that of a Nissl image, while the residual image presents the local contents of the gene expression image. We also used the proposed method to register an ISH brain image to another ISH image with a different gene expression. For this example, the residual image highlighted the gene expression patterns of the moving image, while the support image showed the gene expression patterns of the fixed image. Fig. 4 visually compares the registration performance of the proposed technique, equipped with the exclusion loss, to ANTs. We found that the proposed
method provided good results both in respect to the image registration and the transformation of the manual segmentations. 3.2 quantitative results Table 1 shows Dice scores obtained for the selected marmoset brain regions. Registration techniques based on INRs outperformed the other methods on four
out of five brain regions. ANTs achieved better registration results for only one structure, the VCA, which was the largest among the annotated brain regions and already similar in unregistered images with an initial Dice score of 0.848. Additionally, the Dice score for the VCA was high and comparable across all investigated registration methods. Our approach achieved significantly better Dice scores compared to the standard INRs for AQ, HA, DLG and IC (t-test’s p-values<0.05). Furthermore, incorporating the exclusion loss slightly improved the Dice scores for three structures. SSIM values in Table 2 show that the registration based on implicit networks provided the most structurally similar results to the template images. With respect to the SSIM metric, our method significantly outperformed other approaches (t-test’s p-values<0.05). ANTs and SynthMorph provided smoother deformation fields compared to the implicit networks, with significantly lower percentage of folding (t-test’s p-values<0.05). However, the percentage of the folding obtained for the implicit networks was small and acceptable, as defined by folds in 0.5% of all pixels [11]. The main disadvantage of the proposed approach was the relatively long optimization time of about 90 seconds for a single pairwise registration, resulting from the requirement to jointly train three implicit networks. 4 conclusion Our approach based on implicit networks and registration-guided image decomposition has demonstrated excellent performance for the challenging task of registering ISH gene expression images of the marmoset brain. The results show that our approach outperformed pairwise registration methods based on ANTs and SynthMorph CNN, highlighting the potential of INRs as versatile off-the-shelf tools for image registration. Moreover, the proposed registration-guided image decomposition mechanism not only improved the registration performance, but also could be used to effectively separate the patterns that diverge from the target fixed image.