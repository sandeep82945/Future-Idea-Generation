Argument mining is to analyze argument structure and extract important argument information from unstructured text. An argument mining system can help people automatically gain causal and logical information behind the text. As argumentative corpus gradually increases, like more people begin to argue and debate on social media, argument mining from them is becoming increasingly critical. However, argument mining is still a big challenge in natural language tasks due to its difficulty, and relative techniques are not mature. For example, research on non-tree argument mining needs to be done more. Most works just focus on extracting tree structure argument information. Moreover, current methods cannot accurately describe and capture argument relations and do not predict their types. In this paper, we propose a novel neural model called AutoAM to solve these problems. We first introduce the argument component attention mechanism in our model. It can capture the relevant information between argument components, so our model can better perform argument mining. Our model is a universal end-to-end framework, which can analyze argument structure without constraints like tree structure and complete three subtasks of argument mining in one model. The experiment results show that our model outperforms the existing works on several metrics in two public datasets. 1 introduction Argument mining (AM) is a technique for analyzing argument structure and extracting important argument information from unstructured text, which has gained popularity in recent years [12]. An argument mining system can help people automatically gain causal and logical information behind the text. The argument mining techniques benefit plenty of many fields, like legal [31], public opinions [19], finance, etc. Argument mining is beneficial to human society, but there is still much room for development. Argument mining consists of several tasks and has a variety of different paradigms [12]. In this paper, we focus on the most common argument structure of monologue. It is an argumentative text from one side, not an argument from two sides. The microscopic structure of
ar X
iv :2
30 9. 09 30
0v 1
[ cs
.C L
] 1
7 Se
p 20
argumentation is the primary emphasis of the monologue argument structure, which primarily draws out the internal relations of reasoning. In this setting, an argumentative paragraph can be viewed as an argument graph. An argument graph can efficiently describe and reflect logical information and reasoning paths behind the text. An example of AM result after extraction is shown in Figure 1. The two important elements in an argument graph are the argument component (AC) and the argument relation (AR). ACs are nodes in this graph, and ARs are edges. The goal of an AM system is to construct this argument graph from unstructured text automatically. The process of the AM system definition we use is as following steps:
1. Argument Component Identification (ACI): Given an argumentative paragraph, AM systems will detect ACs from it and separate this text. 2. Argument Component Type Classification (ACTC): AM systems will determine the types of these ACs. 3. Argument Relation Identification (ARI): AM systems will identify the existence of a relationship between any ACs. 4. Argument Relation Type Classification (ARTC): AM systems will determine the type of ARs, which are the existing relations between ACs. Subtask 1) is a token classification task, which is also a named entity recognition task. This task has a large amount of research work on it. Most of the previous argument mining works [25] [10] [3] assume that the subtask 1) argument component identification has been completed, which is the argument component has been identified and can be obtained from the argumentative text. Therefore, the emphasis of argument mining research is placed on other subtasks. Following previous works, we also make such assumptions in this paper. On this basis, we design an end-to-end model to complete ACTC, ARI, and ARTC subtasks simultaneously. ARI and ARTC are the hardest parts of the whole argument mining process. An AR is represented by two ACs. It is difficult to represent AR precisely and
capture this relation. Most ACs pairs do not have a relationship at all, which leads to a serious sample imbalance problem. Among the whole process, ARI and ARTC are parts of ACI and ACTC, so the performance of these tasks will be influenced. Due to these reasons, many previous works give up and ignore the classification of ARs. Besides, much research imposes some argument constraints to do argument mining. In most normal cases, they assume argument information is a tree structure, and they can use the characteristic of the tree to extract information. Tree structure argument information is common in an argumentative essay. However, argument information with no constraints is more normal in the real world, like a huge amount of corpus on social media. This information is just like the general argument graphs mentioned before and needs to be extracted in good quality. In this paper, we solve the above problems with a novel model called AutoAM (the abbreviation of Automactic and UniversalArgumentMining Model). This is an efficient and accurate argument mining model to complete the entire argument mining process. This model does not rely on domain-specific corpus and does not need to formulate special syntactic constraints, etc., to construct argument graphs from argumentative text. To improve the performance of nontree structured argument mining, we first introduce the argument component attention mechanism (ArguAtten) in this model, which can better capture the relevant information of argument components in an argumentative paragraph. It benefits the overall performance of argument mining. We use a distance matrix to add the key distance feature to represent ARs. A stratified learning rate is also a critical strategy in the model to balance multi-task learning. To the best of our knowledge, we are the first to propose an end-to-end universal AM model without structure constraints to complete argument mining. Meanwhile, we combine our novelty and some successful experience to achieve the state of the art in two public datasets. In summary, our contributions are as follows:
– We propose a novel model AutoAM for argument mining which can efficiently solve argument mining in all kinds of the argumentative corpus. – We introduce ArguAtten (argument component attention mechanism) to better capture the relation between argument components and improve overall argument mining performance. – We conduct extensive experiments on two public datasets and demonstrate that our method substantially outperforms the existing works. The experiment results show that the model proposed in this paper achieves the best results to date in several metrics. Especially, there is a great improvement over the previous studies in the tasks of ARI (argument relation identification) and ARTC (argument relation type classification). 2 related work Since argument mining was first proposed [16], much research has been conducted on it. At first, people used rule-based or some traditional machine learning
methods. With the help of deep learning, people begin to get good performance on several tasks and start to focus on non-tree structured argument mining. We discuss related work following the development of AM. 2.1 early argument mining The assumption that the argument structure could be seen as a tree or forest structure was made in the majority of earlier work, which made it simpler to tackle the problem because various tree-based methods with structural restrictions could be used. In the early stage of the development of argument mining, people usually use rule-based structural constraints and traditional machine learning methods to conduct argumentative mining. In 2007, Moens et al. [16] conducted the first argument mining research on legal texts in the legal field, while Kwon et al. [11] also conducted relevant research on commentary texts in another field. However, the former only identified the content of the argument and did not classify the argument components. Although the latter one further completed the classification of argument components, it still did not extract the relationship between argument components, and could not explore the argument structure in the text. It only completed part of the process of argument mining. 2.2 tree structured argument mining with machine learning According to the argumentation paradigm theory of Van Eemeren et al. [6], Palau and Moens [15] modeled the argument information in legal texts as a tree structure and used the hand-made Context-Free Grammar (CFG) to parse and identify the argument structure of the tree structure. This method is less general and requires different context-free grammars to be formulated for different structural constraints of argument. By the Stab and Gurevych [27] [28] tree structure of persuasive Essay (Persuasive Essay, PE) dataset has been in argument mining has been applied in many studies and practices. In this dataset, Persing and Ng [23] and Stab and Gurevych [28] used the Integer Linear Programming (ILP) framework to jointly predict the types of argument components and argument relations. Several structural constraints are defined to ensure a tree structure. The arg-micro text (MT) dataset created by Peldszus [21] is another tree-structured dataset. In studies using this dataset, decoding techniques based on tree structure are frequently used, such as Minimum Spanning tree (MST) [22], and ILP [1]. 2.3 neural network model in argument mining With the popularity of deep learning, neural network models have been applied to various natural language processing tasks. For deep learning methods based on neural networks, Eger et al. [7] studied argument mining as a sequence labeling problem that relies on parsing multiple neural networks. Potash et al. [25] used sequence-to-sequence pointer network [30] in the field of argument mining
and identified the different types of argument components and the presence of argument relations using the output of the encoder and decoder, respectively. Kuribayashi et al. [10] developed a span representation-based argumentation structure parsing model that employed ELMo [24] to derive representations for ACs. 2.4 recent non-tree structured argument mining Recently, more works have focused on the argument mining of non-tree structures. The US Consumer Debt Collection Practices (CDCP) dataset [18] [19] greatly promotes the development of non-tree structured argument mining. The argument structures contained in this dataset are non-tree structures. On this dataset, Niculae et al. [18] carry out a structured learning method based on a factor graph. This method can also handle the tree structure of datasets. It can also be used in the PE dataset, but the factor diagram needs a specific design according to the different types of the argument structure. Galassi et al. [8] used the residual network on the CDCP dataset. Mor IO et al. [17] developed an argument mining model, which uses a task-specific parameterized module to encode argument. In this model, there is also a bi-affine attention module [5] to capture the argument. Recently, Jianzhu Bao et al. [2]tried to solve both tree structure argument and non-tree structure argument by introducing the transformation-based dependency analysis method [4] [9]. This work gained relatively good performance on the CDCP dataset but did not complete the ARTC task in one model and did not show the experiment results of ARTC. However, these methods either do not cover the argument mining process with a good performance or impose a variety of argument constraints. There is no end-to-end model for automatic and universal argument mining before. Thus, we solve all the problems above in this paper. 3 methodology As shown in Figure 2, we propose a new model called AutoAM. This model adopts the joint learning approach. It uses one model to simultaneously learn the ACTC, ARI, and ARTC three subtasks in argument mining. For the argument component extraction, the main task is to classify the argument component type, and the argument component identification task has been completed by default on both the PE and the CDCP datasets. For argument relation extraction, the model regards ARI and ARTC as one task. The model classifies the relationship between the argument components by a classifier and then gives different prediction results for two tasks by post-processing prediction labels. 3.1 task formulation The input data contains two parts: a) A set of n argumentative text T = {T1, T2, ..., Tn}, b) for the ith argumentative text, there are m argument component spans S = {S1, S2, ..., Sm}, where every span marks the start and end scope ar representation of each AC Si = (starti, endi). Our aim is to train an argument mining model and use it to get output data: a) types of m ACs provided in the input data ACs = {AC1, AC2, ..., ACm}, b) k existing ARs ARs = {AR1, AR2, ..., ARk} and their types, where ARi = (ACa → ACb). 3.2 argument component extraction By default, the argument component identification task has been completed. The input of the whole model is an argumentative text and a list of positional spans corresponding to each argument component Si = (starti, endi). We input argumentative text T into pre-trained language models (PLMs) to get contextualized representations H ∈ Rm×db , where db is the dimension of the last hidden state from PLMs. Therefore, we represent argumentative text asH = (h1, h2, ..., hm), where hi denotes the ith token contextualized representation. We separate argument components from the paragraph using argument component spans S. In the PE dataset, the argument components do not appear continuously. We use mean pooling to get the representation of each argument component. Specifically, the i argument component can be represented as:
ACi = 1
endi − starti + 1 endi∑ j=starti hi, (1)
where ACi ∈ Rdb . Therefore, all argument components in the argumentative text can be represented as ACs = (AC1, AC2, ..., ACn). For each argument component, we input it into AC Type Classifier MLPa in order. This classifier contains
a multi-layer perceptron. A Softmax layer is after it. The probability of every type of argument component can be get by:
p(yi|ACi) = Softmax(MLPa(ACi)), (2)
where yi represent the predicted labels of the ith argument component. We get the final predicted label of its argument component as:
ŷi = Argmax(p(yi|ACi)). (3) 3.3 argument relation extraction This model views ARI and ARTC as having the same task and distinguish them by post-processing predictions. We classify every argument component pair (ACi → ACj). Argument component pairs are different of (ACi → ACj) and (ACj → ACi). We add a label, ‘none’ here. ‘none’ represents that there is no relation of ACi → ACj . In the argument relation extraction part, we use the enumeration method. We utilize output results from the ACTC step. We combine two argument components and input them into AR Type Classifier to get the predicted output. First, the model uses ArguAtten (Argument Component Attention mechanism) to enhance the semantic representation of argument components. The self-attention mechanism is first proposed in the Transformer [29]. The core of this mechanism is the ability to capture how each element in a sequence relates to the other elements, i.e., how much attention each of the other elements pays to that element. When the self-attention mechanism is applied to natural language processing tasks, it can often capture the interrelationship of all lexical elements in a sentence and better strengthen the contextual semantic representation. In the task of argument mining, all argument components in an argumentative text also meet this characteristic. The basic task of argument mining is to construct an argument graph containing nodes and edges, where nodes are argument components and edges are argument relations. Before the argument relation extraction task, the self-attention mechanism of argument components can be used to capture the mutual attention of argument components. It means that it can better consider and capture the argument information of the full text. This mechanism is conducive to argument relations extraction and the construction of an argument graph. We define ArguAtten as:
ArguAtten(Q,K, V ) = Softmax( QKT√
dk )× V, (4)
where Q, K, V are got by multiplying ACs with WQ, WK , WV . They are three parameter matrices WQ,WK ,WV ∈ Rdb×dk , and dk is the dimension of attention layer. Besides, we also use ResNet and layer normalization (LN) after the attention layer to avoid gradient explosion:
ResNetOut = LN(ACs+ArgutAtten(ACs)). (5)
Through the self-attention of argument components, we obtain a better contextualized representation of argument components and then begin to construct argument pairs to perform argument relation extraction. We consider that the relative distance between two argument components has a decisive influence on the type of argument relations between them. By observing the dataset, we can find that there is usually no argument relation between the two argument components, which are relatively far apart. It can significantly help the model to classify the argument relation types. Therefore, we incorporate this feature into the representation of argument relations. At first, the distance vector is introduced, and the specific definition is shown as:
Vdist = (i− j)×Wdist, (6)
where (i−j) represents a relative distance, it can be positive or negative. Wdist ∈ R1×ddist is a distance transformation matrix, and it can transform a distance scalar to a distance vector. ddist is the length of the distance vector. For each argument relation, it comes from the source argument component (Src AC), the target argument component (Trg AC), and the distance vector (Dist Vec). We concatenate them to get the representation of an argument relation as:
ARi,j = [ACi, ACj , Vdist], (7)
where ARi,j ∈ Rdb×2+ddist , ddist is the length of distance vector. Therefore, argument relations in an argumentative text can be represented as ARs = (AC1,2, AC1,3, ..., ACn,n−1, ), contains n× (n− 1) potential argument relations in total. We do not consider self-relation like AR = (ACi → ACi). For each potential argument relation, we separately and sequentially input them into the AR Type Classifier MLPb. The classifier uses a Multi-Layer Perceptron (MLP) containing a hidden layer of 512 dimensions. The output of the last layer of the Multi-layer Perceptron is followed by a Softmax layer to obtain the probability of an argument relationship in each possible type label, as shown in:
p(yi,j |ARi,j) = Softmax(MLPb(ARi,j)), (8)
where yi,j denotes the predicted label of the argument relation from the ith argument component to the jth argument component. The final predicted labels are:
ŷi,j = Argmax(p(yi,j |ARi,j)). (9)
To get the predicted labels of ARI and ARTC, we post-processed the prediction of the model. The existence of an argument relation in the ARI task is defined as:
ŷARI =
{ 0 if ŷAR = 0
1 if ŷAR ̸= 0 (10)
where ŷAR is the predicted label from the model output. When we gain the type of an existing argument relation in the ARTC task, we assign the probability of ‘none’ to zero and select the other label with the higher probability. They are represented as:
ŷARTC = Argmax(p(yAR|ARi,j)), ynone = 0, (11)
where ynone is the model output of the label ‘none’. 3.4 loss function design This model jointly learns the argument component extraction and the argument relation extraction. By combining these two tasks, the training objective and loss function of the final model is obtained as:
L(θ) = ∑ i log(p(yi|ACi)) + ∑ i,j p(yi,j |ARi,j) + λ 2 ||θ||2, (12)
where θ represents all the parameters in the model, and λ represents the coefficient of L2 regularization. According to the loss function, the parameters in the model are updated repeatedly until the model achieves better performance results to complete the model training. 4 experiments  4.1 datasets We evaluate our proposed model on two public datasets: Persuasive Essays (PE) [28] and Consumer Debt Collection Practices (CDCP) [18]. The PE dataset only has tree structure argument information. It has three types of ACs: Major-Claim, Claim, and Premise, and two types of AR: support and attack. The CDCP dataset has general structure argument information, not limited to a tree structure. It is different from the PE dataset and is more difficult. The argument information in this dataset is more similar to the real world. There are five types of ACs (propositions): Reference, Fact, Testimony, Value, and Policy. Between these ACs, there are two types of ARs: reason and evidence. We both use the original train-test split of two datasets to conduct experiments. 4.2 setups In the model training, roberta-base [13] was used to fine-tune, and AdamW optimizer [14] was used to optimize the parameters of the model during the training. We apply a stratified learning rate to obtain a better semantic representation of BERT context and downstream task effect. The stratified learning rate is important in this task because this multi-task learning is complex and have three
subtasks. The ARI and ARTC need a relatively bigger learning rate to learn the data better. The initial learning rate of the BERT layer is set as 2e-5. The learning rate of the AC extraction module and the AR extraction module is set as 2e-4 and 2e-3, respectively. After BERT output, the Dropout Rate [26] is set to 0.2. The maximum sequence length of a single piece of data is 512. We cut off ACs and ARs in the over-length text. The batch size in each training step is set to 16 in the CDCP dataset and 2 in the PE dataset. The reason is that there are more ACs in one argumentative text from the PE dataset than in the CDCP dataset. In training, we set an early stop strategy with 5 epochs. We set the minimum training epochs as 15 to wait for the model to become stable. We use MacroF1ARI as monitoring indicators in our early stop strategy. That is because AR extraction is our main improvement direction. Furthermore, the ARI is between the ACTC and the ARTC, so we can better balance the three tasks’ performance in the multi-task learning scenario. The code implementation of our model is mainly written using PyTorch [20] library, and the pre-trained model is loaded using Transformers [32] library. In addition, model training and testing were conducted on one NVIDIA GeForce RTX 3090. 4.3 compared methods We compare our model with several baselines to evaluate the performance:
– Joint-ILP [28] uses Integer Linear Programming (ILP) to extract ACs and ARs. We compare our model with it in the PE dataset. – St-SVM-full [18] uses full factor graph and structured SVM to do argument mining. We compare our model with it in both the PE and the CDCP datasets. – Joint-PN [25] employs a Pointer Network with an attention mechanism to extract argument information. We compare our model with it in the PE dataset. – Span-LSTM [10] use LSTM-based span representation with ELMo to perform argument mining. We compare our model with it in the PE dataset. – Deep-Res-LG [8] uses Residual Neural Network on AM tasks. We compare our model with it in the CDCP dataset. – TSP-PLBA [17] introduces task-specific parameterization and bi-affine attention to AM tasks. We compare our model with it in the CDCP dataset. – BERT-Trans [2] use transformation-based dependency analysis method to solve AM problems. We compare our model with it in both the PE and the CDCP datasets. It is also the state of the art on two datasets. 4.4 performance comparison The evaluation results are summarized in Table 1 and Table 2. In both tables, ‘-’ indicates that the original paper does not measure the performance of this
metric for its model. The best results are in bold, and the second-best results are in italics. On the CDCP dataset, we can see our model achieves the best performance on all metrics in ACTC, ARI, and ARTC tasks. We are the first to complete all the tasks and get ideal results on the CDCP dataset. Our model outperforms the state of the art with an improvement of 2.1 in ACTC and 0.6 in ARI. The method BERT-Trans does not perform ARTC with other tasks at the same time, and it does not report results of ARTC, maybe due to unsatisfactory performance. In particular, compared with the previous work, we have greatly improved the task performance of ARTC and achieved ideal results. On the PE dataset, our model also gets ideal performance. However, we get the second-best scores in several metrics. The first reason is that the PE dataset is tree-structured, so many previous work impose some structure constraints. Their models incorporate more information, and our model assumes they are general argument graphs in contrast. Another reason is that the models BERTTrans, Span-LSTM, and Joint-PN combine extra features to represent ACs, like paragraph types, BoW, position embedding, etc. This information will change in the different corpus, and we want to build an end-to-end universal model. For example, there is no paragraph type information in the CDCP dataset. Therefore, we do not use them in our model. Even if our model does not take these factors into account, we achieve similar results to the state of the art. 4.5 ablation study The ablation study results are summarized in Table 3. We conduct an ablation study on the CDCP dataset to see the impact of key modules in our model. It can be observed that the stratified learning rate is the most critical in this model. It verifies the viewpoint that multi-task learning is complex in this model and ARs extraction module needs a bigger learning rate to perform well. We can see ArguAtten improve the ACTC and ARTC performance by 1.7 and 13.6. However, the ARI matric decreases a little bit. Even though the numbers are small, we think that the reason is the interrelationship between ACs has little impact on the prediction of ARs’ existence. ArguAtten mainly plays an effect in predicting the type of ARs. From this table, we can also find that the distance matrix brings the important distance feature to AR representation with an overall improvement of 6.5. 5 conclusion and future work In this paper, we propose a novel method for argument mining and first introduce the argument component attention mechanism. This is the first end-to-end argument mining model that can extract argument information without any structured constraints and get argument relations of good quality. In the model, ArguAtten can better capture the correlation information of argument components in an argumentative paragraph so as to better explore the argumentative relationship. Our experiment results show that our method achieves the state of the art.