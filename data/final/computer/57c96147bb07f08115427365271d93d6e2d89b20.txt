This paper proposes a fine-grained self-localization method for outdoor robotics that utilizes a flexible number of onboard cameras and readily accessible satellite images. The proposed method addresses limitations in existing cross-view localization methods that struggle to handle noise sources such as moving objects and seasonal variations. It is the first sparse visual-only method that enhances perception in dynamic environments by detecting view-consistent key points and their corresponding deep features from ground and satellite views, while removing off-the-ground objects and establishing homography transformation between the two views. Moreover, the proposed method incorporates a spatial embedding approach that leverages camera intrinsic and extrinsic information to reduce the ambiguity of purely visual matching, leading to improved feature matching and overall pose estimation accuracy. The method exhibits strong generalization and is robust to environmental changes, requiring only geo-poses as ground truth. Extensive experiments on the KITTI and Ford Multi-AV Seasonal datasets demonstrate that our proposed method outperforms existing state-of-the-art methods, achieving median spatial accuracy errors below 0.5 meters along the lateral and longitudinal directions, and a median orientation accuracy error below 2◦ 1. 1. introduction Accurate self-localization is a fundamental problem in mobile robotics, particularly in the context of autonomous driving. While Global Positioning System (GPS) is a widely adopted solution, its accuracy hardly meets the stringent requirements of autonomous driving [20]. Real-Time Kinematic (RTK) positioning systems provide an alternative by correcting GPS errors, but their implementation is hindered by the need for signal reference stations [13], rendering them an expensive solution. On the other hand, odometry [18, 4, 37, 32] or simultaneous localization and mapping (SLAM) [17, 11, 25, 32] methods can generate
1Our project page is https://shanwang-shan.github.io/PureACLwebsite/
accurate short-term trajectories, however, they experience drift accumulation over time that can only be alleviated through loop closures if the agent’s trajectories overlap. Lastly, other self-localization techniques [35, 15, 31, 21] that rely on a pre-constructed 3D High Definition (HD) maps face limitations in terms of the extensive time and resources required for map acquisition and maintenance. Using off-the-shelf satellite images as ready-to-use maps to achieve cross-view localization brings an alternative and promising way for low-cost localization. However, due to the significant disparity between overhead views captured by satellites and views seen by robots, cross-view localization is more challenging than traditional methods. To address this, it is crucial to purify view-consistent features that can support the localization process. Furthermore, satellite views can be captured at different times, leading to variations in seasonal and temporal conditions. The crossview consistent purification can also minimize the impact of moving and seasonal objects. Most previous cross-view localization methods [24, 10, 14, 29, 23, 38] approach the task as an image retrieval problem, leading to coarse localization accuracy that is inferior to commercial GPS which can achieve an error of up to 4.9 meters in open sky conditions [30]. In contrast, our method utilizes a coarse pose that is easily obtainable from the Autonomous Vehicles system, to estimate the fine-grained 3- DoF (lateral, longitudinal, yaw) pose of the robot. This is accomplished through visual cross-view matching, utilizing ground-view images captured by onboard cameras and a
ar X
iv :2
30 8. 08 11
0v 1
[ cs
.C V
] 1
6 A
ug 2
02 3
spatially-consistent satellite map. Additionally, our method supports multiple camera inputs, which extend the field of view of the query robot. The setting is illustrated in Fig. 1. Our fine-grained visual localization method utilizes sparse (keypoint) feature matching, a departure from prior methods that rely on dense feature matching. To reduce the inherent ambiguity in purely visual matching, the method incorporates a camera intrinsic and extrinsic aware spatial embedding. Homography transformation is used to establish correspondences between the two views. An onground confidence map is employed to ensure the validity of the transformation and eliminate off-the-ground objects. Additionally, a view consistency confidence map is utilized to mitigate the impact of moving objects and viewpoint variation. The localization process begins with the extraction of spatially aware deep features and the generation of view-consistent, on-ground confidence maps for both views. View-consistent key points are then detected from the ground view confidence map and matched with their corresponding points in the satellite view. The optimal pose is determined through an iterative search using a differentiable Levenberg-Marquardt (LM) algorithm. Using Google Maps [8] as the satellite view, we evaluate our method on two datasets: the Ford MultiAV Seasonal (FMAVS) [1] and the KITTI Datasets [7]. The results demonstrate the superiority of our proposed method, achieving mean localization error of less than {0.14m, 3.57◦} on KITTI with one front-facing onboard camera, and less than {0.88m, 0.74◦} on FMAVS with four surrounding onboard cameras. We summarize our contributions as below:
• the first sparse visual-only cross-view localization method that estimates accurate pose with low spatial and angular errors. • a view-consistent on-ground key point detector that reduces the impact of dynamic objects and viewpoint variations, as well as removes off-the-ground objects. • a spatial embedding that fully utilizes camera intrinsic and extrinsic information to improve the extraction of spatially aware visual features. • a multi-camera fusion approach that significantly improves localization accuracy. 2. related work Depth Aware Accurate Cross-view Localization. The task of accurate cross-view localization has gained attention in recent years. Researchers have mainly focused on developing solutions for Radar and LiDAR cross-view localization as depth information helps in aligning the ground and satellite perspectives. RSL-Net [26] estimates the robot
pose by registering Radar scans on a satellite image. This method was later extended to a self-supervised learning framework in [28]. Another work [27] matches the topdown representation of a LiDAR scan with 2D points detected from satellite images. These methods have limitations and are only effective in environments with strong prior structure knowledge, failing in general, non-urban environments. [2] performs localization on bird’s eye view (BEV) LiDAR intensity maps using deep feature matching between LiDAR scan and the intensity map. [34] extends this method by incorporating compressed binary maps. Hybrid sensor solutions have also been explored, such as in [16] where an aerial robot achieves global localization through the use of egocentric 3D semantically labelled LiDAR, IMU, and visual information. CSLA [6] and SIBCL [33] extract visual features from ground and satellite images and use LiDAR points to establish correspondence between the two views. CSLA [6] aims to estimate 2-DoF translation, while SIBCL [33] aims to estimate 3-DoF pose, including an additional orientation. All these methods critically rely on depth information to build the correspondence across the two views. In contrast, our method is a visualonly solution that aims to achieve comparable localization accuracy using cheaper commodity sensors. Visual Accurate Cross-view Localization. Most visualonly cross-view localization methods rely on homography transformations of the ground plane, as they lack reliable depth information. [36] aims to estimate 2-DoF translation using similarity matching and produces a dense spatial distribution to address localization ambiguities. HighlyAccurate [22] projects satellite features into the ground view and optimizes the robot pose through dense feature matching. One of its drawbacks is the limited ability to effectively eliminate outliers, such as noise caused by off-theground objects (which violates the assumption of homography transformation of the ground plane) and dynamic objects. As a result, their overall performance is limited. In contrast, our method constructs geometric correspondences across sparse view-consistent on-ground keypoints, ensuring that the pose estimation is based on accurate correspondences leading to improved precision. 3. our method Our work aims to achieve fine-grained cross-view localization by accurately estimating the 3-DoF pose, denoted by Ppred = {ϕpred, φpred, θpred}, where ϕ and φ represent lateral and longitudinal translations, respectively, and θ is the yaw angle. We are given a coarse initial pose Pinit = {ϕinit, φinit, θinit}, a reference satellite view image Is, and a set of ground-view images Ig = {Ii}Ni=1 captured by onboard cameras, where N is the total num-
ber of onboard cameras 2. An overview of the proposed PureACL is shown in Fig. 2. It builds upon three innovative modules: 1) Spatially Aware Feature and Confidence Extractor (SAFCE) (Sec. 3.2), 2) View-consistent On-ground Keypoint Detector (VOKD) (Sec. 3.3), and 3) Multi-camera Fusion (Sec. 3.4). Additionally, our approach utilizes two branches of objective functions inherited from the SIBCL method [33]: the Pose-Aware Branch (PAB) and the Recursive Pose Refine Branch (RPRB). In the following sections, we provide a detailed explanation of each module. 3.1. preliminary For completeness, we provide a brief description of the inherited PAB and RPRB. The PAB utilizes a triplet loss [19] that encourages accurate pose (ground truth) and penalizes incorrect (initial) poses by differentiating the residual between the ground truth and initial pose. Specifically, we compute the loss as follows:
Ltriplet = log(1 + e α(1−
∑ p w[p]Pinit
ρ(∥r[p]Pinit∥ 2 2)∑
p w[p]Pgt ρ(∥r[p]Pgt∥ 2 2)
)
), (1)
where α is a hyper-parameter set to 10 based on experimental results, ∑ p represents the sum of all key points, and ρ is a robust cost function as defined in [9]. The RPRB, on the other hand, aims to refine the initial pose iteratively using the LM algorithm to approach the ground truth pose. It starts with the coarsest level and
2Our method supports varying onboard camera quantities. In the experiments, we employed N = 4 for FMAVS and N = 1 for Kitti-CVL. uses features from each level successively, with each subsequent level initialized with the output of the previous level. Specifically, we update the pose as follows:
δt+1 = δt − (H+ λ diag(H))−1J⊤WΥ, (2)
where δ represents an individual element in the 3-DoF pose. t ∈ {1, · · · ,M × L} represents the current iteration, and M and L represent the iteration count per level and the total number of levels, respectively. The matrices Υ and W are formed by stacking the residuals r[p]P and weights w[p]P, while λ is the damping factors [21]. The Jacobian and Hessian matrices are defined as follows:
J = ∂r[p]P ∂δ = ∂F s[p]
∂[ps2D]P
∂[ps2D]P ∂δ and H = J⊤WJ, (3)
where [ps2D]P is the 2D projection of keypoints p onto the satellite image using the pose P, as shown in the right section of Fig. 6. Finally, we supervise the optimized pose by computing the re-projection error as follows:
Lreproject(Ppred) = ∑ ∥[ps2D]Ppred − [ps2D]Pgt∥22. (4) 3.2. spatially aware feature/confidence extractor Our approach improves the spatial embedding concept proposed in [14] by leveraging the camera’s intrinsic and extrinsic parameters to obtain highly accurate spatial information. The spatial embedding Eg/s ∈ Rh×w×3 has 3 channels: heading, distance, and height information. The explanation of these channels is shown in Fig. 3. To incorporate additional spatial embedding information between
the ground and satellite images, we transform the pixels in the onboard camera and satellite images into a common set of query world coordinates ( e.g., the GPS coordinates of the robot). In this coordinate system, the x-axis corresponds to the direction of motion, the y-axis points to the right, and the z-axis points downward. To perform this transformation, we use an inverse projection formula, which is shown in Eq. 5:
pj2g3D = Rj2gK −1 j (p j 2D ⊕ 1), (5)
where Kj is the intrinsic matrix of camera j, which can be either an onboard camera or a satellite camera j ∈ {iN1 , s}, and ⊕1 concatenates 1 to generate the homogeneous coordinate. The rotation from camera j to the ground coordinate, Rj2g , is obtained from the extrinsic information provided in the datasets for onboard cameras and from the initial coarse pose for the satellite camera. For onboard camera images, the 3D coordinate pi2g3D is a homogeneous coordinate with an unknown scale, while for satellite images, ps2g3D represents a world coordinate with an unknown down axis. This is because satellite images are approximated as parallel projections, and the equation for the calculating ps2D is given by:
ps2D = ( 1/γ 0 cu 0 1/γ cv ) ps3D, (6)
where (cu, cv) represents the center of the satellite image, and γ represents the meter-per-pixel ratio calculated using:
γ = r̃earth × cos(L̃× π180◦ )
2z̃ × s̃ , (7)
where r̃earth = 156543.03392 is radius of the Earth, L̃ is the latitude, z̃ = 18 and s̃ = 2 is the zoom factor and the scale of Google Maps [8], respectively. The heading information is embedded using the cosine value, which is symmetric to both positive and negative orientation noise. This enables distinction between 360- degree views, calculated using the x-axis (pj2g3D [0]) and yaxis (pj2g3D [1]) through trigonometric functions, as shown below:
Ej [0] = pj2g3D [0]/ √ pj2g3D [0] 2 + pj2g3D [1] 2. (8)
The normalized distance embedding of ground images is obtained by assuming all pixels lie on the ground plane:
Ej [1] = √
pj2g 3̃D [0]2 + pj2g 3̃D [1]2/D, (9)
where D is the maximum visible distance, set to 200 meters according to the satellite maps size and
pi2g 3̃D
= hi
pi2g3D [2] ×pi2g3D+ ti2g and p s2g 3̃D = ps2g3D + ts2g, (10)
where hi is the onboard camera height relative to the ground plane. For ground view images, the height embedding E[2] is equal to the value along the down axis, represented as pi2g3D [2]. In the case of satellite images, we set the height embedding to the minimal value to indicate a top-down perspective. Fig. 4 demonstrates that our approach effectively directs greater attention towards the features located in front of the robot by leveraging spatial embedding when using only the front onboard camera. The SAFCE employs a U-Net structure (Fν) to extract the satellite and ground-view feature maps, represented as F j = Fν(Ij ⊕ Ej), where j ∈ {iN1 , s}, and ⊕ denotes channel concatenation. The maps are then processed by a convolutional layer followed by a reverse sigmoid active function (Cψ) to produce view-consistent confidence maps (V j) and on-ground confidence maps (Oj) represented as V j , Oj = Cψ(F j). Each map has multiple resolutions, for example, F = {Fl ∈ Rhl×wl×cl}Ll=1 (Rhl×wl for V and O), where L = 3 is adopted in our setting. The maps are ordered from coarsest to finest level as l = {1, 2, 3}. The feature and confidence extraction from each image is performed in parallel using a shared-weight model, allowing for a flexible number of onboard cameras (N). The view-consistent confidence map V represents the confidence of objects appearing in both satellite and ground-view images. V is used as a multiplying factor for the point weights supervised by PAB and RPRB, and is penalized through the network training for the points with high residual (indicating distinct features between the cross-view). Considering the temporal gap between the two views, V effectively filters out objects that are temporally
or seasonally inconsistent, e.g. vehicles, pedestrians, and leaves. Additionally, it highlights view consistent reference objects, including road marks, lanes, building edges, and tree roots. An example is shown in Fig. 4 (row 2). More visualizations are shown in the supplementary. The on-ground confidence map O is designed to validate the homography transformation between the ground and satellite views. As a multiplying factor for the point weights, off-ground points that cause incorrect Geocorrespondence between the ground and satellite views, resulting in high residuals, have their on-ground confidence penalized to reduce the overall loss. Given that an incorrect height assumption in points can lead to erroneous projections on the satellite map, penalizing the satellite on-ground confidence map is not meaningful. So we only apply the backpropagation to the ground-view on-ground confidence map. An example of the learned confidence maps is shown in Fig. 4 (row 3). 3.3. view-consistent on-ground keypoint detector Fig. 5 illustrates the details of the proposed VOKD. The view-consistent and on-ground confidence maps of different resolutions are fused to generate the final confidence:
Ci = L∑ l=1 Ξ(N (V il ⊗Oil), (hL, wL)), (11)
where hL and wL represents the resolution of the fine level confidence map, Ξ is an interpolation function, and N is a min-max normalisation, and ⊗ represents element-wise multiplication. The bottom row of Fig. 4 demonstrates the efficacy of the fused confidence map in filtering out off-theground objects and emphasizing temporal stability and view consistency in cues such as road markings and curbs for the subsequent pose estimation. More visual examples can be found in the supplementary. In order to achieve on-ground keypoint detection, our focus is limited to the area below the focal point, which corresponds to the on-ground area and is our primary interest. From this area, we select the top-K points with the highest confidence score from the fused confidence map. To avoid overcrowding of keypoints, we partition the fused confidence map into smaller patches of size 8× 8 and enforce a limit of one detected keypoint per patch. This approach ensures that the selected keypoints are well-distributed across the on-ground area, thereby improving the accuracy of subsequent pose estimation. The left part of Fig. 6 displays the detected view-consistent on-ground 2D keypoints. These 2D keypoint coordinates pi2D are used to calculate their corresponding 3D ground world coordinates pi2g
3̃D through the
equations Eq. (5) and Eq. (10). The right part of Fig. 6 shows the projection of these 3D coordinates onto the satellite image (ps2D = Ks(Rg2sp i2g 3̃D + tg2s)). 3.4. multi-camera fusion Our method is flexible and can handle multiple cameras as input, without any restrictions on the field of view. In case there is a potential overlap between the views captured by adjacent cameras, keypoints detected in one camera may be visible in another camera as well. In such cases, we select the point feature with the highest weight:
wg[p] = N
max i
(V i ⊗Oi)[pi2D], (12)
F g[p] = F i[pi2D], i = arg N
max i
(V i ⊗Oi)[pi2D]. (13) 4. datasets To evaluate the effectiveness of the proposed method, we followed the existing methods [22, 33] and conducted experiments on two widely used autonomous driving datasets: the FMAVS dataset [1] and KITTI dataset [7]. We adopted the augmentation method proposed by [33], which involved incorporating spatially-consistent satellite images obtained from Google Maps [8] using the GPS tags provided in the datasets. The satellite images had a resolution of 1, 280 × 1, 280 pixels and a scale of 0.22m per pixel for FordAVCVL, and 0.2m per pixel for KITTI-CVL. In the FMAVS dataset, we utilized query images from four cameras (front left, rear right, side left, and side right) to capture the surrounding environment, providing an almost 360-degree field of view with minimal overlap. Since the KITTI dataset provides only front-facing stereo camera images, we used the images from the left camera of the stereo pair as query images. The FMAVS includes multiple vehicle traversals over a consistent route. To evaluate our proposed method, we split the three traversals of the ‘Log4’ trajectory into training, validation, and test sets, following the split strategy described in [33]. The KITTI dataset [7] comprises various trajectories taken at different times. To assess our model’s generalization ability, we selected test sets from different trajectories based on [22]. 5. experiments Metrics. Our objective is to estimate the 3-DoF pose, which includes lateral, longitudinal, and yaw information. We measure the accuracy of our proposed method by reporting the median and mean errors in lateral and longitudinal translations (in meters) and yaw rotation (in degrees). In addition to these metrics, we also follow the evaluation criteria outlined in [33] and report the average localization recall 3 at distances of 0.25m, 0.5m, 1m, and 2m, as well as at yaw rotation angles of 1◦, 2◦, and 4◦. Implementation Details. In our experiments, we use an input size of 432 × 816 for the ground-view images in the Multi-AV Seasonal Dataset, and 384× 1248 for the KITTI Dataset. RTK GPS 4 is used as the ground truth pose. We add some noise to the RTK GPS poses to generate the initial pose. Unless otherwise stated, the initial pose is randomly sampled with a yaw angle error of ±15◦ and lateral, longitudinal shifts of ±5 meters, as the accuracy of GPS is within 4.9 meters in open sky conditions [30]. We detect 256 ground keypoints from each input ground-view image. We set the batch size to b = 3 for training on an NVIDIA RTX 3090 GPU, and use the Adam optimizer [12] with a learning rate of 10−4. The feature extractor weights are initialized with the pre-trained weights from [33], which are trained on the KITTI-CVL dataset. The weights of the confidence generator are initially randomly initialized to values near 0. Through the application of the inverse sigmoid activation function, these weights are tuned to initialize the confidence values in proximity to 50%. Inference Speed. The SAFCE processes four query ground-view images and one satellite image in approximately 200ms. The detection time for all ground keypoints is about 3.5ms. The optimization process, which runs for 20 iterations at each of the three levels, takes a total of approximately 200ms. Qualitative Results. We compare our method with recent state-of-the-art (SOTA) visual-only methods, CVML [36] and HighlyAccurate [22], as well as the LiDAR-visual hybrid method SIBCL [33]. We present the evaluation results on the KITTI-CVL and FordAV-CVL datasets in Tab. 1 and Tab. 2. To ensure a fair comparison, we trained HighlyAccurate [22] and SIBCL [33] under the same image resolution and initial pose noise range. Since CVML [36] is unable to accurately estimate fine-grained orientations, we only evaluated its performance in terms of location estimation. We trained their model with ground truth orientation. Tab. 1 presents an evaluation of our method’s ability to generalize to previously unseen routes in the KITTI-CVL dataset using a front camera. For translation accuracy, our method exhibits superior performance compared to SOTA
3The percentage of the prediction pose that is within a certain range. 4RTK GPS achieves an accuracy of 2 cm or better [5]. methods, with a significant reduction in the translation error. Specifically, our method achieves a reduction of 86% and 94% in mean lateral and longitudinal localization error. While our orientation accuracy is slightly less accurate than the LiDAR-based method, it maintains a comparable performance to SOTA visual-only method [22] in terms of rotation error. These results demonstrate the ability of our method to generalize to a wide range of scenes. The performance of our method on cross-season generalization is presented in ‘Log4’5 of Tab. 2. The test set in this case includes data from different time and seasons compared to the training set, which allows us to evaluate the performance of our method under varying lighting and seasonal conditions. Furthermore, in ‘Log4→5’ of Tab. 2, we analyze our method’s generalization capability on an unseen route. In both cases, our method outperforms existing SOTA methods by significant margins. Specifically, we achieve a reduction of 52% and 43% in mean localization lateral error, 62% and 52% in mean localization longitudinal error, and 67% and 17% in mean orientation error in terms of seen and unseen routes, respectively. These results once again demonstrate the strong performance and robust generalization capabilities of our proposed method. Performance with Varying Numbers of Camera Inputs. We investigate the impact of multiple onboard cameras on
5The trajectory of ‘Log4’ was selected for method evaluation in SIBCL [33] due to its relatively good satellite view alignment. Additionally, we evaluated other logs and the evaluation results can be found in the supplementary material. the FordAV-CVL dataset and evaluate our method using different camera setups. These setups include the front camera (Front) in the 1-camera setting, two side cameras (2Sides), the front and rear cameras (2FR) in the 2-camera setting, and all front, rear, and two side cameras (4Cams) in the 4-camera setting. Our findings indicate that even with the use of a single front camera (‘Ours (Front)’ in Tab. 2), our method outperforms the SOTA methods. Additional camera inputs lead to further improvements in performance, particularly with regards to orientation estimation, which can be attributed to the fact that a larger field of view (FoV) provides more information to accurately estimate orientation. Furthermore, our study reveals that the front and rear cameras provide more information for localization, whereas the left and right cameras contribute more to the lateral estimation. This could be attributed to the limited visibility of noticeable localization features such as road marks in the side cameras or the sensitivity of the side cameras to the roll angle. It is noteworthy that our method, despite utilizing four onboard cameras, consumes less memory (4499 MB) than HighlyAccurate [22], which requires 6445 MB due to its use of sparse purification. Performance under Different Initial Poses. The proposed method utilizes the LM algorithm and is subject to a convergence range 6 constraint. If the provided initial pose falls outside of this range, the method may fail to converge. To evaluate the method’s robustness under a more stringent
6The convergence range refers to the region in the pose space where the method can converge to the ground truth pose. scenario, we conducted experiments using a comprehensive set of initial poses. The results, shown in Fig. 7, indicate that our approach achieves a satisfactory level of accuracy even when the initial pose is subjected to yaw angle errors of up to ±60◦ and lateral and longitudinal shifts of up to ±15m. The longitudinal estimation is found to be more sensitive to the initial pose compared to the lateral estimation. Moreover, in KITTI-CVL datasets that rely solely on a front onboard camera, a larger difference between the mean and median values suggests more cases falling outside the convergence range. Therefore, the use of multiple camera inputs, such as in the FordAV-CVL dataset with four cameras, can significantly expand both the translation and orientation coverage ranges, with the orientation coverage range being notably more improved. 6. ablation study Two Confidence Maps. The proposed method adopts two types of confidence maps (“2c w/o SE”), i.e., viewconsistent and on-ground maps. An alternative approach was to use a single confidence map (“1c w/o SE”), which combined both on-ground and view-consistent confidences, and disabled gradient backpropagation from the satellite view. A comparison of using different types of confidence maps is reported in Tab. 3. We can see that using two confidence maps with distinct gradient backpropagation mechanisms leads to better performance compared to the alternative approach. Spatial Embedding. We study the impact of Spatial Embedding by comparing the performance of our algorithm with (“Full”) and without Spatial Embedding (“2c w/o SE”), as shown in Tab. 3. The results demonstrate that incorporating Spatial Embedding significantly improves the performance of the PureACL algorithm. View-consistent On-ground Keypoint Detector. We compare our keypoint detection design with the SOTA SuperPoint [3]. In this comparison, we use SuperPoint to detect keypoints and combine it with the two confidence maps to reduce the weights of points located on dynamic objects or above the ground plane. The results are presented in Tab. 3 as “SuperPoint”. Our view-consistent on-ground point detector (“Full”) outperforms “SuperPoint” as it detects a sufficient number of on-ground keypoints, which is more beneficial for cross-view localization. Multi-camera Fusion Method. We compare two fusion methods for keypoints captured by multiple onboard cameras: selecting the highest-confidence 2D projection (“Full”), which is used in our proposed method, and computing the mean of features and confidence scores across all visible onboard camera images (“Mean fusion”). The results in Tab. 3 show that highest-confidence fusion outperforms Mean fusion due to more reliable selection. 7. conclusion This paper presents PureACL, a novel cross-view localization approach for accurate 3-DoF pose estimation that supports flexible multi-camera inputs. Our approach utilizes a view-consistent on-ground keypoint detector to handle dynamic objects and viewpoint variations while removing off-the-ground objects to establish the homography transformer assumption. Additionally, PureACL incorporates a spatial embedding that maximizes the use of camera intrinsic and extrinsic information to reduce visual matching ambiguity. PureACL is the first sparse visual-only approach and the first visual-only cross-view method capable of achieving a mean translation error of less than one meter. 8. acknowledgements The research is funded in part by an ARC Discovery Grant (grant ID: DP220100800) to HL. a. evaluation of other fordav-cvl dataset logs 12  b. performance with different initial poses 12 C. Visualization of Confidence Maps 13 a. evaluation of other fordav-cvl dataset logs The ’Log4’ trajectory was chosen for method evaluation in SIBCL [33] owing to its alignment accuracy with the satellite image. Furthermore, we also evaluated other logs from the FordAV-CVL Dataset in Tab. 4 to supplement the results presented in Tab. 2 of the main paper. There are three travelings included in every log: ’2017-08-04-26’, ’2017-07-24’, and ’2017-10- 26’. For the purpose of training, evaluation, and test dataset split, we use ’2017-07-24’ as the evaluation dataset for all logs. We select the traveling sequence with a higher number of images as the training dataset. Specifically, the training dataset of ’Log1’ and ’Log3’ is ’2017-10-26’, whereas the training dataset of ’Log4’ and ’Log5’ is ’2017-08-04-26’. The results demonstrate that our method is capable of estimating accurate 3-DoF pose with low spatial and angular errors in various scenarios, including freeway (log1), residential (log3, log5), university (log4) and vegetation (log4, log5). b. performance with different initial poses In our main paper, we presented a chart illustration in Fig.7. Here, we further provide complete metrics results in Tab. 5 and Tab. 6. By presenting these tables, we aim to provide a comprehensive view of the data and enable readers to analyze the metrics more thoroughly. In order to facilitate comparison, we have included a performance analysis with a single front onboard camera from the FordAV-CVL dataset, as depicted in Fig. 8. To bring a comprehensive view of the data and enable readers to analyze the metrics more thoroughly, we further provide complete metrics results in Tab. 5 and Tab. 6, as a supplementary of the chat illustration in Fig. 7 in our main paper. In order to facilitate comparison, we have included a performance analysis with a single front onboard camera from the FordAV-CVL dataset, as depicted in Fig. 8