Background noise considerably reduces the accuracy and reliability of speaker verification (SV) systems. These challenges can be addressed using a speech enhancement system as a front-end module. Recently, diffusion probabilistic models (DPMs) have exhibited remarkable noisecompensation capabilities in the speech enhancement domain. Building on this success, we propose Diff-SV, a noise-robust SV framework that leverages DPM. Diff-SV unifies a DPM-based speech enhancement system with a speaker embedding extractor, and yields a discriminative and noise-tolerable speaker representation through a hierarchical structure. The proposed model was evaluated under both in-domain and out-of-domain noisy conditions using the VoxCeleb1 test set, an external noise source, and the VOiCES corpus. The obtained experimental results demonstrate that Diff-SV achieves state-of-the-art performance, outperforming recently proposed noise-robust SV systems. 1. introduction Speaker verification (SV) involves determining whether the speaker of a given utterance matches an authorized identity. Although recent advances in deep learning have yielded highly accurate deep neural network-based SV systems in clean and controlled environments [1–4], the performances of these systems degrade significantly under noisy, real-world conditions. Background noise diminishes speech intelligibility and quality, consequently hindering the extraction of accurate speaker representations [5,6]. Prior research has incorporated speech enhancement models as front-end modules in SV systems to mitigate the negative effects of noise [7–9]. In the speech enhancement field, generative models such as variational autoencoders [10], generative adversarial networks [11], and flowbased models [12] are widely used to generate clean speech from noisy speech. Recently, diffusion-based generative models have exhibited outstanding generation capabilities with respect to traditional generative models [13, 14]. As a representative approach, the denoising diffusion probabilistic model (DDPM) learns to predict Gaussian noise added to original data through a series of steps (i.e., forward diffusion process) and generates data by denoising random Gaussian noise iteratively via the Markov chain property using the trained model (i.e., reverse diffusion process) [15]. Leveraging the DDPM’s potent data generation capabilities, researchers have proposed speech enhancement systems that adopt diffusion principles. For instance, Lu et al. [16] recovered clean speech from noisy speech using the DDPM’s Markov chain process. Furthermore, CDiffuSE [17] was developed for non-Gaussian real-world noise adaptation by incorporating noisy speech into the forward and reverse processes of DDPM. †Corresponding author. This work was supported by the National Research Foundation of Korea(NRF)
grant funded by the Korea government. (MSIT) (2023R1A2C1005744)
Despite its exceptional noisy-compensation capabilities, the direct application of a DDPM-based speech enhancement model may not be ideal for SV. Due to the stochastic generation process, DDPMs tend to produce low-consistency data (i.e., different data from the same input) [15,18]. Consequently, speaker embedding extractors may yield representations with increased and decreased intra- and inter-class variances, respectively, ultimately degrading SV performance. Furthermore, the numerous reverse steps owing to the Markov chain entail a considerable computational cost [18], complicating the joint learning process with the SV system. These challenges can be alleviated using a score-based generation model [19] (refer to the Section 2 for details). In comparison with the stepwise prediction process of the DDPM, the score-based diffusion probabilistic model (DPM) generates data using fewer steps by numerically solving differential equations in continuous-condition sampling procedures. In addition, by employing an ordinary differential equation (ODE) solver instead of a stochastic differential equation (SDE) solver, deterministic data sampling can be achieved with minimal distribution perturbation. In this study, we propose a unified noise-robust SV framework named Diff-SV, which applies a score-based DPM as a front-end module of a speaker embedding extractor. Diff-SV uses not only a score-based DPM (denoiser) solely, but also an auxiliary enhancement system (enhancer) to remove noise from the input. Therefore, the denoiser only needs to remove residual noise contained in the features derived from the enhancer, allowing for highly stable and effective data sampling. Furthermore, the proposed framework combines the original noisy spectrogram, the enhanced feature from the enhancer, and the denoised feature from the denoiser to extract speaker embeddings. By simultaneously leveraging features with different characteristics, this hierarchical approach encourages embedding extraction that is robust to distributional perturbation and informative. VoxCeleb1 [20] was used for model training, and this dataset was augmented with the MUSAN dataset [21]. We evaluated the generalizability of the model under in- and out-of-domain noise conditions using external data. Our proposed Diff-SV outperformed recent SV systems, demonstrating the efficacy of the framework through ablation experiments and visualizations. 2. score-based diffusion probabilistic models DPMs [13] have demonstrated promising results in the field of generative modeling by training the model to reverse data from noise. Recently, Song et al. [19] introduced a generalized framework for DPMs, employing SDEs and score, the gradient of the probability density function (p) for the data z. The score can be simply expressed as follows:
∇zlogp(z). (1)
Considering z0∼p0 and zT ∼pT as the data and prior probability distributions, respectively, the forward diffusion process with a continuous time variable t∈ [0,T ] is formulated as a solution to the following SDE:
dzt=f(zt,t)dt+g(t)dwt, (2)
ar X
iv :2
30 9. 08 32
0v 2
[ ee
ss .A
S] 1
4 D
ec 2
02 3
where wt represents the standard Wiener process, also known as Brownian motion. The drift coefficient for zt and t is denoted as f(zt,t), and g(t) signifies the diffusion coefficient. The infinitesimal timesteps close to 0 are designated as dt. From zT , the sample z0 can be recovered via the reverse diffusion process (i.e., data sampling or data generation) as follows:
dzt= [ f(zt,t)−g(t)2∇zt logpt(zt) ] dt+g(t)dw̄, (3)
In this equation, w̄ indicates a standard Wiener process, with time changing from T to 0. As accurately computing ∇zt logpt(zt) is difficult during the reverse process, score-based DPMs train a score estimator sθ(zt,t) to predict the score (i.e., score matching) for data sampling. 3. proposed framework In this section, we introduce Diff-SV, the proposed noise-robust SV framework that uses a score-based DPM, and the overview is illustrated in Fig. 1 (a). The architecture of Diff-SV includes three primary components, namely, an enhancer, a denoiser, and an extractor. Initially, the enhancer pre-processes the original Mel-spectrogram by reducing noise. Subsequently, the DPM-based denoiser further refines the enhanced feature. Lastly, the extractor produces speaker embeddings by hierarchically processing features obtained from previous stages. All modules are trained in a unified approach. 3.1. enhancer Lu et al. [16] employed a DPM-based speech enhancement system that uses the original noisy speech as the initial value in the reverse diffusion process to produce a denoised speech. However, we hypothesized that preliminary enhanced features can closely resemble to clean speech, and thus, allowing the DPM to sample denoised features more reliably compared to using the original noisy input. This assumption was similarly applied to the speech synthesis domain using DPM, which demonstrated impressive naturalness of the generated speech [25]. Therefore, Diff-SV utilizes an enhancer to obtain enhanced features for initializing the sampling process conducted in the denoiser, a DPM-based speech enhancement system. As depicted in Fig. 1 (b), the enhancer (fenh) comprises a linear block containing two fully connected layers, a Mish activation [26], a dropout layer, and transformer blocks [27]. The goal of the enhancer is to extract the enhanced features (x̂) from the noisy Mel-spectrogram (x) as follows:
x̂=fenh(x), x, x̂∈RL×F×1×B, (4)
where L, F , and B represent the feature length, feature frequency, and batch size, respectively. The enhancer is optimized to minimize the L2 distance between the output and the clean Mel-spectrogram (y). Lenh= 1
B B∑ i=1 ||yi−x̂i||22. (5) 3.2. denoiser The denoiser, which comprises a score-based DPM, removes any residual noise from the enhanced features. We designed the denoiser by transforming all data distributions of infinite-time-horizon forward diffusion to N(x̂,I), rather than N(0,I). This concept is inspired by the methods reported in [25], which generalizes the data distribution of diffusion processes. Therefore, the terminal condition zT can be considered as data sampled from a normal distribution with the enhancer’s output (x̂) as the mean. We redefined the standard forward diffusion process (Equation (2)) as the following SDE:
dzt= 1
2 (x̂−zt)βtdt+
√ βtdwt, (6)
where βt is the noise scheduling function [19]. Consequently, the denoiser can yield the denoised feature from the enhanced feature using the reverse diffusion process. To generate high-fidelity features for SV tasks, we employed ODEs with the random Wiener terms removed and formulated the sampling process as follows:
dzt= 1
2 (x̂−zt−sθ(zt, t, x̂))βtdt, (7)
As ∇zt logpt(zt) cannot be computed accurately in the reverse process, we estimated the score using sθ. As shown in Fig. 1 (c), the structure of the score estimator is based on U-Net as reported in [25] and takes zt, t, and x̂ as input to predict the score corresponding to each time point t. Thus, the feature z0, which is deterministically derived from zT , can be used in the SV framework owing to less perturbation of the speaker distribution and fast sampling. To train sθ, we calculated the expectation by marginalizing over a tractable transition kernel. Given that p(zt|z0) follows a Gaussian distribution, the loss function for score estimation is as follows:
Ldif=Et∼U(0,T)Ez0∼p0Eϵt∼N(0,I)∥sθ(zt, t, x̂)−σ −1 t ϵt∥22, (8) where σt= √ 1−e− ∫ t 0β(s)ds. 3.3. extractor In the proposed framework, the ResNet-based extractor (fext) derives embedding (v) (Fig. 1 (d)). To configure a unified noise-robust SV framework, the original, enhanced, and denoised features are jointly fed to the extractor. However, we empirically observed that the output of Diff-SV failed to converge, possibly due to gradient exploding or vanishing problems caused by repeated Gaussian denoising operations during the denoiser’s reverse process. Therefore, we delivered z0 after the stop gradient calculation operation (sg) to prevent the exploding or vanishing gradient of the denoiser from occurring by backpropagation through the extractor’s objective function as follows:
v=fext(x̃),
x̃=[x, x̂, sg(z0)], x̃∈RL×F×3×B, (9)
where [·] denotes the concatenation of each element on the channel axis. Thus, using a combination of (i) noisy but non-destructive original features x, (ii) enhanced features x̂ considering the target task, and (iii) independently denoised features z0, which are almost similar to clean features, the extractor can enrich the information from different perspectives while mitigating the distortion of speaker information via speech enhancement. The embeddings were optimized to classify the speakers included in the training data using an additional linear layer (W ), and we employed the additive angular margin (AAM)-Softmax function [28]. Lspk=− 1
B B∑ i=1 log es·(cos(θci,i+m)) es·(cos(θci,i+m))+ ∑ j≠yi es·(cos(θj,i)) , (10)
where θj,i is the angle between the j-th weight vector Wj and i-th embedding vector vi, and ci denotes the speaker label of vi. Additionally, the scaling factor (s) and margin (m) are set to 0.3 and 30, respectively. Finally, Diff-SV was trained to optimize the following three losses:
L=Lenh+Ldif+Lspk. (11) 4. experiments  4.1. datasets The models were trained using VoxCeleb1 [20] training data, consisting of 1,211 speakers. We employed MUSAN corpus [21] to generate noise
data, which we divided into non-overlapping training and test subsets. Noisy data for training was constructed using the MUSAN training subset with a randomly selected signal-to-noise ratio (SNR) between 0–20, further augmented with room impulse response reverberation, pitch shift, and gain variations. We constructed three test conditions to evaluate the noise robustness of our proposed model from different perspectives. First, in-domain speech evaluation with in-domain noisy data was conducted by augmenting the VoxCeleb1 test set with the SNR values {0, 5, 10, 15, and 20} for each noise type in the MUSAN test subset. Although the noise sources are separated, the data distributions within the same corpus could be similar. Therefore, we organized an in-domain speech evaluation with out-of-domain noisy data using a separate noise source. We used the Nonspeech100 dataset [29] as the out-of-domain noise source and evaluated the system with the same configurations as those corresponding to the in-domain noise evaluation conditions. Finally, to verify the generalizability of the system, we conducted out-of-domain speech evaluation with out-of-domain noisy data. To achieve this, we used the VOiCES development and evaluation dataset. The VOiCES dataset [30] was collected at different distances and under various acoustic conditions using array microphones in rooms of different sizes. We evaluated the models based on the parameter that achieved the best performance in the clean VoxCeleb1 test scenario. 4.2. implementation details We inputted an 80-dimensional log Mel-spectrogram extracted by conducting a 512-point fast Fourier transform with a Hamming-window width of 25 ms and 10-ms hopping. Diff-SV was trained using the AMSGrad optimizer [31] with a mini-batch of 160. The initial learning rate (LR) was 1e-3, which was decreased to 1e-7 over four cycles for 320 epochs using a cosine LR scheduler. The models were compared based on their equal error rates (EER) obtained using the cosine similarity score and the minimum detection cost function (Cmindet ). The enhancer in the proposed framework consists of four transformer blocks with a hidden size of 80 dimensions, and the architecture of our extractor is identical to ResNet structures reported in [9], respectively. The baseline had the same structure as that of the extractor, except that the channel size of the first convolution layer was changed from 3 to 1 using only the original features as the input. Additional details are present under the experimental code at https://github.com/wngh1187/Diff-SV. 5. results and discussion Table 1 lists the in-domain evaluation results using the VoxCeleb1 test set and the MUSAN evaluation partition. The baseline surpasses the average results obtained from noise-robust SV systems developed in recent years This improvement can be attributed to the optimization of the advanced objective function (AAM-Softmax) and the application of various data augmentation techniques during training. Nevertheless, the baseline performance deteriorates under considerably noisy conditions (e.g., under babble and noise with an SNR of 0 dB), indicating the need for specialized noise-reduction approaches. Our proposed method, Diff-SV, demonstrates enhanced noise robustness with respect to that of the baseline across all evaluation scenarios. Diff-SV achieves a relative error reduction (RER) that is 14.29% higher while using fewer parameters than ExU-Net, the top-performing noise-robust system (4.55% vs. 3.9%). Table 2 presents the results of each model under out-of-domain noise evaluation with in-domain speech conditions. In contrast to in-domain noise evaluation results, ExU-Net showed superior performance with respect to the baseline, indicating the effectiveness of the noise compensation. Moreover, Diff-SV outperforms all models, achieving an average EER and Cmindet of 4.65% and 0.247, respectively. These results reveal the noise robustness of the proposed model under in- and out-of-domain noise scenarios. Table 3 displays the results obtained from each model using the VOiCES development and evaluation datasets. Diff-SV achieves RERs of 15.38% and 24.98% for each dataset with respect to the baseline due to the proposed noise-compensation modules. Based on the evaluation results obtained from the out-of-domain speech and noise datasets, we confirmed the outstanding generalizability of our proposed framework. An ablation study was performed to evaluate the efficacy of each component of the proposed framework. The models were evaluated based on their corresponding EER and Cmindet under the same evaluation scheme of Table 1. The results are displayed in Table 4, in which system #1 corresponds to Diff-SV. System #2 signifies an architecture that employed a pre-trained enhancer and denoiser to train an extractor, rather than applying unified training. This approach is sub-optimal because the enhancer and denoiser are not specifically designed for SV tasks, making early stopping challenging. In addition, feeding only the denoised features into the extractor without applying the sg operation (system #3), instated of following the hierarchical input structure, leads to non-convergence. We suspect that this stems from the gradient vanishing or exploding in the repeated score matching operation during the reverse process of the denoiser. Furthermore, the results of systems #4 and #5 reveal that excluding the denoiser and enhancement loss significantly degrades the generaliza-
tion performance. Therefore, these results emphasize the importance of integrating the components of Diff-SV with a hierarchical input structure. We visualized the internal spectrograms of the Diff-SV framework to verify the denoising effectiveness. Fig. 2 presents the original spectrogram (a) of a randomly chosen speech from the VoxCeleb1 evaluation set, a noisy spectrogram (d) synthesized by adding music at an SNR of 0 dB, and the output features of the enhancer (b), (e) and denoiser (c), (f). As noise is introduced, the original utterance becomes contaminated with signals of varying shapes and frequency bands ((a) vs. (d)). Although an enhancer trained to map input to clean speech can effectively eliminate noise, simultaneous optimization for the target task results in over-smoothing (blurring) or collapse (horizontal lines) of low- and high-frequency information ((a) and (d) vs. (b) and (e)). Due to the difficulty of accurately removing Gaussian noise during the inversion process, the output of the denoiser retains more noise but exhibits better pitch information owing to the superior generalizability of DPM ((b) and (e) vs. (c) and (f)). Therefore, by simultaneously supplying features with complementary properties, Diff-SV is capable of producing speaker embeddings that are highly discriminative and noise-tolerant. 6. conclusion Based on the impressive generative capabilities of DPM, we propose a unified noise-robust SV framework that uses a DPM-based speech enhancement system. Using the ODE solver of the score-based DPM, DiffSV generates denoised features from the enhancer’s output in an efficient and deterministic manner. Furthermore, the complementary hierarchical inputs improve the extractor’s ability to derive discriminative and noiserobust speaker embeddings. Our proposed model outperforms recently reported models, including baselines, under both in- and out-of-domain scenarios. Additionally, we demonstrate the effectiveness of Diff-SV and the importance of each component through visualization and ablation analyses. 