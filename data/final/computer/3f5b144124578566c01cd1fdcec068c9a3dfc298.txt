Community detection is an important and powerful way to understand the latent structure of complex networks in social network analysis. This paper considers the problem of estimating community memberships of nodes in a directed network, where a node may belong to multiple communities. For such a directed network, existing models either assume that each node belongs solely to one community or ignore variation in node degree. Here, a directed degree corrected mixed membership (DiDCMM) model is proposed by considering degree heterogeneity. An efficient spectral clustering algorithm with a theoretical guarantee of consistent estimation is designed to fit DiDCMM. We apply our algorithm to a small scale of computer-generated directed networks and several real-world directed networks. 1. introduction Many real-world complex networks have community structure such that nodes within the same community (also known as cluster or module) have more links than across communities. For example, in social networks, communities can be groups of students in the same department; in co-authorship networks, a community can be formed by researchers in the same field. However, community structure for a real-world network is usually not directly observable. To process this problem, community detection, also known as graph clustering, is a popular tool for uncovering a latent community structure in a network [1,2]. For decades, many community detection methods have been proposed for non-overlapping undirected networks in which each node belongs to a single community, and the interactions between two nodes are symmetric or undirected. The stochastic block model (SBM) [3] is a popular generative model for non-overlapping undirected networks. In SBM, it is assumed that each node only belongs to one community and that nodes in the same community have the same expectation degrees. [4] proposes the classical degree corrected stochastic block model (DCSBM) which extends SBM by considering variation in node degree. In recent years, numerous algorithms have been developed to estimate node community for non-overlapping undirected networks generated from SBM and DCSBM, see [5–15]. For recent developments about SBM, see the wonderful review paper [16]. However, in most real-world networks, a node may belong to more than one community at a time. In recent years, the problem of estimating mixed memberships for the undirected network has received a lot of attention [17–29], and references therein. [17] extends the SBM model from non-overlapping undirected networks to mixed membership undirected networks and designs the mixed membership stochastic block (MMSB) model. Based on the MMSB model, ref. [24] designs a model called the degree corrected mixed membership (DCMM) model by considering degree heterogeneity, where DCMM can also be seen as an extension of the non-overlapping model DCSBM, and ref. [24] also develops an efficient and provably consistent spectral algorithm. [27] presents a spectral algorithm under MMSB and establishes per-node rates for mixed memberships by sharp
Entropy 2023, 25, 345. https://doi.org/10.3390/e25020345 https://www.mdpi.com/journal/entropy
row-wise eigenvector deviation. [29] proposes an overlapping continuous community assignment model (OCCAM), which is also an extension of MMSB, by considering degree heterogeneity. To fit OCCAM, ref. [29] develops a spectral algorithm requiring a relatively small fraction of mixed nodes when building theoretical frameworks. [26] finds the cone structure inherent in the normalization of the eigen-decomposition of the population adjacency matrix under DCMM and develops a spectral algorithm to hunt corners in the cone structure. Though the above works are encouraging and appealing, they focus on undirected networks. In reality, there exist substantial directed networks, such as citation networks, protein–protein interaction networks, and the hyperlink network of websites. In recent years, a lot of works with encouraging results have been developed for directed networks. [30] proposes a stochastic co-block model (ScBM) and its extension DC-ScBM by considering degree heterogeneity to model non-overlapping directed networks, where ScBM and DC-ScBM can model directed networks whose row nodes may be different from column nodes, and the number of row communities may also be different from the number of column communities. [31] studies the theoretical guarantees for the algorithm DSCORE [32] and its variants designed under DC-ScBM. [33] studies the spectral clustering algorithms designed by a data-driven regularization of the adjacency matrix under ScBM. [34] studies higher-order spectral clustering of directed graphs by designing a nearly linear time algorithm. Based on the fact that the above works only consider non-overlapping directed networks, ref. [35] develops a directed mixed membership stochastic block model (DiMMSB), which is an extension of ScBM, and models directed networks with mixed memberships. DiMMSB can also be seen as a direct extension of MMSB from an undirected network to a directed network. Recall that DCSBM, DCMM, and DCScBM are extensions of SBM, MMSB, and ScBM by considering node degree variation, respectively, this paper aims at proposing a model as an extension of DiMMSB by considering node degree heterogeneity and building an efficient spectral algorithm to fit the proposed model. In this paper, we focus on the directed network with mixed membership. Our contributions are as follows:
(i) We propose a novel generative model for directed networks with a mixed membership, the directed degree corrected mixed membership (DiDCMM) model. DiDCMM models a directed network with mixed memberships when row nodes have degree heterogeneities, while column nodes do not. We present the identifiability of DiDCMM under popular conditions which are also required by models modeling mixed membership networks when considering degree heterogeneity. Meanwhile, our results also show that modeling a directed network with mixed membership when considering degree heterogeneity for both row and column nodes needs nontrivial conditions. DiDCMM can be seen as an extension of the DCScBM model from a non-overlapping directed network to an overlapping directed network. DiDCMM also extends the DCMM model from an undirected network to a directed network and extends the DiMMSB model by considering node degree heterogeneity. For a detailed comparison of our DiDCMM with previous models, see Remark 2. (ii) To fit DiDCMM, we present a spectral algorithm called DiMSC, which is designed based on the investigation that there exists an ideal cone structure inherent in the normalized version of the left singular vectors and an ideal simplex structure inherent in the right singular vectors of the population adjacency matrix. We prove that our DiMSC exactly recovers the membership matrices for both row and column nodes in the oracle case under DiDCMM, and this also supports the identifiability of DiDCMM. We obtain the upper bounds of error rates for each row (and column) node and show that our method produces asymptotically consistent parameter estimations under mild conditions. Our theoretical results are consistent with classical results when DiDCMM degenerates to SBM and MMSB under mild conditions. Numerical results of simulated directed networks support our theoretical results and show that our approach outperforms its competitors. We also apply our algorithm to several real-
world directed networks to test the existence of highly mixed nodes and asymmetric structures between row and column communities. Notations. We take the following general notations in this paper. For a vector x and fixed q > 0, ‖x‖q denotes its lq-norm. For a matrix M, M′ denotes the transpose of the matrix M, ‖M‖ denotes the spectral norm, ‖M‖F denotes the Frobenius norm, and ‖M‖2→∞ denotes the maximum l2-norm of all the rows of M. Let rank(M) denote the rank of matrix M. Let σi(M) be the i-th largest singular value of matrix M, and λi(M) denote the i-th largest eigenvalue of the matrix M ordered by the magnitude. M(i, :) and M(:, j) denote the i-th row and the j-th column of matrix M, respectively. M(Sr, :) and M(:, Sc) denote the rows and columns in the index sets Sr and Sc of matrix M, respectively. For any matrix M, we simply use Y = max(0, M) to represent Yij = max(0, Mij) for any i, j. For any matrix M ∈ Rm×m, let diag(M) be the m×m diagonal matrix whose i-th diagonal entry is M(i, i). Here, 1 and 0 are column vectors with all entries being ones and zeros, respectively; ei is a column vector whose i-th entry is one, while other entries are zero. C is a positive constant that may vary occasionally. 2. the directed degree corrected mixed membership model Consider a directed network N = (Vr,Vc, E), where Vr = {1, 2, . . . , nr} is the set of row nodes, Vc = {1, 2, . . . , nc} is the set of column nodes (nr and nc indicate the number of row nodes and the number of column nodes, respectively), and E is the set of edges. Note that when Vr = Vc such that row nodes are the same as column nodes, N is a traditional directed network [31,36–42]; when Vr 6= Vc, N is a bipartite network (also known as a bipartite graph) [30,33,35,43–45]; see Figure 1 for illustrations of the topological structures for a directed network and a bipartite network. Without confusion, we also call bipartite networks directed networks occasionally in this paper. We assume that the row nodes of the directed network N belong to K perceivable communities (called row communities in this paper)
C(1)r , C (2) r , . . . , C (K) r , (1)
and the column nodes of the directed network N belong to K perceivable communities (called column communities in this paper)
C(1)c , C (2) c , . . . , C (K) c . (2)
Define an nr × K row nodes membership matrix Πr and an nc × K column nodes membership matrix Πc such that Πr(i, :) is a 1× K probability mass function (PMF) for row node i, Πc(j, :) is a 1× K PMF for column node j, and
Πr(i, k) is the weight of row node i on C(k)r , 1 ≤ k ≤ K, (3)
Πc(j, k) is the weight of column node j on C(k)c , 1 ≤ k ≤ K. (4)
Call row node i ‘pure’ if Πr(i, :) is degenerate (i.e., one entry is 1, all other K − 1 entries are 0) and ‘mixed’ otherwise. The same definitions hold for column nodes. Note that mixed nodes considered in this article are not the boundary nodes introduced in [46] since boundary nodes are defined based on non-overlapping networks, while mixed nodes belong to multiple communities. Let A ∈ {0, 1}nr×nc be the bi-adjacency matrix ofN such that for each entry, A(i, j) = 1 if there is a directional edge from row node i to column node j, and A(i, j) = 0 otherwise. So, the i-th row of A records how row node i sends edges, and the j-th column of A records how column node j receives edges. Let P be a K× K matrix such that
P(k, l) ≥ 0 for 1 ≤ k, l ≤ K. (5)
Note that since we consider a directed network in this paper, P may be asymmetric. Without loss of generality, suppose that row nodes have degree heterogeneities, while column nodes do not i.e., row nodes have variation in degree, while column nodes do not. Note that in a directed network, if column nodes have degree heterogeneities while row nodes do not, to detect memberships of both row nodes and column nodes, we set the transpose of the adjacency matrix as input when applying our algorithm DiMSC. Meanwhile, in a directed network, if both row and column nodes have degree heterogeneity, to model such a directed network with mixed memberships, we need nontrivial constraints on the degree heterogeneities between row nodes and column nodes for model identifiability, for detail, see Remark 1. Let θr be an nr × 1 vector whose i-th entry is the positive degree heterogeneity of row node i. For all pairs of (i, j) with 1 ≤ i ≤ nr, 1 ≤ j ≤ nc, DiDCMM models the entries of A such that A(i, j) are independent Bernoulli random variables satisfying
P(A(i, j) = 1) = θr(i) K
∑ k=1
K
∑ l=1 Πr(i, k)Πc(j, l)P(k, l). (6)
Equation (6) means that P(A(i, j) = 1) = θr(i)Πr(i, :)PΠ′c(j, :), i.e., the probability of generating a directional edge from row node i to column node j is θr(i)Πr(i, :)PΠ′c(j, :), and this probability is controlled by the degree heterogeneity parameter θr(i) of row node i, the connecting matrix P, and the memberships of nodes i and j. Equation (6) functions similarly to Equation (1.4) in [24], and both equations define the probability of generating an edge. For comparison, Equation (6) defines the probability of generating a directional edge under DiDCMM for a directed network, while Equation (1.4) in [24] defines the probability of generating an edge under DCMM for an undirected network, i.e., DiDCMM can be seen as an extension of DCMM from an undirected network to a directed network. Introduce the degree heterogeneity diagonal matrix Θr ∈ Rnr×nr for row nodes such that
Θr(i, i) = θr(i) for 1 ≤ i ≤ nr. (7)
Equation (7) uses a diagonal matrix Θr to contain all degree heterogeneities, and Θr is useful for further theoretical analysis through Equation (8). Definition 1. Call model (1)–(6) the directed degree corrected mixed membership (DiDCMM) model, and denote it by DiDCMMnr ,nc(K, P, Πr, Πc, Θr). The following conditions are sufficient for the identifiability of DiDCMM:
• (I1) rank(P) = K, and P has unit diagonals. • (I2) There is at least one pure node for each of the K row and K column communities. When building statistical models for a network in which nodes can belong to multiple communities, the full rank requirement of connecting matrix P and pure nodes condition are always necessary for model identifiability, see models for an undirected network such as MMSB considered in [23,27], DCMM considered in [24,26], and OCCAM considered in [26,29]. Meanwhile, if models modeling networks with mixed memberships consider degree heterogeneity, the unit diagonals requirement on connecting matrix P is also necessary for model identifiability, see the identifiability requirement of DCMM and OCCAM considered in [24,26,29]. Furthermore, based on the fact that DiDCMM, DCMM, and OCCAM can include the well-known model SBM, letting P have unit diagonals is not a serious problem since many wonderful works study a special case of SBM when P has unit diagonals and a network has K equal size clusters (this special case of SBM is also known as a planted partition model), see [12,47–52]. Let Ω = E[A] be the expectation of the adjacency matrix A. Under DiDCMM, we have
Ω = ΘrΠrPΠ′c. (8)
We refer to Ω as the population adjacency matrix. Since rank(Θr) = K, rank(Πr) = K, rank(Πc) = K and rank(P) = K by Equation (7) and Conditions (I1) and (I2), the rank of Ω is K. Recall that K is the number of communities, and it is much smaller than network size. We see that Ω has a low dimensional structure. The form of Ω given in Equation (8) is powerful to build the spectral algorithm developed in this paper to fit DiDCMM. Analyzing properties of the population adjacency matrix to build a spectral algorithm fitting statistical model is a common strategy in community detection, for example, references [24,26,27,35] also use this strategy to design their algorithms fitting DCMM, MMSB, and DiDCMM. For 1 ≤ k ≤ K, let I (k)r = {i ∈ {1, 2, . . . , nr} : Πr(i, k) = 1} and I (k) c = {j ∈ {1, 2, . . . , nc} : Πc(j, k) = 1}. By Condition (I2), I(k)r and I (k) c are nonempty for all 1 ≤ k ≤ K. For 1 ≤ k ≤ K, select one row node from I (k)r to construct the index set Ir, i.e., Ir is the indices of row nodes corresponding to K pure row nodes, one from each community, and Ic is defined similarly. W.L.O.G., let Πr(Ir, :) = IK and Πc(Ic, :) = IK (Lemma 2.1 [27] has a similar setting to design their spectral algorithm under MMSB. ), where IK is the K× K identity matrix. The proposition below shows that the DiDCMM model is identifiable. Proposition 1. (Identifiability). When Conditions (I1) and (I2) hold, DiDCMM is identifiable: for eligible (P, Πr, Πc, Θr) and (P̃, Π̃r, Π̃c, Θ̃r), set Ω = ΘrΠrPΠ′c and Ω̃ = Θ̃rΠ̃r P̃Π̃′c. If Ω = Ω̃, then Θr = Θ̃r, Πr = Π̃r, Πc = Π̃c and P = P̃. Remark 1. (The reason that we do not model a directed network with mixed memberships where both row and column nodes have degree heterogeneities). Suppose both row and column nodes have degree heterogeneities in a mixed membership directed network. To model such a directed network, the probability of generating an edge from row node i to column node j is
P(A(i, j) = 1) = θr(i)θc(j) K
∑ k=1
K
∑ l=1 Πr(i, k)Πc(j, l)P(k, l),
where θc is an nr × 1 vector whose j-th entry is the degree heterogeneity of column node j. Set Ω = E[A], then Ω = ΘrΠrPΠ′cΘc, where Θc ∈ Rnc×nc is a diagonal matrix whose j-th diagonal entry θc(j). Set Ω = UΛV′ as the compact SVD of Ω. Follow similar analysis as Lemma 1, we see that U = ΘrΠrBr and V = ΘcΠcBc (without causing confusion, we still use Bc here for convenience.). For model identifiability, follow similar analysis as the proof of Proposition 1, since Ω(Ir, Ic) = Θr(Ir, Ir)Πr(Ir, ; )PΠ′c(Ic, :)Θc(Ic, Ic) = Θr(Ir, Ir)PΘc(Ic, Ic) = U(Ir, :
)ΛV′(Ic, :), we see that Θr(Ir, Ir)PΘc(Ic, Ic) = U(Ir, :)ΛV′(Ic, :). To obtain Θr(Ir, Ir) and Θc(Ic, Ic) from U(Ir, :)ΛV′(Ic, :), when P has unit diagonals, we see that it is impossible to recover Θr(Ir, Ir) and Θc(Ic, Ic) unless we add a condition that Θr(Ir, Ir) = Θc(Ic, Ic). Now, suppose Θr(Ir, Ir) = Θc(Ic, Ic) holds and call it Condition (I3); we have Θr(Ir, Ir)PΘr(Ir, Ir) = U(Ir, :)ΛV′(Ic, :); hence, Θr(Ir, Ir) = Θc(Ic, Ic) = √ diag(U(Ir, :)ΛV′(Ic, :)) when P has unit diagonals. However, Condition (I3) is nontrivial since it requires Θr(Ir, Ir) = Θc(Ic, Ic), and we always prefer a directed network in which there are no connections between row nodes degree heterogeneities and column nodes degree heterogeneities. For example, when all nodes are pure in a directed network, ref. [30] models such directed network using model DC-ScBM such that Ω = ΘrΠrPΠ′cΘc when all nodes are pure, and Θr and Θc are independent under DC-ScBM. Because Condition (I3) is nontrivial, we do not model a mixed membership directed network with all nodes having degree heterogeneities. For DiDCMM’s identifiability, the number of row communities should equal that of column communities when both row and column nodes may belong to more than one community. However, when only row nodes have mixed memberships while column nodes do not, the number of row communities can be lesser than that of column communities, and this is also discussed in [53]. All proofs of our theoretical results are provided in the Appendix A.1. Unless specified, we treat Conditions (I1) and (I2) as default from now on. Proposition 1 is important since it guarantees that our model DiDCMM is well-defined, and we can design efficient spectral algorithms to fit DiDCMM based on its identifiability. The reason that we do not consider degree heterogeneity for column nodes for our DiDCMM is mainly for its identifiability. As analyzed in Remark 1, considering degree heterogeneity for both row and column nodes make the model unidentifiable unless adding some nontrivial conditions on model parameters. Meanwhile, many previous statistical models in the community detection areas are identifiable, and spectral algorithms can be applied to fit them. For examples, SBM [3], DCSBM [4], MMSB [17], DCMM [24], OCCAM [29], ScBM (and DCScBM), [30], and DiMMSB [35] are identifiable. Especially, though different statistical models may have different requirements on model parameters for identifiability, the proof of identifiability enjoys a similar idea as that of Proposition 1, for instance, Proposition 1.1 [24] and Theorem 2.1 [27] build theoretical guarantees on identifiability for DCMM and MMSB, respectively. Remark 2. We compare our DiDCMM with some previous models in this remark. • When Θr = ρI for ρ > 0, Equation (8) gives Ω = ρΠrPΠ′c and DiDCMM degenerates to DiMMSB [35], where ρ is known as a sparsity parameter [9,27,35]. So, DiDCMM includes DiMMSB as a special case, and the relationship between DiDCMM and DiMMSB is similar to that between DCSBM [3,4]. Meanwhile, DiDCMM considers degree heterogeneity parameter Θr at the cost that DiDCMM requires P to have unit diagonals for model identifiability, while there is no such requirement for P on DiMMSB’s identifiability. Note that both DiDCMM and DiMMSB are identifiable only when P is a full-rank square matrix. • When Θr = ρI for ρ > 0 and all nodes are pure, DiDCMM reduces to ScBM [30]. DiDCMM can model a directed network in which nodes enjoy overlapping memberships, while ScBM cannot. Meanwhile, DiDCMM enjoys this advantage at the cost of requiring rank(P) = K for model identifiability, while ScBM is identifiable even when P is not a square matrix, i.e., ScBM can model a directed network in which the number of row communities can be different from the number of column communities. A comparison between DiDCMM and DCScBM [30] is similar. • When Θr = ρI and the network is undirected, DiDCMM reduces to MMSB [17]. However, DiDCMM models directed networks with mixed memberships, while MMSB only models undirected networks with mixed memberships. Again, DiDCMM enjoys its advantage at the cost of P having unit diagonals for its identifiability (not that DiDCMM allows P to be asymmetric since DiDCMM models directed networks), while MMSB is identifiable even when
P has non-unit diagonals (note that P is symmetric under MMSB since it models undirected networks). Meanwhile, the identifiability of both DiDCMM and MMSB requires the square matrix P to have full rank. • When Θr = ρI, the network is undirected and all nodes are pure, DiDCMM reduces to SBM [3]. For comparison, DiDCMM models directed networks and allows nodes to belong to multiple communities, while SBM only models undirected networks in which a node only belongs to one community. Meanwhile, DiDCMM enjoys these advantages at the cost of requiring P to be full rank with unit diagonals for its identifiability, while SBM is identifiable even when P is not full rank and P has non-unit diagonals. Note that DiDCMM allows P to be asymmetric, while P must be symmetric for SBM since DiDCMM models directed networks, while SBM models undirected networks. Comparison between DiDCMM and DCSBM [4] is similar. • Compared with DCMM introduced in [24] and OCCAM introduced in [29], DCMM, and OCCAM model undirected networks with mixed memberships, while DiDCMM models directed networks with mixed memberships. DiDCMM, DCMM, and OCCAM all consider degree heterogeneity for overlapping networks, and they are identifiable only when the full rank matrix P has unit diagonals. These three models are identifiable only when the square matrix P is full rank. Meanwhile, DiDCMM allows P to be asymmetric, while P must be symmetric for DCMM and OCCAM since DiDCMM models directed networks, while DCMM and OCCAM model undirected networks. 3. algorithm The primary goal of the proposed algorithm is to estimate the row membership matrix Πr and column membership matrix Πc from the observed adjacency matrix A with given K. We start by considering the ideal case when Ω is known, and then we extend what we learn in the ideal case to the real case. 3.1. The Ideal Simplex (IS), the Ideal Cone (IC), and the Ideal DiMSC
Recall that rank(Ω) = K under Conditions (I1) and (I2), and K is much smaller than min{nr, nc}. Let Ω = UΛV′ be the compact singular value decomposition of Ω such that U ∈ Rnr×K, Λ ∈ RK×K, V ∈ Rnc×K, U′U = IK, V′V = IK. The goal of the ideal case is to use U, Λ, and V to exactly recover Πr and Πc. As stated in [8,24], θr is one of the major nuisances, and similar to [7], we remove the effect of θr by normalizing each row of U to have a unit l2 norm. Set U∗ ∈ Rnr×K by U∗(i, :) = U(i,:)‖U(i,:)‖F , and let NU be the nr × nr diagonal matrix such that NU(i, i) = 1‖U(i,:)‖F for 1 ≤ i ≤ nr. Then, U∗ can be rewritten as U∗ = NUU. The existences of the ideal cone (IC for short) structure inherent in U∗ and the ideal simplex (IS for short) structure inherent in V are guaranteed by the following lemma. Lemma 1. (Ideal Simplex and Ideal Cone). Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), there exist a unique K× K matrix Br and a unique K× K matrix Bc such that • U = ΘrΠrBr, where Br = Θ−1r (Ir, Ir)U(Ir, :), and U∗ = YU∗(Ir, :) where
Y = NMΠrΘ−1r (Ir, Ir)N−1U (Ir, Ir) with NM being an nr × nr diagonal matrix whose diagonal entries are positive. Meanwhile, U∗(i, :) = U∗(ī, :) if Πr(i, :) = Πr(ī, :) for 1 ≤ i, ī ≤ nr. • V = ΠcBc, where Bc = V(Ic, :). Meanwhile, V(j, :) = V( j̄, :) if Πc(j, :) = Πc( j̄, :) for 1 ≤ j, j̄ ≤ nc. Lemma 1 says that the rows of V form a K-simplex in RK which we call the ideal simplex (IS), with the K rows of Bc being the vertices. Such IS is also found in [24,27,35]. Lemma 1 also shows that the form of U∗ = YU∗(Ir, :) is actually the ideal cone structure mentioned in [26]. Meanwhile, we remove the influence of θr by normalizing each row of U to have a unit norm in this paper. Using the idea of the entry-wise ratio in [8] also works, where ref. [24] develops their spectral algorithms to fit DCMM using the idea of
entry-wise ratio. Designing algorithms based on the nonnegative matrix factorization [25] to fit DiDCMM by adding some constraints on Ω may also work. We leave the study of using these ideas to fit DiDCMM or its submodels for our future work. For column nodes (recall that column nodes have no degree heterogeneities), since Bc is full rank if V and Bc are known in advance, ideally we can exactly recover Πc by setting Πc = VB′c(BcB′c)−1 ≡ VB−1c . For convenience, to transfer the ideal case to the real case, set Zc = VB−1c . Since Zc ≡ Πc, we have
Πc(j, :) = Zc(j, :) ‖Zc(j, :)‖1 , 1 ≤ j ≤ nc. With given V, since it enjoys IS structure V = ΠcBc ≡ ΠcV(Ic, :), as long as we can obtain V(Ic, :) (i.e., Bc), we can recover Πc exactly. As mentioned in [24,27], for such IS, the successive projection (SP) algorithm [54] (i.e., Algorithm A2 in the Appendix E) can be applied to V with K column communities to find the column corner matrix Bc. The above analysis gives how to recover Πc with given Ω and K under DiDCMM ideally. Next, we aim to recover Πr from U with the given K. Since rank(U∗) = K, rank(U∗(Ir, : )) = K. As U∗(Ir, :) ∈ RK×K, the inverse of U∗(Ir, :) exists. Therefore, Lemma 1 also gives that
Y = U∗U−1∗ (Ir, :). (9)
Equation (9) holds because U∗ = YU∗(Ir, :) and U∗(Ir, :) is a nonsingular matrix. By Lemma 1, we know that for row nodes, their membership matrix Πr appears in the expression of Y. Therefore, we aim to use Equation (9) to find the exact expression of Πr using U, V, and Λ by putting Y at the left-hand side of equality. For our next step, we aim at finding Πr using Equation (9). Since Y = NMΠrΘ−1r (Ir, Ir)N−1U (Ir, Ir) by Lemma 1 and U∗ = NUU, using NMΠrΘ−1r (Ir, Ir)N−1U (Ir, Ir) and NUU to replace Y and U∗ in Equation (9), respectively, we have N−1U NMΠrΘ −1 r (Ir, Ir)N−1U (Ir, Ir) = UU−1∗ (Ir, :), which gives
N−1U NMΠr = UU −1 ∗ (Ir, :)NU(Ir, Ir)Θr(Ir, Ir). (10)
From Equation (10), we have found the expression of Πr as a function of U, U∗, Θr, NU , and Ir, where we do not move N−1U NM to the right-hand side of Equation (10) because it is a diagonal matrix and does not influence the expression of Πr, see our next step for details. When designing a spectral algorithm in the ideal case with given Ω and K, we aim at recovering Πr and Πc by taking advantage of the singular value decomposition of Ω. We find that though Equation (10) provides an expression for Πr by Ω’s SVD, there is a term Θr(Ir, Ir) which relates to degree heterogeneity, and we aim at expressing Θr(Ir, Ir) through Ω’s SVD. By the proof of Lemma 1, we know that Θr(Ir, Ir) = diag(U(Ir, : )ΛV′(Ic, :)) when Condition (I1) holds. Thus, substituting diag(U(Ir, :)ΛV′(Ic, :)) for Θr(Ir, Ir) in Equation (10), we obtain an expression of Πr such that this expression is directly related to Ω’s SVD and two index set Ir and Ic. For convenience, set J∗ = NU(Ir, Ir)Θr(Ir, Ir) ≡ diag(U∗(Ir, :)ΛV′(Ic, :)), Zr = N−1U NMΠr, Y∗ = UU−1∗ (Ir, :). By Equation (10), we have
Zr = Y∗ J∗ ≡ UU−1∗ (Ir, :)diag(U∗(Ir, :)ΛV′(Ic, :)). (11)
Equation (11) looks similar to Equation (7) of [55]. However, Equation (11) is related to two index sets Ir and Ic, while Equation (7) of [55] is only related to one index set because Equation (11) aims at designing a spectral algorithm for directed network generated under DiDCMM and Equation (7) of [55] aims at reviewing the generation of the SVM-cone-
DCMMSB algorithm proposed in [26] for undirected network generated under DCMM. Meanwhile, since N−1U NM is an nr × nr positive diagonal matrix, we have
Πr(i, :) = Zr(i, :) ‖Zr(i, :)‖1 , 1 ≤ i ≤ nr. (12)
With given Ω and K, we can obtain U, V; thus, the above analysis shows that once the two index sets Ir and Ic are known, we can exactly recover Πr by Equations (11) and (12). Meanwhile, from Equation (10), we see that it is important to express Θr(Ir, Ir) as a combination of U, V, Λ, and the two index sets Ir and Ic, where we successfully obtain an expression of Θr(Ir, Ir) by Condition (I1), the unit diagonal constraint on P. Otherwise, if P has no unit diagonals, we cannot obtain an expression of Θr(Ir, Ir) unless adding some nontrivial conditions on model parameters, just as analyzed in Remark 1. Similarly, references [24,26] also design their spectral algorithms to fit DCMM by using the unit diagonal constraint on P to obtain an expression of a sub-matrix of degree heterogeneity matrix, see Equations (6)–(8) of [55] as an example. Given Ω and K, to recover Πr in the ideal case, we need to obtain Zr by Equation (11), which means that the only difficulty is in finding the index set Ir since V(Ic, :) can be obtained by SP algorithm from the IS structure V = ΠcV(Ic, :). From Lemma 1, we know that U∗ = YU∗(Ir, :) forms the IC structure. In [26], their SVM-cone algorithm (i.e., Algorithm A3 in the Appendix F) can exactly obtain the row nodes corner matrix U∗(Ir, :) from the ideal cone U∗ = YU∗(Ir, :) as long as the Condition (U∗(Ir, :)U′∗(Ir, :))−11 > 0 holds (see Lemma 2). Lemma 2. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), (U∗(Ir, :)U′∗(Ir, :))−11 > 0 holds. Based on the above analysis, we are now ready to give the following four-stage algorithm which we call ideal DiMSC. Input Ω, K. Output: Πr and Πc. • Let Ω = UΛV′ be the compact SVD of Ω such that U ∈ Rnr×K, V ∈ Rnc×K, Λ ∈ RK×K, U′U = I, V′V = I. Let U∗ = NUU, where NU is an nr × nr diagonal matrix whose i-th diagonal entry is 1‖U(i,:)‖F for 1 ≤ i ≤ nr. • Run the SP algorithm on V assuming that there are K column communities to obtain the column corner matrix V(Ic, :) (i.e.,Bc). Run the SVM-cone algorithm on U∗ assuming that there are K row communities to obtain Ir. • Set J∗ = diag(U∗(Ir, :)ΛV′(Ic, :)), Y∗ = UU−1∗ (Ir, :), Zr = Y∗ J∗ and Zc = VV−1(Ic, :). • Recover Πr and Πc by setting Πr(i, :) =
Zr(i,:) ‖Zr(i,:)‖1 for 1 ≤ i ≤ nr, and Πc(j, :) = Zc(j,:)‖Zc(j,:)‖1 for 1 ≤ j ≤ nc. The following theorem guarantees that ideal DiMSC exactly recovers nodes memberships, and this verifies the identifiability of DiDCMM in turn. Meanwhile, it should be noted that many spectral algorithms designed to fit identifiable statistical models in the community detection area can exactly recover node memberships for the ideal case. For example, the spectral clustering for K many clusters algorithm addressed in [5] under SBM, the regularized spectral clustering designed in [7] under DCSBM, the SCORE algorithm designed in [8] under DCSBM, the two algorithms designed and studied in [9] under SBM and DCSBM, the RSC-τ algorithm studied in [11] under SBM, the mixed-SCORE algorithm designed in [24] under DCMM, the DI-SIM algorithm designed in [30] under DCScBM, the D-SCORE algorithm studied in [31,32] under DCScBM, the SVM-cone-DCMMSB algorithm designed in [26] under DCMM, and the SPACL algorithm designed in [27] under MMSB can exactly recover membership matrices under respective models for the ideal case by using the population adjacency matrix to replace the adjacency matrix in the input of these algorithms. The fact that ideal cases for the above spectral algorithms can return community information also supports the identifiability of the above models. Theorem 1. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), the ideal DiMSC exactly recovers the row nodes membership matrix Πr and the column nodes membership matrix Πc. To demonstrate that U∗ has the ideal cone structure, we drew Panel (a) of Figure 2. The simulated data used for Panel (a) is generated from DiDCMMnr ,nc(K, P, Πr, Πc, Θr) with nr = 600, nc = 400, K = 3; each row (and column) community has 120 pure nodes. For the 240 mixed row nodes, we set Πr(i, 1) = rand(1)/2, Πr(i, 2) = rand(1)/2, Πr(i, 3) = 1−Πr(j, 1)−Πr(j, 2), where rand(1) is any random number in (0, 1), and i is a mixed row node. For the 40 mixed column nodes, set Πc(j, 1) = rand(1)/2, Πc(j, 2) = rand(1)/2, Πc(j, 3) = 1 − Πc(j, 1) − Πc(j, 2). For the degree heterogeneity parameter, set θr(i) = rand(1) for all row nodes i. The matrix P is set as
P =  1 0.4 0.30.2 1 0.1 0.1 0.4 1 . Under such a setting, after computing Ω and obtaining U∗, V from Ω, we can plot Figure 2. Panel (a) shows that all rows respective to mixed row nodes of U∗ are located at one side of the hyperplane formed by the K rows of U∗(Ir, :), and this phenomenon occurs since each row of U∗ is a scaled convex combination of the K rows of U∗(Ir, :) guaranteed by the IC structure U∗ = YU∗(Ir, ; ). Thus Panel (a) shows the existence of the ideal cone structure formed by U∗. Similarly, to demonstrate that V has the ideal simplex structure, we drew Panel (b) of Figure 2, where Panel (b) is obtained under the same setting as Panel (a). Panel (b) shows that rows respective to mixed column nodes of V are located inside of the simplex formed by the K rows of V(Ic, :), and this phenomenon occurs since each row of V is a convex linear combination of the K rows of V(Ic, :) guaranteed by the IS structure V = ΠcV(Ic, ; ). Thus Panel (b) shows the existence of the ideal simplex structure formed by V.
3.2. Dimsc Algorithm
We now extend the ideal case to the real case. Set Ã = ÛΛ̂V̂′ to be the top-Kdimensional SVD of A such that Û ∈ Rnr×K, V̂ ∈ Rnc×K, Λ̂ ∈ RK×K, Û′Û = IK, V̂′V̂ = IK, and Λ̂ contains the top K singular values of A. Let Û∗ be the row-wise normalization of Û such that Û = NÛÛ, where NÛ ∈ Rnr×nr is a diagonal matrix whose i-th diagonal entry is 1‖Û(i,:)‖F . For the real case, we use Ĵ∗, Ŷ∗, Ẑr, Ẑc, Π̂r, Π̂c given in Algorithm 1 to estimate
J∗, Y∗, Zr, Zc, Πr, Πc, respectively. Algorithm 1 called directed mixed simplex and cone (DiMSC for short) algorithm is a natural extension of the ideal DiMSC to the real case. Algorithm 1 Directed Mixed Simplex and Cone (DiMSC) algorithm
Require: The adjacency matrix A ∈ Rnr×nc of a directed network, the number of row (column) communities K. Ensure: The estimated nr×K row membership matrix Π̂r and the estimated nc×K column membership matrix Π̂c. 1: Obtain Ã = ÛΛ̂V̂′, the top-K-dimensional SVD of A. Compute Û∗ from Û. 2: Apply SP algorithm (i.e., Algorithm A2) on the rows of V̂ assuming there are K column
communities to obtain Îc, the index set returned by SP algorithm. 3: Similarly, apply SVM-cone algorithm (i.e., Algorithm A3) on the rows of Û∗ with K row
communities to obtain Îr, the index set returned by SVM-cone algorithm. 4: Set Ĵ∗ = diag(Û∗( Îr, :)Λ̂V̂′(Îc, :)), Ŷ∗ = ÛÛ−1∗ (Îr, :), Ẑr = Ŷ∗ Ĵ∗ and Ẑc = V̂V̂−1(Îc, :). Then, set Ẑr = max(0, Ẑr) and Ẑc = max(0, Ẑc). 5: Estimate Πr(i, :) by Π̂r(i, :) = Ẑr(i, :)/‖Ẑr(i, :)‖1, 1 ≤ i ≤ nr and estimate Πc(j, :) by
Π̂c(j, :) = Ẑc(j, :)/‖Ẑc(j, :)‖1, 1 ≤ j ≤ nc. In the third step, we set the negative entries of Ẑr as 0 by setting Ẑr = max(0, Ẑr) for the reason that weights for any row node should be nonnegative, while there may exist some negative entries of Ŷ∗ Ĵ∗. A similar argument holds for Ẑc. The flowchart of DiMSC is displayed in Figure 3. Meanwhile, in community detection, researchers often use top-Kdimensional SVD of A or its variants such as Laplacian matrix or regularized Laplacian matrix to design their spectral clustering algorithms to fit identifiable statistical models such as spectral methods designed or studied in [5,7–9,11,24,26,27,29,31,33,35,56]. Furthermore, as discussed in [57], the SVS+ and SVS∗ algorithms may be used as substitutions of the SP algorithm in our DiMSC for a better estimation of Πr. When applying the entry-wise normalization idea developed in [8] to deal with U, as analyzed in [24], we obtain a simplex structure, and we can use the SP algorithm (or the combinatorial vertex search and sketched vertex search approaches developed in [24]) to hunt for the corners. The above ideas suggest that we can design different spectral algorithms to fit our model DiDCMM. In particular, in this paper, we apply the SVM-cone algorithm to hunt for the corners of the cone structure inherent in U∗ mainly for the theoretical convenience of the SVM-cone algorithm because ref. [26] has developed a nice theoretical framework on the performance for the SVM-cone algorithm. 3.3. Computational Complexity
The computing cost of DiMSC mainly comes from SVD, SP, and SVM-cone. The computational complexity of SVD is O(max(nr, nc)min(n2r , n2c )). Since the adjacency matrix A for real-world network data sets is usually sparse, using the power method discussed in [58], the computation complexity for obtaining the top-K-dimensional SVD of A is only slightly larger than O(max(n2r , n2c )K) [8,24]. The SP algorithm step in DiMSC has a complexity of O(max(nr, nc)K2) [24]. The complexity of the one-class SVM step for SVM-cone algorithm is O(max(nr, nc)K2) [26,59]. The complexity of the K-means step for SVM-cone algorithm is O(max(nr, nc)K2) [60]. Since the number of communities K considered in this paper is much smaller than the network size, the total complexity of DiMSC is O(max(n2r , n2c )K). Results in Section 5 show that, for a computer-generated network with 15,000 nodes un-
der SBM, DiMSC takes hundreds of seconds to process a standard personal computer (Thinkpad X1 Carbon Gen 8) using MATLAB R2021b. Meanwhile, many spectral methods developed under models SBM, DCSBM, MMSB, ScBM, DCScBM, OCCAM, DCMM, and DiMMSB for community detection also have complexity O(max(n2r , n2c )K), see spectral algorithms designed or studied in [5,7–9,11,24,26,27,29–31,33,35,61,62]. Researchers design spectral algorithms for community detection under various identifiable statistical models mainly for their convenience on building a theoretical guarantee of consistent estimation, and we also provide a theoretical guarantee on DiMSC’s estimation consistency in next section. 4. consistency results In this section, we show the consistency of our algorithm for fitting the DiDCMM by proving that the sample-based estimates Π̂r and Π̂c concentrate around the true mixed membership matrices Πr and Πc. Throughout this paper, K is a known positive integer. Set θr,max = max1≤i≤nr θr(i) and θr,min = min1≤i≤nr θr(i). Assume that
Assumption 1. Pmaxmax(‖θr‖1, θr,maxnc) ≥ log(nr + nc). Assumption 1 means that the network cannot be too sparse, and it also means that we allow θr,max to go to zero with increasing numbers of row nodes and column nodes. When building theoretical guarantees on consistent estimation, controlling network sparsity is popular in the community detection area. For examples, Condition (2.9) of [8], Theorem 3.1 of [9], Condition (2.13) of [24], Assumption 3.1 of [27], and Assumption 2 of [31] all control network sparsity for their theoretical analysis. Especially, when DiDCMM reduces to SBM by letting Θr = ρI, n = nr = nc, Πr = Πc, and all nodes are pure for ρ > 0, Assumption 1 requires that ρn log(n), which is consistent with the sparsity requirement in [8,9,24,31]. As analyzed in [55], we know that our requirement on network sparsity is optimal since it matches the sharp threshold of obtaining a connected Erdös–Rényi (ER) random graph [63] when SBM reduces to an ER random graph by letting K = 1. For notation convenience, set v = max(‖ÛÛ′ − UU′‖2→∞, ‖V̂V̂′ − VV′‖2→∞), f̂r = max1≤i≤nr‖e′i(Π̂r −ΠrPr)‖1, f̂c = max1≤j≤nc‖e′j(Π̂c −ΠcPc)‖1, and πr,min = min1≤k≤K 1′Πrek, where v is the row-wise singular vector deviation which can be bounded by Theorem 4.4 of [64], f̂r and f̂c measures per node clustering error of DiMSC, and πr,min measures the minimum summation of row nodes belonging to a certain row community. Increasing πr,min makes the network tend to be more balanced and vice versa. Meanwhile, row-wise singular vector deviation is important when building a theoretical guarantee of spectral methods fitting models for a network with mixed memberships, for example, refs. [24,26,27,35] also consider v when building consistent estimation for their spectral methods. The next theorem gives theoretical bounds on estimations of memberships for both row and column nodes, which is the main theoretical result for our DiMSC method. Theorem 2. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), let Π̂r and Π̂c be obtained from Algorithm 1, when Assumption 1 holds, suppose σK(Ω) ≥ C √ θr,maxPmax(nr + nc)log(nr + nc), with probability at least 1− o((nr + nc)−3), we have
f̂r = O( K5.5θ15r,maxvκ4.5(Π′rΠr)κ(Πc)λ1.51 (Π ′ rΠr)
θ15r,minπr,min ), f̂c = O(vKκ(Π′cΠc)
√ λ1(Π′cΠc)). In Theorem 2, the Condition σK(Ω) ≥ C √
θr,maxPmax(nr + nc)log(nr + nc) is necessary when applying Theorem 4.4 [64] to obtain a theoretical upper bound of v. When building a theoretical guarantee on estimation consistency for spectral methods fitting models modeling network with mixed memberships, it is necessary to have a lower bound requirement on σK(Ω), see [24,26,27,35]. Actually, this requirement matches with the con-
sistent requirement on σK(P)√Pmax obtained from the theoretical upper bound of error rates for a balanced network, see Remark 4 for details. Meanwhile, similar to [7,11,30], we can design a spectral algorithm via an application of regularized Laplacian matrix to fit DiDCMM. The following corollary is obtained by adding conditions on model parameters similar to Corollary 3.1 in [27], where these conditions give a directed network in which each community has the same order of size, and each node has the same order of degree, i.e., a balanced network. Corollary 1. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), when conditions of Theorem 2 hold, suppose λK(Π′rΠr) = O( nr K ), λK(Π ′ cΠc) = O( nc K ), πr,min = O( nr K ) and K = O(1), with probability at least 1− o((nr + nc)−3), we have
f̂r = O(( θr,max θr,min )15.5 1 σK(P)
√ Pmaxlog(nr + nc)
θr,minnc ), f̂c = O(( θr,max θr,min )0.5 1 σK(P)
√ Pmaxlog(nr + nc)
θr,minnr ). Meanwhile, • when θr,max = O(ρ), θr,min = O(ρ) (i.e., θr,min θr,max = O(1)), we have
f̂r = O( 1
σK(P)
√ Pmaxlog(nr + nc)
ρnc ), f̂c = O( 1 σK(P)
√ Pmaxlog(nr + nc)
ρnr ). • when nr = O(n), nc = O(n) and θr,max = O(ρ), θr,min = O(ρ), we have
f̂r = O( 1
σK(P)
√ Pmaxlog(n)
ρn ), f̂c = O( 1 σK(P)
√ Pmaxlog(n)
ρn ). Consider a directed mixed membership network under the settings of Corollary 1 when θr,max = O(ρ), θr,min = O(ρ) for ρ > 0, to obtain consistent estimations for both row
nodes and column nodes, by Corollary 1, σK(P)√Pmax should shrink slower than √ log(nr+nc) ρmin(nr ,nc) , where consistent estimation means that the theoretical upper bound of error rate goes to zero when increasing network size. Especially, when nr = O(n) and nc = O(n),
σK(P)√ Pmax should shrink slower than √
log(n) n . We further assume that P = (2− β)IK + (β− 1)11 ′
for β ∈ [1, 2) ∪ (2, ∞) and let P̃ = ρP (note that for this P, we have σK(P) = |β− 2| and Pmax = max(1, β− 1)). So the diagonal elements for P̃ are ρ and non-diagonal elements are ρ(β− 1). Set pin as the diagonal entries of P̃, and pout as the non-diagonal entries of P̃, we have pin = ρ , pout = ρ(β− 1), and |pin−pout|√max(pin,pout) = √ ρ|β−2|√ max(1,β−1) = √ ρσK(P)√ Pmax . Hence, for
consistent estimation, we see that |pin−pout|√ max(pin,pout)
should shrink slower than √
log(nr+nc) min(nr ,nc) by Corollary 1 and should shrink slower than √
log(n) n when nr = O(n) and nc = O(n), where
this result is consistent with classical separation condition for a standard network with two equal-sized clusters by applying the separation condition and sharp threshold criterion developed in [55]. Remark 3. When the network is undirected (i.e., nr = nc = n, Πr = Πc) with K = O(1) by setting θr(i) = ρ for 1 ≤ i ≤ nr, DiDCMM degenerates to MMSB considered in [27], the upper bound of error rate for DiMSC is O( 1
σK(P)
√ log(n)
ρn ) when Pmax = 1. Replacing the Θ in [24] by Θ = √ ρI, their DCMM model degenerates to MMSB. Then, their conditions in Theorem 2.2 are our Assumption 1 and λK(Π′Π) = O( nK ), where Π = Πr = Πc for MMSB. When K = O(1), the error bound in Theorem 2.2 in [24] is O( 1 σK(P) √ log(n) ρn ), which is consistent with ours. Remark 4. By Lemma A5 in the Appendix D, we know σK(Ω) ≥ θr,minσK(P)σK(Πr)σK(Πc). To ensure the Condition σK(Ω) ≥ C(θr,maxPmax(nr + nc)log(nr + nc))1/2 in Theorem 2 holds, we need
σK(P)√ Pmax
≥ C ( θr,max(nr + nc)log(nr + nc)
θ2r,minλK(Π ′ rΠr)λK(Π′cΠc) )1/2. (13) When K = O(1), nr = O(n), nc = O(n), λK(Π′rΠr) = O( nr K ), λK(Π ′ cΠc) = O( nc K ) and θr,max = O(ρ), θr,min = O(ρ), Equation (13) gives that σK(P)√
Pmax should shrink slower than
√ log(n)
ρn ,
which matches with the consistency requirement on σK(P)√Pmax of Corollary 1. For convenience, we need the following definition. Definition 2. Let DiDCMM(n, K, Πr, Πc, αin, αout) be a special case of DiMMDFnr ,nc(K, P, Πr, Πc, Θr) when Θr = ρI, nr = nc = n, λK(Π′rΠr) = O(n/K), λK(Π′cΠc) = O(n/K), πr,min = O(n/K), K = O(1), and P̃ = ρP has diagonal entries pin = αin log(n) n and non-diagonal entries pout = αout log(n)
n . DiDCMM(n, K, Πr, Πc, αin, αout) denotes a special directed network such that row communities have nearly equal sizes since λK(Π′rΠr) = O(n/K), and column communities also have nearly equal sizes. By Corollary 1, for consistent estimation, we need |pin−pout|√ max(pin,pout) √ log(n) n under DiDCMM(n, K, Πr, Πc, αin, αout). Since |pin−pout|√ max(pin,pout) =
|αin−αout| √
log(n) n√
max(αin,αout) , for consistent estimation, we need
|αin − αout|√ max(αin, αout)
1 (14)
Our numerical results in Section 5 support that DiMSC can estimate memberships for both row and column nodes when the threshold |αin−αout|√
max(αin,αout) 1 holds under DiDCMM(n, K,
Πr, Πc, αin, αout). Remark 5. When K = 2, the network is undirected (i.e., Πr = Πc), all nodes are pure, and each community has an equal size, DiDCMM(n, K, Πr, Πc, αin, αout) reduces to the SBM case such that nodes connect with probability pin within clusters and pout across clusters. This case has been well studied in recent years, see [50] and references therein. Especially, for this case, ref. [50] finds that exact recovery is possible if |√αin − √ αout| > √ 2 and impossible if |√αin − √ αout| < √ 2. For convenience, we use SBM(n, pin, pout) to denote this case. Our numerical results in Section 5 show that DiMSC return consistent estimation under SBM(n, pin, pout) when αin and αout are set in the impossible region of exact recovery but satisfy Equation (14). Remark 6. In information theory, Shannon entropy [65] quantifies the amount of information in a variable, and it is a measure of uncertainty information of a probability distribution. We use a node membership entropy (NME) derived from Shannon theory to measure the node’s uncertainty about the node and all communities [66,67]. For row node i with membership Πr(i, :), since ∑Kk=1 Πr(i, k) = 1 and Πr(i, k) can be seen as the probability that row node i belongs to row cluster k for 1 ≤ k ≤ K, NME of row node i is the Shannon entropy related to Πr(i, :):
NME(i) = − K
∑ k=1 Πr(i, k)log(Πr(i, k)). (15)
For column node j with membership Πc(j, :), we can also obtain its NME by Equation (15). In particular, if a node belongs to each cluster with equal probability 1K , its NME is log(K) which is
the maximum among all NME; if a node belongs to two clusters with equal probability 12 , its NME is log(2) which is less than log(K) when K ≥ 3. Generally, we see that recovering memberships for mixed nodes is harder than for pure nodes since NME is 0 for pure nodes, while NME is larger than 0 for mixed nodes by the definition of NME. 5. simulations In this section, several experiments are conducted to investigate the performance of our DiMSC under DiDCMM. We compare our DiMSC with three model-based methods that can be thought of as special cases of our model DiDCMM. Model-based methods we compare include the DISIM algorithm proposed in [30], the DSCORE algorithm studied in [31], and the DiPCA algorithm which is obtained by using the adjacency matrix A to replace the regularized graph Laplacian matrix in the DISIM algorithm. Similar to [24,27], for simulations, we measure the errors for the inferred community membership matrices instead of simply each node. We measure the performance of DiMSC and its competitors by the mixed Hamming error rate (MHamm for short) defined below
MHamm = max( minP∈SP ‖Π̂rP −Πr‖1
nr , minP∈SP ‖Π̂cP −Πc‖1 nc ), (16)
where SP is the set of K× K permutation matrices. For all simulations in this section, unless specified, we set the parameters (nr, nc, K, P, Πr, Πc, Θr) under DiDCMM as follows: let each row community and each column community have n0 pure nodes; let all mixed row nodes (and mixed column nodes) have membership (1/K, 1/K, . . . , 1/K); for z ≥ 1, we generate the degree parameters for row nodes as below: let θ̄r ∈ Rnr×1 such that 1/θ̄r(i)
iid∼ U(1, z) for 1 ≤ i ≤ nr, where U(1, z) denotes the uniform distribution on [1, z], and set θr = ρθ̄r, where we use ρ to control the sparsity of the network; when K = 2, P is set as
P1 = [
1 0.1 0.2 1
] or P2 = [ 0.8 0.1 0.2 0.9 ] ;
when K = 3,
P3 =  1 0.1 0.30.2 1 0.4 0.5 0.2 1 or P4 = 0.8 0.1 0.30.2 0.9 0.4 0.5 0.2 1 ; where P2 and P4 have non-unit diagonals, and we consider the two cases because we want to investigate DiMSC’s sensitivity when P has non-unit diagonals such that P disobeys Condition (I1). After obtaining P, Πr, Πc, θr, similar to the five simulation steps in [8], each simulation experiment contains the following steps: (a) Let Θr be the nr × nr diagonal matrix such that Θr(i, i) = θr(i), 1 ≤ i ≤ nr. Set Ω = ΘrΠrPΠ′c. (b) Let W be an nr × nc matrix such that W(i, j) are independent centered-Bernoulli with parameters Ω(i, j). Let Ã = Ω + W.
(c) Set S̃r = {i : ∑ncj=1 Ã(i, j) = 0} and S̃c = {j : ∑ nr i=1 Ã(i, j) = 0}, i.e., S̃r (S̃c) is the set of row (column) nodes with 0 edges. Let A be the adjacency matrix obtained by removing rows respective to nodes in S̃r and removing columns respective to nodes in S̃c from Ã. Similarly, update Πr by removing nodes in S̃r and update Πc by removing nodes in S̃c. (d) Apply the DiMSC algorithm (and its competitors) to A. Record MHamm under investigations. (e) Repeat (b)–(d) 50 times, and report the averaged MHamm over the 50 repetitions. Let nr,A be the number of rows of A and nc,A be the number of columns of A. In our experiments, nr,A and nc,A are usually very close to nr and nc; therefore we do not
report the exact values of nr,A and nc,A. After providing the above steps about how to generate A numerically under DiDCMM and how to record the error rates, now we describe our experiments in detail. We consider six experiments here. In experiments 1–6, we study the influence of the fraction of pure nodes, degree heterogeneity, connectivity across communities, sparsity, phase transition, and network size on performances of these methods, respectively. Experiment 1 (a): Fraction of pure nodes. Set nr = 200, nc = 300, z = 5, ρ = 1 and P as P1. Let n0 range in {10, 20, 30, . . . , 100}. The numerical results are shown in Panel (a) of Figure 4. The results show that as the fraction of pure nodes increases for both row and column communities, all approaches perform better. Meanwhile, DiMSC performs best among all methods in Experiment 1 (a). Experiment 1 (b): Fraction of pure nodes. All parameters are set the same as Experiment 1 (a) except that we set P as P2 here. The numerical results are shown in Panel (b) of Figure 4. The results show that all methods perform better as n0 increases, DiMSC outperforms its competitors, and DiMSC enjoys satisfactory performance even when P has non-unit diagonals. Experiment 1 (c): Fraction of pure nodes. Set nr = 600, nc = 900, z = 5, ρ = 1, and P as P3. Let n0 range in {20, 40, 60, . . . , 200}. The numerical results are shown in Panel (c) of Figure 4, and we see that all methods perform better when there are more pure nodes and our DiMSC performs best. Experiment 1 (d): Fraction of pure nodes. All parameters are set the same as Experiment 1 (c) except that we set P as P4 here. The numerical results are shown in Panel (d) of Figure 4, and the analysis is similar to that of Experiment 1 (b). Experiment 2 (a): Degree heterogeneity. Set nr = 200, nc = 300, n0 = 80, ρ = 1, and P as P1. Let z range in {2, 3, 4, . . . , 12}. A lager z generates lesser edges. The results are displayed in Panel (a) of Figure 5. The results suggest that the error rates of DiMSC for both row and column nodes tend to increase as z increases. This phenomenon happens because decreasing degree heterogeneities for row nodes lowers the number of edges in the directed network; thus the network becomes harder to be detected for both row and column nodes. Meanwhile, DiMSC outperforms its competitors in this experiment, and it is interesting to see that the error rates of DI-SIM, DiPCA, and DSCORE are almost the same for this experiment. Experiment 2 (b): Degree heterogeneity. All parameters are set the same as Experiment 2 (a) except that we set P as P2 here. The results are displayed in Panel (b) of Figure 5, and we see that DiMSC performs satisfactorily when the directed network is not too sparse (i.e., a small z case) even when P has non-unit diagonals. Meanwhile, DiMSC significantly outperforms its competitors in this experiment. Experiment 2 (c): Degree heterogeneity. Set nr = 600, nc = 900, n0 = 150, ρ = 1, and P as P3. Let z range in {2, 3, 4, . . . , 12}. The results are shown in Panel (c) of Figure 5 and can be analyzed similarly to Experiment 2 (a). Experiment 2 (d): Degree heterogeneity. All parameters are set the same as Experiment 2 (c) except that we set P as P4 here. The results are displayed in Panel (d) of Figure 5 and are similar to that of Experiment 2 (b). Experiment 2 (e): Degree heterogeneity. All parameters are set the same as Experiment 2(a) except that we set n0 = 0 (so there are no pure nodes in both row and column communities), and all mixed row nodes have two different memberships (0.9, 0.1) and (0.1, 0.9), each with nrK = 100 number of row nodes, and all mixed column nodes also have the above two memberships, each with ncK = 150 number of column nodes. Panel (e) of Figure 5 shows the results, and we see that DiMSC performs satisfactorily for a small z even for the case when there are no pure nodes for both row and column communities. Meanwhile, DiMSC performs better than its competitors when z < 7, and it perform poorer than its competitors when z ≥ 8 for this experiment. Furthermore, compared with numerical results of Experiment 2 (a), we see that DI-SIM, DiPCA, and DSCORE have better performances in Experiment 2 (e). The possible reason is the memberships (0.9, 0.1) and (0.1, 0.9) are close to (1, 0) and (0, 1) somewhat. Experiment 2 (f): Degree heterogeneity. All parameters are set the same as Experiment 2 (b) except that we set Πr and Πc the same as Experiment 2 (e). The results are shown in Panel (f) of Figure 5 and are similar to that of Experiment 2 (e). Experiment 2 (g): Degree heterogeneity. All parameters are set the same as Experiment 2 (c) except that we set n0 = 0, all mixed row nodes have three different memberships (0.8, 0.1, 0.1), (0.1, 0.8, 0.1), and (0.1, 0.1, 0.8), each with nrK = 200 number of row nodes, and all mixed column nodes also have the above four memberships, each with nc K = 300 number of column nodes. The results are displayed in Panel (g) of Figure 5 and are similar to that of Experiment 2 (e). Experiment 2 (h): Degree heterogeneity. All parameters are set the same as Experiment 2 (d) except that we set Πr and Πc the same as Experiment 2 (g). The results are shown in Panel (h) of Figure 5 and are similar to that of Experiment 2 (e). Experiment 3 (a): Connectivity across communities. Set nr = 200, nc = 300, n0 = 80, z = 5, ρ = 1. Set
P = [
1 β− 1 β− 1 1
] . and let β range in {1, 1.2, 1.4, . . . , 4}. Decreasing |β− 2| increases the hardness of detecting such directed networks. Note that P(A(i, j) = 1) = Ω(i, j) = θr(i)Πr(i, :)PΠ′c(j, :) gives maxi,jΩ(i, j) = θr,maxPmax should be no larger than 1. Since Pmax may be larger than one in this experiment, after obtaining θr, we need to update θr as θr/Pmax. The results are displayed in Panel (a) of Figure 6, and they support the arguments given after Corollary 1
such that DiMSC performs better when |β− 2| increases and vice versa. Meanwhile, our DiMSC outperforms its competitors in this experiment. Experiment 3 (b): Connectivity across communities. All parameters are set the same as Experiment 3 (a) except that we set
P = [
0.8 β− 1 β− 1 0.9
] . The results are displayed in Panel (b) of Figure 6, and we see that DiMSC performs better when |β− 2| increases even for the case that P has non-unit diagonals.Meanwhile, our DiMSC performs better than its competitors here. Experiment 3 (c): Connectivity across communities. Set nr = 600, nc = 900, n0 = 150, z = 5, ρ = 1. Set
P =  1 β− 1 β− 1β− 1 1 β− 1 β− 1 β− 1 1 . and let β range in {1, 1.2, 1.4, . . . , 4}. The results are displayed in Panel (c) of Figure 6 and can be analyzed similarly to Experiment 3 (a). Experiment 3 (d): Connectivity across communities. All parameters are set the same as Experiment 3(c) except that we set
P =  0.8 β− 1 β− 1β− 1 0.9 β− 1 β− 1 β− 1 1 . The results are displayed in Panel (d) of Figure 6 and can be analyzed similarly to Experiment 3 (b). Experiment 3 (e): Connectivity across communities. All parameters are set the same as Experiment 3(a) except that we let Πr and Πc be the same as that of Experiment 2 (e) (so there are no pure nodes in both row and column communities.). Panel (e) of Figure 6 shows the results, and we see that DiMSC enjoys better performance when |β− 2| increases even in the case that there are no pure nodes for both row and column communities. Meanwhile, all methods have competitive performances for this experiment, and the possible reason that DiMSC’s competitors enjoy better performances here than in Experiment 3 (a) is analyzed in Experiment 2 (e). Experiment 3 (f): Connectivity across communities. All parameters are set the same as Experiment 3 (b) except that we set Πr and Πc the same as Experiment 2 (e). The results are displayed in Panel (f) of Figure 6 and can be analyzed similarly to Experiment 3 (e). Experiment 3 (g): Connectivity across communities. All parameters are set the same as Experiment 3 (c) except that we let Πr and Πc be the same as that of Experiment 2 (g) (so there are no pure nodes). Panel (g) of Figure 6 shows the results, and the analysis is similar to that of Experiment 3 (b). Experiment 3 (h): Connectivity across communities. All parameters are set the same as Experiment 3 (d) except that we set Πr and Πc the same as Experiment 2 (g). Panel (h) of Figure 6 shows the results, and the analysis is similar to that of Experiment 3 (b). Experiment 4 (a): Sparsity. Set nr = 200, nc = 300, n0 = 80, z = 5, and P as P1. Let ρ range in {0.2, 0.3, . . . , 1}. A larger ρ indicates a denser network. Panel (a) in Figure 7 displays the simulation results of this experiment. We see that DiMSC performs better as the simulated directed network becomes denser, and DiMSC significantly outperforms its competitors in this experiment. Experiment 4 (b): Sparsity. All parameters are set the same as Experiment 4 (a) except that P is set as P2. Panel (b) of Figure 7 shows the results, and the analysis is similar to that of Experiment 2 (b). Experiment 4 (c): Sparsity. Set nr = 600, nc = 900, n0 = 150, z = 5, and P as P3. Let ρ range in {0.2, 0.3, . . . , 1}. Panel (c) of Figure 7 shows the results, and the analysis is similar to that of Experiment 4 (a). Experiment 4 (d): Sparsity. All parameters are set the same as Experiment 4 (c) except that P is set as P4. Panel (d) of Figure 7 displays the results, and the analysis is similar to that of Experiment 4 (b). Experiment 4 (e): Sparsity. All parameters are set the same as Experiment 4 (a) except that we let Πr and Πc be the same as that of Experiment 2 (e). Panel (e) of Figure 7 shows the results, and we see that DiMSC’s error rates decrease for a denser directed network even when all nodes are mixed. Meanwhile, all methods enjoy similar performances in this experiment. Experiment 4 (f): Sparsity. All parameters are set the same as Experiment 4 (b) except that we set Πr and Πc the same as Experiment 2(e). Panel (f) of Figure 7 shows the results, and the analysis is similar to that of Experiment 4 (e). Experiment 4 (g): Sparsity. All parameters are set the same as Experiment 4 (c) except that we let Πr and Πc be the same as that of Experiment 2 (g). Panel (g) of Figure 7 shows the results, and the analysis is similar to that of Experiment 4 (e). Experiment 4 (h): Sparsity. All parameters are set the same as Experiment 4 (d) except that we set Πr and Πc the same as Experiment 2(g). Panel (h) of Figure 7 shows the results, and the analysis is similar to that of Experiment 4 (e). Experiment 5 (a): Phase transition. Under DiDCMM(n, K, Πr, Πc, αin, αout), set K = 2, n = nr = nc = 300. Let each row community have 100 pure nodes, each column community have 120 pure nodes, and all mixed nodes have membership (1/2, 1/2). Since max(pin, pout) = max(αin, αout) log(n) n ≤ 1, αin and αout should be set in (0, n log(n) ]. We let αin and αout be in the range of {2.5, 5, 7.5, . . . , 50}. Panel (a) of Figure 8 displays the results. We see that DiMSC performs satisfactorily when αin and αout satisfy Equation (14), and this means that DiMSC achieves the threshold provided in Equation (14) under DiDCMM(n, K, Πr, Πc, αin, αout). Experiment 5 (b): Phase transition. Under DiDCMM(n, K, Πr, Πc, αin, αout), set K = 3, n = nr = nc = 300. Let each row community have 60 pure nodes, each column community have 80 pure nodes, and all mixed nodes have membership (1/3, 1/3, 1/3). We also let αin and αout be in the range of {2.5, 5, 7.5, . . . , 50}. Panel (b) of Figure 8 displays the results, and the analysis is similar to that of Experiment 5 (a). represent |αin−αout|√
max(αin,αout) = 1. Panel (a): Experiment 5 (a); Panel (b): Experiment 5 (b). For Experiments 1–5, we can conclude that DiMSC outperforms its competitors, and this supports our analysis in Remark 6 because DiMSC is designed to estimate mixed memberships, while its competitors are designed for community partition of pure nodes. Experiment 6: Network size. Under SBM(n, pin, pout), let αin = 2 and αout = 0.0001. On the one hand, we have √ αin − √ αout = √ 2− 0.01 < √ 2, i.e., αin and αout locates in the impossible region of exact recovery introduced in [50]. On the other hand, we have αin−αout√
αin > 1, i.e., αin and αout satisfy Equation (14) for DiMSC’s consistent estimation. Let n range in {1000, 2000, 3000, . . . , 15000}. For each n in this experiment, we report the averaged error rate and running time of DiMSC over 10 independent repetitions. The results are shown in Figure 9. From Panel (a) of Figure 9, we see that DiMSC enjoys satisfactory performance with a small error rate for this experiment. Panels (b) of Figure 9 says that DiMSC processes computer-generated networks of up to 15,000 nodes within hundreds of seconds. Remark 7. For visuality, we provide some examples of different types of directed networks generated under DiDCMM in this remark. Let θr(i) = 0.9 + i 2
9n2r for 1 ≤ i ≤ nr. Let each row community
has nr,0 pure nodes, and each column community has nc,0 pure nodes. Let all mixed nodes have membership (1/K, . . . , 1/K). For the setting of P, we set it as
Pa = [ 0.9 0.05 0.1 0.95 ] or Pb = [ 0.1 0.95 0.9 0.05 ] or Pc = [ 12 1 0 12 ] log(nr) nr or Pd = [ 0 12 12 1 ] log(nr) nr or
Pe = 12 1 00 12 0 1 0 12  log(nr)nr or Pf =  1 0 1212 0 0 0 12 1  log(nr)nr , where K = 2 when P is Pa, Pb, Pc or Pd, and K = 3 when P is Pe or Pf . Meanwhile, we can generate different types of directed networks under DiDCMM by considering the above six different settings of P, where these different types are also considered in Experiments 1–6, and we mainly provide the visuality for these directed networks with different structures provided in different P for this remark. Note that we allow P to have non-unit diagonals here because Condition (I1) is mainly for our theoretical buildings, and results for previous experiments show that DiMSC performs stable even when P has non-unit diagonals. We consider below eight settings. Model Setup 1: Set nr = 16, nr,0 = 6, nc = 16, nc,0 = 7, and P as Pa. For this setup, a directed network with 16 row nodes and 16 column nodes is generated from DiDCMM. Figure 10 shows a directed network N generated under Model Setup 1, where we also report DiMSC’s error rate. Figure 10 says that there are more directed edges sent from row nodes 1–6 to column nodes 1–7 than from row nodes 7–12 to column nodes 1–7 for Pa. With given adjacency matrix A and known memberships Πr and Πc for this setup, readers can apply our DiMSC directly to A given in Panel (a) of Figure 10 to check the effectiveness of DiMSC. Model Setup 2: All settings are the same as Model Setup 1 except that we let P be Pb. The directed network N and its adjacency matrix are shown in Figure 11. We see that there are more directed edges sent from row nodes 1–6 to column nodes 10–16 than from row nodes 7–12 to column nodes 10–16 for Pb, which means that directed network generated using Pb and directed network from Pa has different structures. Model Setup 3: Set nr = 32, nr,0 = 14, nc = 28, nc,0 = 12, and P as Pa. For this setup, a bipartite network with 32 row nodes and 28 column nodes are generated from DiDCMM. Figure 12 shows this bipartite network and its adjacency matrix. Model Setup 4: All settings are the same as Model Setup 3 except that we let P be Pb. Figure 13 displays the results, and we see that the bipartite network from Pb also has a different structure compared with the one generated from using Pa under DiDCMM. Model Setup 5: Set nr = 100, nr,0 = 48, nc = 100, nc,0 = 45, and P as Pc. Figure 14 shows the row and column communities for a directed network generated from Setup 5 under DiDCMM, where we plot the directed network directly. Model Setup 6: All settings are the same as Model Setup 5 except that we let P be Pd. Figure 15 shows a directed network obtained from this setup, and we see that the structure of the
directed network from Pd in Figure 15 differs a lot from that of the directed network from Pc shown in Figure 14. Model Setup 7: Set nr = 100, nr,0 = 30, nc = 100, nc,0 = 32, and P as Pe. Figure 16 shows a directed network generated from this setup. Model Setup 8: All settings are the same as Model Setup 7 except that we let P be Pf . Figure 17 displays a directed network generated from this setup, and we see that directed networks from Pf and Pe have different structures by comparing Figures 16 and 17. 6. application to real-world directed networks For the empirical directed networks considered here, row nodes are always the same as column nodes, so we let nr = nc = n. For Π̂r, we call node i highly mixed node if 0.8 ≥ max1≤k≤KΠ̂r(i, k), similar for Π̂c. A highly mixed node tells us whether a node has mixed memberships and belongs to multiple communities. Let τr = |i:0.8≥max1≤k≤KΠ̂r(i,k)| n be the proportion of highly mixed nodes among all nodes to measure the mixability of all row communities. Define τc similar to τr. Let ˆ̀r be a vector such that ˆ̀r(i) = argmax1≤k≤KΠ̂r(i, k) for 1 ≤ i ≤ n, where we use ˆ̀r(i) to denote the home base row community of node i. Define ˆ̀c similar to ˆ̀r. To measure the asymmetric structure of a directed network, we use
Hammrc = minP∈SP ‖Π̂cP − Π̂r‖1
n ,
where a large Hammrc means that the structure of row clusters differs a lot from that of column clusters. For 1 ≤ i ≤ n, let dr(i) = ∑nj=1 A(i, j) be the number of edges sent by node i, dc(i) = ∑nj=1 A(j, i) be the number of edges received by node i, where dr(i) (and dc(i)) is the out degree (in degree) of node i. Since there are many nodes with zero in degree or out degree for real-world directed network, we need the below pre-processing: for any directed network N , we let Am be its adjacency matrix for any positive integer m such that Am is connected, and every node has at least m in degree and m out degree in Am. We apply DiMSC to the following real-world directed networks to discover their mixability, asymmetries, and directional communities. Political blogs: This is a directed network of hyperlinks between weblogs on US politics [68]. In this data, node means a blog, and edge means a hyperlink. This data can be downloaded from http://www-personal.umich.edu/~mejn/netdata/ (accessed on 28 August 2022). It is well-known that there are two parties, “liberal” and “conservative”, so K = 2 for this data. The are 1490 nodes in the original data. After pre-processing, A1 ∈ {0, 1}813×813, A3 ∈ {0, 1}495×495, A6 ∈ {0, 1}285×285, A9 ∈ {0, 1}158×158, where we focus on the cases when m = 1, 3, 6, 9 for this data here. Meanwhile, we use political blogs Am to denote this network when its adjacency matrix is Am, where every node has a degree at least m. Similar notations hold for other real-world directed networks used in this paper. Wikipedia links (gan): This directed network consists of the Wikilinks of Wikipedia in the Gan Chinese language (gan). In this data, node means an article, and the directed edge is a Wikilink [69]. This data can be downloaded from http://konect.cc/networks/ wikipedia_link_gan (accessed on 28 August 2022). There are 9189 nodes in the original data. After pre-processing, A1 ∈ {0, 1}6012×6012, A30 ∈ {0, 1}820×820, A60 ∈ {0, 1}559×569, A90 ∈ {0, 1}240×240, where we study the cases m = 1, 30, 60, 90 for this data. The leading 20 singular values of A1, A30, A60, A90 shown in Panels (e)–(h) of Figure 18 suggest K = 2 for these four adjacency matrices, where [30] also uses eigengap to estimate K. Wikipedia links (nah): This network consists of the Wikilinks of the Nāhuatl language (nah) [69] and can be downloaded from http://konect.cc/networks/wikipedia_link_nah/ (accessed on 28 August 2022). The original data has 10285 nodes. After pre-processing, A1 ∈ {0, 1}6924×6924, A20 ∈ {0, 1}1057×1057, A30 ∈ {0, 1}486×486, A40 ∈ {0, 1}136×136. Panel (i) of Figure 18 suggests K = 4 for A1, and Panels (j)–(l) of Figure 18 suggest K = 2 for A20, A30, and A40. Note that it only takes around 4 seconds for DiMSC to estimate memberships of Wikipedia links (nah) A1. The proportions of highly mixed nodes and Hammrc when applying DiMSC on the above real-world directed networks are reported in Table 1. For the political blogs network, small τr, τc, and Hammrc indicate that there are only a few highly mixed nodes, and the structure of row communities is similar to that of column communities, i.e., there is a slight asymmetry for this data. For Wikipedia links (gan) A1 and Wikipedia links (nah) A1, they have a large proposition of highly mixed nodes in both row and column communities, and the row communities differ a lot from column communities, suggesting heavy asymmetric structure between row and column communities for these two data. For Wikipedia links (gan) A30, A60, and Wikipedia links (nah) A20, we see that the proportion of highly mixed nodes for row (column) communities is small (large), and there is a slight asymmetric for these data. For Wikipedia links (gan) A90 and Wikipedia links (nah) A30, A40, there is no highly mixed node, and the structure of row clusters is similar to that of column clusters. For visualization, we plot the row and column communities as well as highly mixed nodes by applying DiMSC to some of these directed networks in Figures 19 and 20. 7. discussion and conclusions In this paper, we propose a novel directed degree corrected mixed membership (DiDCMM) model. DiDCMM models a directed network with mixed memberships for row nodes with degree heterogeneities and column nodes without degree heterogeneities. DiDCMM is identifiable when the two well-used Conditions (I1) and (I2) hold. It should be mentioned that a model modeling a directed network with mixed memberships for both row and column nodes with degree heterogeneities is unidentifiable unless considering some nontrivial conditions. To fit the model, we propose a provably consistent spectral algorithm called DiMSC to infer community memberships for both row and column nodes in a directed network generated by DiDCMM. DiMSC is designed based on the SVD of the adjacency matrix, where we apply the SP algorithm to hunt for the corners in the simplex structure and the SVM-cone algorithm to hunt for the corners in the cone structure. The theoretical results of DiMSC show that it consistently recovers memberships of both row nodes and column nodes under mild conditions. Meanwhile, when DiDCMM degenerates to MMSB, our theoretical results match that of Theorem 2.2 [24] when their DCMM degenerates to MMSB under mild conditions. Experiments conducted on synthetic directed networks generated from DiDCMM verify the effectiveness and the stability of Conditions (I1) and (I2) of DiMSC. Results for real-world directed networks show that DiMSC reveals highly mixed nodes and asymmetries in the structure of row and column communities. The model DiDCMM and the algorithm DiMSC developed in this paper are useful to discover asymmetry for a directed network with mixed memberships. DiDCMM can also generate an artificially directed network with mixed memberships as a benchmark directed network for research purposes. We wish that DiDCMM and DiMSC can be widely applied in social network analysis. Funding: This research was funded by the Scientific research start-up fund of CUMT NO. 102520253, the High-level personal project of Jiangsu Province NO. JSSCBS20211218. Institutional Review Board Statement: Not applicable. Data Availability Statement: Not applicable. Conflicts of Interest: The author declares no conflict of interest. Abbreviations The following abbreviations are used in this manuscript:
SBM Stochastic Blockmodel DCSBM Degree Corrected Stochastic Blockmodel MMSB Mixed Membership Stochastic Blockmodel DCMM Degree Corrected Mixed Membership model OCCAM Overlapping Continuous Community Assignment model ScBM Stochastic co-Blockmodel DC-ScBM Degree Corrected Stochastic co-Blockmodel DiMMSB Directed Mixed Membership Stochastic Blockmodel DiDCMM Directed Degree Corrected Mixed Membership model SP Successive projection algorithm SVD Singular value decomposition DiMSC Directed Mixed Simplex & Cone algorithm appendix a. proof for identifiability Appendix A.1. Proof of Proposition 1
Proof. Let Ω = UΛV′ be the compact singular value decomposition of Ω. Lemma 1 gives V = ΠcBc ≡ ΠcV(Ic, :). Since Ω = Ω̃, V also equals to Π̃cV(Ic, :), which gives that Πc = Π̃c. Since Ω(Ir, Ic) = Θr(Ir, Ir)Πr(Ir, :)PΠ′c(Ic, :) = Θr(Ir, Ir)P = U(Ir, :)ΛV′(Ic, :) by Condition (I2), we have Θr(Ir, Ir)P = U(Ir, :)ΛV′(Ic, :), which gives that Θr(Ir, Ir) = diag(U(Ir, :)ΛV′(Ic, :)). From this step, we see that if P’s diagonal entries are not ones, we cannot obtain Θr(Ir, Ir) = diag(U(Ir, :)ΛV′(Ic, :)) which leads to a consequence that Θr(Ir, Ir) does not equal to Θ̃r(Ir, Ir); hence Condition (I1) is necessary by Condition (I1). Since Ω = Ω̃, we also have Θ̃r(Ir, Ir) = diag(U(Ir, :)ΛV′(Ic, :)), which gives that Θr(Ir, Ir) = Θ̃r(Ir, Ir). Since Θ̃r(Ir, Ir)P̃ also equals to U(Ir, :)ΛV′(Ic, :), we have P = P̃. Lemma 1 gives that U = ΘrΠrBr, where Br = Θ−1r (Ir, Ir)U(Ir, :). Since Ω = Ω̃, we also have U = Θ̃rΠ̃r B̃r. Since B̃r = Θ̃−1r (Ir, Ir)U(Ir, :) = Θ−1r (Ir, Ir)U(Ir, :), we have B̃r = Br. Since U = ΘrΠrBr = Θ̃rΠ̃r B̃r = Θ̃rΠ̃rBr, we have ΘrΠr = Θ̃rΠ̃r. Since each row of Πr or Π̃r is a PMF, Θr = Θ̃r, Πr = Π̃r, and the claim follows. appendix b. ideal simplex, ideal cone Appendix B.1. Proof of Lemma 1
Proof. First, we consider U and V. Since Ω = UΛV′, we have U = ΩVΛ−1 since V′V = IK. Recall that Ω = ΘrΠrPΠ′c, we have U = ΘrΠrPΠ′cVΛ−1 = ΘrΠrBr, where we set Br = PΠ′cVΛ−1 and sure it is unique. Since U(Ir, :) = Θr(Ir, Ir)Πr(Ir, :)Br = Θr(Ir, Ir)Br, we have Br = Θ−1r (Ir, Ir)U(Ir, :). Similarly, since Ω = UΛV′, we have V′ = Λ−1U′Ω since U′U = IK, hence V = Ω′UΛ−1. Recall that Ω = ΘrΠrPΠ′c, we have V = (ΘrΠrPΠ′c)′UΛ−1 = ΠcP′Π′rΘrUΛ−1 = ΠcBc, where we set Bc = P′Π′rΘrUΛ−1 and sure it is unique. Since V(Ic, :) = Πc(Ic, : )Bc = Bc, we have Bc = V(Ic, :). Meanwhile, for 1 ≤ j ≤ nc, we have V(j, :) = e′jΠcBc = Πc(j, :)Bc. Hence, we have V(j, :) = V( j̄, :) as long as Πc(j, :) = Πc( j̄, :). Now, we show the ideal cone structure that appears in U∗. For convenience, set M = ΠrBr, hence U = ΘrΠrBr gives U = Θr M. Hence, we have U(i, :) = e′iU = Θr(i, i)M(i, :). Therefore, U∗(i, :) =
U(i,:) ‖U(i,:)‖F = M(i,:)‖M(i,:)‖F , combine it with the fact that
Br = Θ−1r (Ir, Ir)U(Ir, :), we have
U∗ =  1 ‖M(1,:)‖F 1 ‖M(2,:)‖F . . . 1 ‖M(nr ,:)‖F
ΠrBr
=  Πr(1, :)/‖M(1, :)‖F Πr(2, :)/‖M(2, :)‖F
... Πr(nr, :)/‖M(nr, :)‖F Θ−1r (Ir, Ir)N−1U (Ir, Ir)U∗(Ir, :). Therefore, we have
Y =  Πr(1, :)/‖M(1, :)‖F Πr(2, :)/‖M(2, :)‖F
... Πr(nr, :)/‖M(nr, :)‖F Θ−1r (Ir, Ir)N−1U (Ir, Ir) = NMΠrΘ−1r (Ir, Ir)N−1U (Ir, Ir), where NM is a diagonal matrix with NM(i, i) = 1‖M(i,:)‖F for 1 ≤ i ≤ nr. All entries of Y are nonnegative, and since we assume that each community has at least one pure node, no row of Y is 0. Then, we prove that U∗(i, :) = U∗(ī, :) when Πr(i, :) = Πr(ī, :). For 1 ≤ i ≤ nr, we have
U∗(i, :) = e′iU∗ = 1
‖M(i, :)‖F e′i M = 1 ‖Πr(i, :)Br‖F Πr(i, :)Br,
and the claim follows immediately. Appendix B.2. Proof of Lemma 2
Proof. Since I = U′U = B′rΠ′rΘ2r ΠrBr = U′(Ir, :)Θ−1r (Ir, Ir)Π′rΘ2r ΠrΘ−1(Ir, Ir)U(Ir, :) and rank(U(Ir, :)) = K (i.e., the inverse of U(Ir, :) exists), we have (U(Ir, :)U′(Ir, :))−1 = Θ−1r (Ir, Ir)Π′rΘ2r ΠrΘ−1r (Ir, Ir). Since U∗(Ir, :) = NU(Ir, Ir)U(Ir, :), we have
(U∗(Ir, :)U′∗(Ir, :))−1 = N−1U (Ir, Ir)Θ −1(Ir, Ir)Π′rΘ2r ΠrΘ−1r (Ir, Ir)N−1U (Ir, Ir). Since all entries of N−1U (Ir, Ir), Πr, Θr and nonnegative and N, Θr are diagonal matrices, we see that all entries of (U∗(Ir, :)U′∗(Ir, :))−1 are nonnegative, and its diagonal entries are strictly positive, hence we have (U∗(Ir, :)U′∗(Ir, :))−11 > 0. Appendix B.3. Proof of Theorem 1
Proof. For column nodes, Remark A1 guarantees that SP algorithm returns Ic when the input is V with K column communities, hence ideal DiMSC recovers Πc exactly. For row nodes, Remark A2 guarantees that SVM-cone algorithm returns Ir when the input is U∗ with K row communities, hence ideal DiMSC recovers Πr exactly, and this theorem follows. appendix c. equivalence algorithm In this subsection, we design one algorithm DiMSC-equivalence which returns the same estimations as DiMSC. Set U2 = UU′ ∈ Rnr×nr , Û2 = ÛÛ′ ∈ Rnr×nr , V2 = VV′ ∈ Rnc×nc , V̂2 = V̂V̂′ ∈ Rnc×nc . Set U∗,2 ∈ Rnr×nr as U∗,2(i, :) = U2(i,:)‖U2(i,:)‖F for 1 ≤ i ≤ nr. Û∗,2 is defined similarly. The next lemma guarantees that V2 enjoys IS structure, and U∗,2 enjoys IC structure. Lemma A1. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), we have V2 = ΠcV2(Ic, :), and U∗,2 = YU∗,2(Ir, :). Proof. By Lemma 1, we know that V = ΠcV(Ic, :), which gives that V2 = VV′ = ΠcV(Ic, :)V′ = Πc(VV′)(Ic, :) = ΠcV2(Ic, :). For U, since U = ΘrΠrΘ−1r (Ir, Ir)U(Ir, :) by Lemma 1, we have U2 = UU′ = ΘrΠrΘ−1r (Ir, Ir)U(Ir, :)U′ = ΘrΠrΘ−1r (Ir, Ir)(UU′) (Ir, :) = ΘrΠrΘ−1r (Ir, Ir)U2(Ir, :). Set M2 = ΠrΘ−1r (Ir, Ir)U2(Ir, :), we have U2 = Θr M2. Then, follow similar proof as Lemma 1, we have U∗,2 = Y2U∗,2(Ir, :), where Y2 = NM2 ΠrΘ −1 r (Ir, Ir)N−1U2 (Ir, Ir), and NM2 , NU2 are nr × nr diagonal matrices whose ith diagonal entries are 1‖M2(i,:)‖F , 1 ‖U2(i,:)‖F , respectively. Since ‖U2(i, :)‖F = ‖U(i, :)U′‖F = ‖U(i, :)‖F, we have NU2 = NU . Since ‖M2(i, :)|F = ‖ΠrΘ−1r (Ir, Ir)U2(Ir, :)‖F = ‖ΠrΘ−1r (Ir, Ir)U(Ir, :)U′‖F = ‖M(i, :)‖F, we have NM2 = NM. Hence, Y2 ≡ Y and the claim follows. Since U∗,2(Ir, :) ∈ RK×nr and V2(Ic, :) ∈ RK×nc , U∗,2(Ir, :) and V2(Ic, :) are singular matrix with rank K by Condition (I1), while the inverses of U∗,2(Ir, :)U′∗,2(Ir, :) and V2(Ic, : )V′2(Ic, :) exist. Therefore, Lemma A1 gives that
Y = U∗,2U′∗,2(Ir, :)(U∗,2(Ir, :)U′∗,2(Ir, :))−1, Πc = V2V′2(Ic, :)(V2(Ic, :)V′2(Ic, :))−1. Since U∗,2 = NUU2 and Y = NMΠrΘ−1r (, Ir, Ir)N−1U (Ir, Ir), we see that Y∗ also equals to U2U′∗,2(Ir, :)(U∗,2(Ir, :)U′∗,2(Ir, :))−1 by basic algebra. Based on the above analysis, we are now ready to give the ideal DiMSC-equivalence. Input Ω. Output: Πr and Πc. • Obtain U, Λ, V, U∗,2, V2 from Ω. • Run SP algorithm on V2 with K column communities to obtain V2(Ic, :). Run SVMcone algorithm on U∗,2 with K row communities to obtain Ir. • Set J∗ = diag(U∗(Ir, :)ΛV′(Ic, :)), Y∗ = U2U′∗,2(Ir, :)(U∗,2(Ir, :)U′∗,2(Ir, :))−1, Zr = Y∗ J∗ and Zc = V2V′2(Ic, :)(V2(Ic, :)V′2(Ic, :))−1. • Recover Πr and Πc by setting Πr(i, :) =
Zr(i,:) ‖Zr(i,:)‖1 for 1 ≤ i ≤ nr, and Πc(j, :) = Zc(j,:)‖Zc(j,:)‖1 for 1 ≤ j ≤ nc. For the real case, set Û2 = ÛÛ′, V̂2 = V̂V̂′, Û∗,2 = NÛÛ2. We now extend the ideal case to the real one given by Algorithm A1. Algorithm A1 DiMSC-equivalence
Require: The adjacency matrix A ∈ Rnr×nc of a directed network, the number of row communities (column communities) K. Ensure: The estimated nr × K row membership matrix Π̂r,2 and the estimated nc × K column membership matrix Π̂c,2. 1: Obtain Ã = ÛΛ̂V̂′, the top-K-dimensional SVD of A. Compute Û∗, Û2, V̂2, Û∗,2. 2: Apply SP algorithm on the rows of V̂2 assuming there are K column communities to
obtain Îc,2, the index set returned by SP algorithm. 3: Apply SVM-cone algorithm on the rows of Û∗,2 with K row communities to obtain Îr,2, the index set returned by SVM-cone algorithm. 4: Set Ĵ∗,2 = diag(Û∗( Îr,2, :)Λ̂V̂′(Îc,2, :)), Ŷ∗,2 = Û2Û′∗,2(Îr,2, :)(Û∗,2(Îr,2, :)Û′∗,2(Îr,2, : ))−1, Ẑr,2 = Ŷ∗,2 Ĵ∗,2 and Ẑc,2 = V̂2V̂′2(Îc,2, :)(V̂2(Îc,2, :)V̂′2(Îc,2, :))−1. Then, set Ẑr,2 = max(0, Ẑr,2) and Ẑc,2 = max(0, Ẑc,2). 5: Estimate Πr(i, :) by Π̂r,2(i, :) = Ẑr,2(i, :)/‖Ẑr,2(i, :)‖1, 1 ≤ i ≤ nr and estimate Πc(j, :) by Π̂c,2(j, :) = Ẑc,2(j, :)/‖Ẑc,2(j, :)‖1, 1 ≤ j ≤ nc. Lemma A2. (Equivalence). For the empirical case, we have Îr,2 ≡ Îr, Îc,2 ≡ Îc, Û∗,2(Îr,2, : )Û′∗,2(Îr,2, :) ≡ Û∗(Îr, :)Û′∗(Îr, :), Ŷ∗,2 ≡ Ŷ∗, Ĵ∗,2 ≡ Ĵ∗, Ẑr,2 ≡ Ẑr, Ẑc,2 ≡ Ẑc, Π̂r,2 ≡ Π̂r and Π̂c,2 ≡ Π̂c. Proof. For column nodes, Lemma 3.2 [27] gives Îc = Îc,2 (i.e., SP algorithm will return the same indices on both V̂ and V̂2. ), which gives that V̂2V̂′2(Îc,2, :) = V̂2V̂′2(Îc, : ) = V̂V̂′((V̂V̂′)(Îc, :))′ = V̂V̂′(V̂(Îc, :)V̂′)′ = V̂V̂′V̂V̂′(Îc, :) = V̂V̂′(Îc, :), and V̂2(Îc,2, : )V̂′2(Îc,2, :) = V̂2(Îc, :)V̂′2(Îc, :) = V̂(Îc, :)V̂′(V̂(Îc, :)V̂′)′ = V̂(Îc, :)V̂′(Îc, :). Therefore, we have Ẑc,2 = Ẑc, Π̂c,2 = Π̂c. For row nodes, Lemma G.1 [26] guarantees that Îr = Îr,2 (i.e., SVM-cone algorithm will return the same indices on both Û∗ and Û∗,2. ), so immediately we have Ĵ∗,2 = Ĵ∗. Since Û∗,2(Îr,2, :) = Û∗,2(Îr, :) = NÛ(Îr, Îr)Û2(Îr, :) = NÛ(Îr, Îr)Û(Îr, :)Û′ = Û∗(Îr, :)Û′, we have Û2Û′∗,2(Îr,2, :) = Û2Û′∗,2(Îr, :) = ÛÛ′ÛÛ′∗(Îr, :) = ÛÛ′∗(Îr, :) and (Û∗,2(Îr,2, : )Û′∗,2(Îr,2, :))−1 = (Û∗,2(Îr, :)Û′∗,2(Îr, :))−1 = (Û∗(Îr, :)Û′∗(Îr, :))−1, which give that Ŷ∗,2 = Ŷ∗, and the claim follows immediately. Lemma A2 guarantees that the DiMSC and DiMSC-equivalence return same estimations for both row and column nodes’s memberships. In this article, we introduce the DiMSC-equivalence algorithm since it is helpful to build a theoretical framework for DiMSC, see Remark A3 and A4 for detail. appendix d. basic properties of ω Lemma A3. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), we have
θr,min θr,max √ Kλ1(Π′rΠr) ≤ ‖U(i, :)‖F ≤
θr,max θr,min √
λK(Π′rΠr) , 1 ≤ i ≤ nr,√
1 Kλ1(Π′cΠc)
≤ ‖V(j, :)‖F ≤ √
1 λK(Π′cΠc) , 1 ≤ j ≤ nc. Proof. Since I = U′U = U′(Ir, :)Θ−1r (Ir, Ir)Π′rΘ2r ΠrΘ−1(Ir, Ir)U(Ir, :), we have
((Θ−1r (Ir, Ir)U(Ir, :))((Θ−1r (Ir, Ir)U(Ir, :))′)−1 = Π′rΘ2r Πr,
which gives that
maxk‖e′k(Θ −1 r (Ir, Ir)U(Ir, :))‖2F = maxke′k(Θ −1 r (Ir, Ir)U(Ir, :))(Θ−1r (Ir, Ir)U(Ir, :))′ek
≤ max‖x‖F=1x ′(Θ−1r (Ir, Ir)U(Ir, :))(Θ−1r (Ir, Ir)U(Ir, :))′x
= λ1((Θ−1r (Ir, Ir)U(Ir, :))(Θ−1r (Ir, Ir)U(Ir, :))′) = 1
λK(Π′rΘ2r Πr) ≤ 1 θ2r,minλK(Π ′ rΠr) . Similarly, we have
mink‖e′k(Θ −1 r (Ir, Ir)U(Ir, :))‖2F ≥ 1 λ1(Π′rΘ2r Πr) ≥ 1 θ2r,maxλ1(Π′rΠr) . Since U(i, :) = e′iU = e ′ iΘrΠrΘ −1 r (Ir, Ir)U(Ir, :) = θr(i)Πr(i, :)Θ−1r (Ir, Ir)U(Ir, :) for 1 ≤ i ≤ nr, we have
‖U(i, :)‖F = ‖θr(i)Πr(i, :)Θ−1r (Ir, Ir)U(Ir, :)‖F = θr(i)‖Πr(i, :)Θ−1r (Ir, Ir)U(Ir, :)‖F ≤ θr(i)maxi‖Πr(i, :)‖Fmaxi‖e′i(Θ−1r (Ir, Ir)U(Ir, :))‖F
≤ θr(i)maxi‖e′i(Θ−1r (Ir, Ir)U(Ir, :))‖F ≤ θr,max θr,min √ λK(Π′rΠr) . Similarly, we have
‖U(i, :)‖F ≥ θr(i)mini‖Πr(i, :)‖Fmini‖e′i(Θ−1r (Ir, Ir)U(Ir, :))‖F
≥ θr(i)mini‖e′i(Θ−1r (Ir, Ir)U(Ir, :))‖F/ √
K ≥ θr,min θr,max √ Kλ1(Π′rΠr) . For ‖V(j, :)‖F, since V = ΠcBc, we have
minj‖e′jV‖2F = minje′jVV′ej = minjΠc(j, :)BcB′cΠ′c(j, :)
= minj‖Πc(j, :)‖2F Πc(j, :) ‖Πc(j, :)‖F BcB′c Π′c(j, :) ‖Πc(j, :)‖F ≥ minj‖Πc(j, :)‖2Fmin‖x‖F=1x ′BcB′cx = minj‖Πc(j, :)‖2FλK(BcB′c)
By Lemma A4 = minj‖Πc(j, :)‖2F λ1(Π′cΠc) ≥ 1 Kλ1(Π′cΠc) . Meanwhile,
maxj‖e′jV‖2F = maxj‖Πc(j, :)‖2F Πc(j, :) ‖Πc(j, :)‖F BcB′c Π′c(j, :) ‖Πc(j, :)‖F
≤ maxj‖Πc(j, :)‖2Fmax‖x‖F=1x ′BcB′cx = maxj‖Πc(j, :)‖2FλK(BcB′c)
By Lemma A4 = maxj‖Πc(j, :)‖2F λK(Π′cΠc) ≤ 1 λK(Π′cΠc) . Lemma A4. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), we have
θ2r,minλK(Π ′ rΠr)
θ2r,maxλ1(Π′rΠr) ≤ λK(U∗(Ir, :)U′∗(Ir, :)), λ1(U∗(Ir, :)U′∗(Ir, :)) ≤ θ2r,maxKλ1(Π′rΠr) θ2r,minλK(Π ′ rΠr) , and λ1(BcB′c) = 1
λK(Π′cΠc) , λK(BcB′c) = 1 λ1(Π′cΠc) . Proof. Recall that V = ΠcBc and V′V = I, we have I = B′cΠ′cΠcBc. As Bc is full rank, we have Π′cΠc = (BcB′c)−1, which gives
λ1(BcB′c) = 1
λK(Π′cΠc) , λK(BcB′c) = 1 λ1(Π′cΠc) . By the proof of Lemma 2, we know that
(U∗(Ir, :)U′∗(Ir, :))−1 = N−1U (Ir, Ir)Θ −1(Ir, Ir)Π′rΘ2r ΠrΘ−1r (Ir, Ir)N−1U (Ir, Ir),
which gives that
U∗(Ir, :)U′∗(Ir, :) = NU(Ir, Ir)Θ(Ir, Ir)(Π′rΘ2r Πr)−1Θr(Ir, Ir)NU(Ir, Ir). Then, we have
λ1(U∗(Ir, :)U′∗(Ir, :)) = λ1(NU(Ir, Ir)Θ(Ir, Ir)(Π′rΘ2r Πr)−1Θr(Ir, Ir)NU(Ir, Ir)) = λ1(N2U(Ir, Ir)Θ2r (Ir, Ir)(Π′rΘ2r Πr)−1) ≤ λ21(NU(Ir, Ir)Θr(Ir, Ir))λ1((Π′rΘ2r Πr)−1) = λ21(NU(Ir, Ir)Θr(Ir, Ir))/λK(Π′rΘ2r Πr) ≤ (maxi∈Ir θr(i)/‖U(i, :)‖F) 2/λK(Π′rΘ 2 r Πr)
≤ θ2r,maxKλ1(Π′rΠr)
λK(Π′rΘ2r Πr) ≤ θ2r,maxKλ1(Π′rΠr) θ2r,minλK(Π ′ rΠr) . Similarly, we have
λK(U∗(Ir, :)U′∗(Ir, :)) = λK(NU(Ir, Ir)Θ(Ir, Ir)(Π′rΘ2r Πr)−1Θr(Ir, Ir)NU(Ir, Ir)) = λK(N2U(Ir, Ir)Θ2r (Ir, Ir)(Π′rΘ2r Πr)−1) ≥ λ2K(NU(Ir, Ir)Θr(Ir, Ir))λK((Π′rΘ2r Πr)−1) = λ2K(NU(Ir, Ir)Θr(Ir, Ir))/λ1(Π′rΘ2r Πr) ≥ (mini∈Ir θr(i)/‖U(i, :)‖F) 2/λ1(Π′rΘ 2 r Πr)
≥ θ2r,minλK(Π ′ rΠr)
λ1(Π′rΘ2r Πr) ≥
θ2r,minλK(Π ′ rΠr) θ2r,maxλ1(Π′rΠr) . Lemma A5. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), we have
σK(Ω) ≥ θr,minσK(P)σK(Πr)σK(Πc), σ1(Ω) ≤ θr,maxσ1(P)σ1(Πr)σ1(Πc). Proof. For σK(Ω), we have
σ2K(Ω) = λK(ΩΩ ′) = λK(ΘrΠrPΠ′cΠcP ′Π′rΘr) = λK(Θ 2 r ΠrPΠ ′ cΠcP ′Π′r)
≥ θ2r,minλK(Π′rΠrPΠ′cΠcP′) ≥ θ2r,minλK(Π′rΠr)λK(PΠ′cΠcP′) = θ2r,minλK(Π′rΠr)λK(Π′cΠcP′P) ≥ θ2r,minλK(Π′rΠr)λK(Π′cΠc)λK(PP′) = θ2r,minσ2K(Πr)σ2K(Πc)σ2K(P),
where we have used the fact for any matrices X, Y, the nonzero eigenvalues of XY are the same as the nonzero eigenvalues of YX. Following a similar analysis, the lemma follows. Lemma A6. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), when Assumption 1 holds, with probability at least 1− o((nr + nc)−3), we have
‖A−Ω‖ = O( √
Pmaxmax(‖θr‖1, θr,maxnc)log(nr + nc)). Proof. Since the proof is similar to that of Lemma 7 [35], we omit most of the details. Let ei be an nr × 1 vector, where ei(i) = 1 and 0 elsewhere, for row nodes 1 ≤ i ≤ nr, and ẽj be an nc × 1 vector, where ẽj(j) = 1 and 0 elsewhere, for column nodes 1 ≤ j ≤ nc. Set W = ∑n r
i=1 ∑ nc j=1 W(i, j)ei ẽ ′ j, where W = A−Ω. Set W(i,j) = W(i, j)ei ẽ′j, for 1 ≤ i ≤ nr, 1 ≤ j ≤ nc. Then, we have E(W(i,j)) = 0. For 1 ≤ i ≤ nr, 1 ≤ j ≤ nc, we have
‖W(i,j)‖ = ‖W(i, j)ei ẽ′j‖ = |A(i, j)−Ω(i, j)| ≤ 1. Next, we consider the variance parameter
σ2 := max(‖ nr
∑ i=1
nc
∑ j=1
E(W(i,j)(W(i,j))′)‖, ‖ nr
∑ i=1
nc
∑ j=1
E((W(i,j))′W(i,j))‖). Since
E(W2(i, j)) = E((A(i, j)−Ω(i, j))2) = Var(A(i, j)),
where Var(A(i, j)) denotes the variance of the Bernoulli random variable A(i, j), we have
E(W2(i, j)) = Var(A(i, j)) = P(A(i, j) = 1)(1− P(A(i, j) = 1)) ≤ P(A(i, j) = 1) = Ω(i, j) = e′iΘrΠrPΠ′c ẽj = θr(i)e′iΠrPΠ′c ẽj ≤ θr(i)Pmax. Since eie′i is an nr × nr diagonal matrix with (i, i)-th entry being one and other entries being zero, we have
‖ nr ∑ i=1 nc ∑ j=1 E(W(i,j)(W(i,j))′)‖ = ‖ nr ∑ i=1 nc ∑ j=1 E(W2(i, j))eie′i‖ = max1≤i≤nr | nc ∑ j=1 E(W2(i, j))| ≤ θr,maxPmaxnc. Similarly, we have ‖∑nri=1 ∑ nc j=1 E((W (i,j))′W(i,j))‖ ≤ Pmax‖θr‖1, which gives that
σ2 = max(‖ nr ∑ i=1 nc ∑ j=1 E(W(i,j)(W(i,j))′)‖, ‖ nr ∑ i=1 nc ∑ j=1 E((W(i,j))′W(i,j))‖) ≤ Pmaxmax(‖θr‖1, θr,maxnc). By the rectangular version of the Bernstein inequality [71], combining with σ2 ≤ Pmaxmax(‖θr‖1, θr,maxnc), R = 1, d1 + d2 = nr + nc, set t = α+1+ √ α2+20α+19
3
√ Pmaxmax(‖θr‖1, θr,maxnc)log(nr + nc) for any α > 0, we have
P(‖W‖ ≥ t) = P(‖ nr
∑ i=1 nc ∑ j=1 W(i,j)‖ ≥ t) ≤ (nr + nc)exp(− t2/2 σ2 + Rt3 )
≤ (nr + nc)exp(− t2/2
Pmaxmax(‖θr‖1, θr,maxnc) + t/3 )
= (nr + nc)exp(−(α + 1)log(nr + nc) · 1
18 ( √ α+19+ √ α+1)2 + 2
√ α+1√
α+19+ √ α+1
√ log(nr+nc)
Pmaxmax(‖θr‖1,θr,maxnc)
)
≤ (nr + nc)exp(−(α + 1)log(nr + nc)) = 1
(nr + nc)α ,
where we have used Assumption 1 in the last inequality. Set α = 3, and the claim follows. appendix e. proof of consistency for dimsc Similar to [24,26,27], for our DiMSC, the main theoretical results (i.e., Theorem 2) rely on the row-wise singular vector deviation bounds for the singular eigenvectors of the adjacency matrix. Lemma A7. (Row-wise singular vector deviation) Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), when Assumption 1 holds, suppose σK(Ω) ≥ C √ θr,max(nr + nc)log(nr + nc), with probability at least 1− o((nr + nc)−3), we have
max(‖ÛÛ′ −UU′‖2→∞, ‖V̂V̂′ −VV′‖2→∞) = O( √
Pmaxθr,maxKlog(nr + nc) θr,minσK(P)σK(Πr)σK(Πc) ). Proof. Let HÛ = Û ′U, and HÛ = UHÛ ΣHÛ V ′ HÛ be the SVD decomposition of HÛ with UHÛ , VHÛ ∈ R nr×K, where UHÛ and VHÛ represent, respectively, the left and right sin-
gular matrices of HÛ . Define sgn(HÛ) = UHÛ V ′ HÛ ; sgn(HV̂) is defined similarly. Since E(A(i, j)−Ω(i, j)) = 0, E[(A(i, j)−Ω(i, j))2] ≤ θr(i)Pmax ≤ θr,maxPmax by the proof of Lemma A6, 1√
θr,maxPmaxmin(nr ,nc)/(µlog(nr+nc)) ≤ O(1) holds by Assumption 1, where µ is the
incoherence parameter defined as µ = max( nr‖U‖ 2 2→∞ K , nc‖V‖22→∞
K ). By Theorem 4.4 [64], with high probability, we have below row-wise singular vector deviation
max(‖Ûsgn(HÛ)−U‖2→∞, ‖V̂sgn(HV̂)−V‖2→∞)
≤ C
√ Pmaxθr,maxK(κ(Ω) √ max(nr ,nc)µ min(nr ,nc) + √ log(nr + nc))
σK(Ω) ≤ C √
Pmaxθr,maxKlog(nr + nc) σK(Ω) , (A1)
provided that c1σK(Ω) ≥ √ θr,maxPmax(nr + nc)log(nr + nc) for some sufficiently small
constant c1, and here we set √
max(nr ,nc)µ min(nr ,nc) = O(1) for convenience since this term has little
effect on the error bounds of DiMSC, especially for the case when nrnc = O(1). Since U′U = I, Û′Û = I, we have ‖ÛÛ′ −UU′‖2→∞ ≤ 2‖U − Ûsgn(HÛ)‖2→∞ by basic algebra. Now, we are ready to bound ‖ÛÛ′ −UU′‖2→∞:
‖ÛÛ′ −UU′‖2→∞ = max1≤i≤nr‖e ′ i(UU ′ − ÛÛ′)‖F ≤ 2‖U − Ûsgn(HÛ)‖2→∞ ≤ C √
Pmaxθr,maxKlog(nr + nc) σK(Ω) By Lemma A5 ≤ C
√ Pmaxθr,maxKlog(nr + nc) θr,minσK(P)σK(Πr)σK(Πc) . The lemma holds by following similar proof for ‖V̂V̂′ −VV′‖2→∞. When Θr = ρI, nr = nc, Πr = Πc = Π, Pmax = O(1), and DiCCMM degenerates
to MMSB, the bound in Lemma A7 is O( √
Klog(n) σK(P) √ ρλK(Π′Π) ). if we further assume that
λK(Π′Π) = O( nK ) and K = O(1), the bound is of order O( 1 σK(P) 1√ n
√ log(n)
ρn ). Set the Θ in [24] as √ ρI, their DCMM degenerates to MMSB, their assumptions are translated to our λK(Π′Π) = O( nK ), when K = O(1), the row-wise singular vector deviation bound in the fourth bullet of Lemma 2.1 [24] is O( 1 σK(P) 1√ n √ log(n) ρn ), which is consistent with ours. Meanwhile, if we further assume that σK(P) = O(1), the bound is of order 1√n √ log(n)
ρn . The next lemma is the cornerstone to characterizing the behaviors of DiMSC. Lemma A8. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), when conditions of Lemma A7 hold, there exist two permutation matrices Pr,Pc ∈ RK×K such that with probability at least 1− o((nr + nc)−3), we have
max1≤k≤K‖e′k(Û∗,2(Îr, :)−P ′ rU∗,2(Ir, :))‖F = O(
K3θ11r,maxvκ3(Π′rΠr)λ1.51 (Π ′ rΠr)
θ11r,minπr,min ),
max1≤k≤K‖e′k(V̂2(Îc, :)−P ′ cV2(Ic, :))‖F = O(vκ(Π′cΠc)). Proof. First, we consider column nodes. The detail of the SP algorithm is in Algorithm A2. Algorithm A2 Successive Projection (SP) [54]
Require: Near-separable matrix Ysp = Ssp Msp + Zsp ∈ Rm×n+ , where Ssp, Msp should satisfy Assumption 1 [54], the number r of columns to be extracted. Ensure: Set of indices K such that Y(K, :) ≈ S (up to permutation) 1: Compute Ûr ∈ Rnr×Kr and Ûc ∈ Rnc×Kr from the top-Kr-dimensional SVD of A. 2: Let R = Ysp,K = {}, k = 1. 3: While R 6= 0 and k ≤ r do 4: k∗ = argmaxk‖R(k, :)‖F. 5: uk = R(k∗, :). 6: R← (I − uku ′ k
‖uk‖2F )R.
7: K = K ∪ {k∗}. 8: k=k+1. 9: end while
Based on Algorithm A2, the following theorem is Theorem 1.1 in [54]. Theorem A1. Fix m ≥ r and n ≥ r. Consider a matrix Ysp = Ssp Msp + Zsp, where Ssp ∈ Rm×r has a full column rank, Msp ∈ Rr×n is a nonnegative matrix such that the sum of each column is at most 1, and Zsp = [Zsp,1, . . . , Zsp,n] ∈ Rm×n. Suppose Msp has a submatrix equal to Ir. Write e ≤ max1≤i≤n‖Zsp,i‖F. Suppose e = O(
σmin(Ssp)√ rκ2(Ssp)
), where σmin(Ssp) and κ(Ssp) are the minimum singular value and condition number of Ssp, respectively. If we apply the SP algorithm to columns of Ysp, then it outputs an index set K ⊂ {1, 2, . . . , n} such that |K| = r and max1≤k≤rminj∈K‖Ssp(:, k) − Ysp(:, j)‖F = O(eκ2(Ssp)), where Ssp(:, k) is the k-th column of Ssp. Let m = K, r = K, n = nc, Ysp = V̂′2, Zsp = V̂ ′ 2 −V′2, Ssp = V′2(Ic, :), and Msp = Π′c. By
Condition (I2), Msp has an identity submatrix IK. By Lemma A7, we have
ec = max1≤j≤nc‖V̂2(j, :)−V2(j, :)‖F = ‖V̂2(j, :)−V2(j, :)‖2→∞ ≤ v.
By Theorem A1, there exists a permutation matrix Pc such that
max1≤k≤K‖e′k(V̂2(Îc, :)−P ′ cV2(Ic, :))‖F = O(ecκ2(V2(Ic, :)) √ K) = O(vκ2(V2(Ic, :))). Since κ2(V2(Ic, :)) = κ(V2(Ic, :)V′2(Ic, :)) = κ(V(Ic, :)V′(Ic, :)) = κ(Π′cΠc), where the last equality holds by Lemma A4, we have
max1≤k≤K‖e′k(V̂2(Îc, :)−P ′ cV2(Ic, :))‖F = O(vκ(Π′cΠc)). Remark A1. For the ideal case, let m = K, r = K, n = nc, Ysp = V′, Zsp = V′ −V′ ≡ 0, Ssp = V′(Ic, :), and Msp = Π′c. Then, we have max1≤j≤nc‖V(j, :)− V(j, :)‖F = 0. By Theorem A1, SP algorithm returns Ic when the input is V assuming there are K column communities. Now, we consider row nodes. From Lemma 2, we see that U∗(Ir, :) satisfies Condition 1 in [26]. Meanwhile, since (U∗(Ir, :)U′∗(Ir, :))−11 > 0, we have (U∗(Ir, :)U′∗(Ir, :))−11 ≥ η1, hence U∗(Ir, :) satisfies Condition 2 in [26]. Now, we give a lower bound for η to show that η is strictly positive. By the proof of Lemma A4, we have
(U∗(Ir, :)U′∗(Ir, :))−1 = N−1U (Ir, Ir)Θ −1(Ir, Ir)Π′rΘ2r ΠrΘ−1r (Ir, Ir)N−1U (Ir, Ir)
≥ θ2r,min
θ2r,maxN2U,max Π′rΠr ≥ θ4r,min θ4r,maxKλ1(Π′rΠr) Π′rΠr,
where we set NU,max = max1≤i≤nr NU(i, i), and we have used the facts that NU , Θr are
diagonal matrices, and NU,max ≤ θr,max √ Kλ1(Π′rΠr) θr,min by Lemma A3. Then, we have
η = min1≤k≤K((U∗(Ir, :)U′∗(Ir, :))−11)(k) ≥ θ4r,min
θ4r,maxKλ1(Π′rΠr) min1≤k≤Ke′kΠ ′ rΠr1
= θ4r,min
θ4r,maxKλ1(Π′rΠr) min1≤k≤Ke′kΠ ′ r1 =
θ4r,minπr,min
θ4r,maxKλ1(Π′rΠr) ,
i.e., η is strictly positive. Since U∗,2(Ir, :)U′∗,2(Ir, :) ≡ U∗(Ir, :)U′∗(Ir, :), we have U∗,2(Ir, :) also satisfies Conditions 1 and 2 in [26]. The above analysis shows that we can directly apply Lemma F.1 of [26] since the ideal DiMSC algorithm satisfies Conditions 1 and 2 in [26], therefore there exists a permutation matrix Pr ∈ RK×K such that
max1≤k≤K‖e′k(Û∗,2(Îr, :)−P ′ rU∗,2(Ir, :))‖F = O(
√ Kζer
λ1.5K (U∗,2(Ir, :))U′∗,2(Ir, :) ),
where ζ ≤ 4K ηλ1.5K (U∗,2(Ir ,:)U′∗,2(Ir ,:)) = O( K ηλ1.5K (U∗(Ir ,:)U′∗(Ir ,:)) ), and er = max1≤i≤nr‖Û∗,2(i, : )−U∗,2(i, :)‖. Next, we bound er as below
‖Û∗,2(i, :)−U∗,2(i, :)‖F = ‖ Û2(i, :)‖U2(i, :)‖F −U2(i, :)‖Û2(i, :)‖F
‖Û2(i, :)‖F‖U2(i, :)‖F ‖F ≤ 2‖Û2(i, :)−U2(i, :)‖F ‖U2(i, :)‖F
≤ 2‖Û2 −U2‖2→∞‖U2(i, :)‖F ≤ 2v‖U2(i, :)‖F = 2v ‖(UU′)(i, :)‖F = 2v ‖U(i, :)U′‖F = 2v ‖U(i, :)‖F
≤ 2v θr,max √
Kλ1(Π′rΠr) θr,min ,
where the last inequality holds by Lemma A3. Then, we have er = O(v θr,max √ Kλ1(Π′rΠr) θr,min
). Finally, by Lemma A4, we have
max1≤k≤K‖e′k(Û∗,2(Îr, :)−P ′ rU∗,2(Ir, :))‖F = O(
K3θ11r,maxvκ3(Π′rΠr)λ1.51 (Π ′ rΠr)
θ11r,minπr,min ). Remark A2. For the ideal case, when setting U∗ as the input of the SVM-cone algorithm assuming there are K row communities, since ‖U∗ −U∗‖2→∞ = 0, Lemma F.1; [26] guarantees that SVMcone algorithm returns Ir exactly. Meanwhile, another view to see that the SVM-cone algorithm exactly obtains Ir when the input is U∗ (also U2,∗) is given in Appendix F, which focuses on following the three steps of SVM-cone algorithm to show that it returns Ir with input U∗ (also U∗,2), instead of simply applying Lemma F.1. [26]. Lemma A9. Under DiDCMMnr ,nc(K, P, Πr, Πc, Θr), when conditions of Lemma A7 hold, with probability at least 1− o((nr + nc)−3), we have
max1≤i≤nr‖e ′ i(Ẑr − ZrPr)‖F = O(
K5θ15r,maxvκ4.5(Π′rΠr)κ(Πc)λ1.51 (Π ′ rΠr)
θ14r,minπr,min ),
max1≤j≤nc‖e ′ j(Ẑc − ZcPc)‖F = O(vκ(Π′cΠc) √ Kλ1(Π′cΠc)). Proof. First, we consider column nodes. Recall that V(Ic, :) = Bc. For convenience, set V̂(Îc, :) = B̂c, V2(Ic, :) = B2c, V̂2(Îc, :) = B̂2c. We bound ‖e′j(Ẑc − ZcPc)‖F when the input
is V̂ in the SP algorithm. Recall that Zc = max(VV′(Ic, :)(V(Ic, :)V′(Ic, :))−1, 0) ≡ Πc, for 1 ≤ j ≤ nc, we have
‖e′j(Ẑc − ZcPc)‖F = ‖e ′ j(max(0, V̂B̂ ′ c(B̂c B̂ ′ c) −1)−VB′c(BcB′c)−1Pc)‖F
≤ ‖e′j(V̂B̂ ′ c(B̂c B̂ ′ c) −1 −VB′c(BcB′c)−1Pc)‖F = ‖e′j(V̂ −V(V ′V̂))B̂′c(B̂c B̂ ′ c) −1 + e′j(V(V ′V̂)B̂′c(B̂c B̂ ′ c) −1 −V(V′V̂)(P ′c(BcB′c)(B′c)−1(V′V̂))−1)‖F ≤ ‖e′j(V̂ −V(V ′V̂))B̂′c(B̂c B̂ ′ c) −1‖F + ‖e′jV(V ′V̂)(B̂′c(B̂c B̂ ′ c) −1 − (P ′c(BcB′c)(B′c)−1(V′V̂))−1)‖F ≤ ‖e′j(V̂ −V(V ′V̂))‖F‖B̂−1c ‖F + ‖e′jV(V ′V̂)(B̂′c(B̂c B̂ ′ c) −1 − (P ′c(BcB′c)(B′c)−1(V′V̂))−1)‖F ≤ √
K‖e′j(V̂ −V(V ′V̂))‖F/ √ λK(B̂c B̂′c) + ‖e′jV(V ′V̂)(B̂−1c − (P ′cBc(V′V̂))−1)‖F
= √
K‖e′j(V̂V̂ ′ −VV′)V̂‖FO( √ λ1(Π′cΠc)) + ‖e′jV(V ′V̂)(B̂−1c − (P ′cBc(V′V̂))−1)‖F
≤ √
K‖e′j(V̂V̂ ′ −VV′)‖FO( √ λ1(Π′cΠc)) + ‖e′jV(V ′V̂)(B̂−1c − (P ′cBc(V′V̂))−1)‖F
≤ √ KvO( √
λ1(Π′cΠc)) + ‖e′jV(V ′V̂)(B̂−1c − (P ′cBc(V′V̂))−1)‖F = O(v √
Kλ1(Π′cΠc)) + ‖e′jV(V ′V̂)(B̂−1c − (P ′cBc(V′V̂))−1)‖F,
where we have used similar idea in the proof of Lemma VII.3 in [27] such that apply O( 1
λK(BcB′c) ) to estimate 1 λK(B̂c B̂′c) , then by Lemma A4, we have 1 λK(B̂c B̂′c) = O(λ1(Π′cΠc)). Now,we aim to bound ‖e′jV(V′V̂)(B̂−1c − (P ′cBc(V′V̂))−1)‖F. For convenience, set Tc = V′V̂, Sc = P ′cBcTc. We have
‖e′jV(V ′V̂)(B̂−1c − (P ′cBc(V′V̂))−1)‖F = ‖e′jVTcS −1 c (Sc − B̂c)B̂−1c ‖F
≤ ‖e′jVTcS −1 c (Sc − B̂c)‖F‖B̂−1c ‖F ≤ ‖e′jVTcS −1 c (Sc − B̂c)‖F
√ K
|λK(B̂c)|
= ‖e′jVTcS −1 c (Sc − B̂c)‖F
√ K√
λK(B̂c B̂′c) ≤ ‖e′jVTcS −1 c (Sc − B̂c)‖FO(
√ Kλ1(Π′cΠc))
= ‖e′jVTcT −1 c B ′ c(BcB ′ c) −1Pc(Sc − B̂c)‖FO( √ Kλ1(Π′cΠc))
= ‖e′jVB ′ c(BcB ′ c) −1Pc(Sc − B̂c)‖FO( √ Kλ1(Π′cΠc))
= ‖e′jZcPc(Sc − B̂c)‖FO( √ Kλ1(Π′cΠc)) By Zc=Πc ≤ max1≤k≤K‖e′k(Sc − B̂c)‖FO( √ Kλ1(Π′cΠc))
= max1≤k≤K‖e′k(B̂c −P ′ cBcV ′V̂)‖FO( √ Kλ1(Π′cΠc))
= max1≤k≤K‖e′k(B̂cV̂ ′ −P ′cBcV′)V̂‖FO( √ Kλ1(Π′cΠc))
≤ max1≤k≤K‖e′k(B̂cV̂ ′ −P ′cBcV′)‖FO( √ Kλ1(Π′cΠc))
= max1≤k≤K‖e′k(B̂2c −P ′ cB2c)‖FO( √ Kλ1(Π′cΠc)) (A2)
= O(vκ(Π′cΠc) √ Kλ1(Π′cΠc)). Remark A3. Equation (A2) supports our statement that building the theoretical framework of DiMSC benefits a lot by introducing the DiMSC-equivalence algorithm since ‖B̂2c −P ′cB2c‖2→∞ is obtained from DiMSC-equivalence (i.e., inputing V̂2 in the SP algorithm obtains ‖B̂2c −P ′cB2c‖2→∞). Then, we have
‖e′j(Ẑc − ZcPc)‖F ≤ O(v √ Kλ1(Π′cΠc)) + ‖e′jV(V ′V̂)(B̂−1c − (P ′cBc(V′V̂))−1)‖F
≤ O(v √ Kλ1(Π′cΠc)) + O(vκ(Π′cΠc) √ Kλ1(Π′cΠc)) = O(vκ(Π′cΠc) √ Kλ1(Π′cΠc)). Next, we consider row nodes. For 1 ≤ i ≤ nr, since Zr = Y∗ J∗, Ẑr = Ŷ∗ Ĵ∗, we have
‖e′i(Ẑr − ZrPr)‖F = ‖e′i(max(0, Ŷ∗ Ĵ∗)−Y∗ J∗Pr)‖F ≤ ‖e′i(Ŷ∗ Ĵ∗ −Y∗ J∗Pr)‖F = ‖e′i(Ŷ∗ −Y∗Pr) Ĵ∗ + e′iY∗Pr( Ĵ∗ −P ′r J∗Pr)‖F ≤ ‖e′i(Ŷ∗ −Y∗Pr)‖F‖ Ĵ∗‖F + ‖e′iY∗Pr‖F‖ Ĵ∗ −P ′r J∗Pr‖F = ‖e′i(Ŷ∗ −Y∗Pr)‖F‖ Ĵ∗‖F + ‖e′iY∗‖F‖ Ĵ∗ −P ′r J∗Pr‖F. Therefore, the bound of ‖e′i(Ẑr − ZrPr)‖F can be obtained as long as we bound ‖e′i(Ŷ∗ −Y∗Pr)‖F, ‖ Ĵ∗‖F, ‖e′iY∗‖F and ‖ Ĵ∗ −P ′r J∗Pr‖F. We bound the four terms as below: • We bound ‖e′i(Ŷ∗−Y∗Pr)‖F first. Similar as bounding ‖e′j(Ẑc− ZcPc)‖, we set U∗(Ir, :
) = BR, Û∗(Îr, :) = B̂R, U∗,2(Ir, :) = B2R, Û∗,2(Îr, :) = B̂2R for convenience. We bound ‖e′i(Ŷ∗ − Y∗Pr)‖F when the input is Û∗ in the SVM-cone algorithm. For 1 ≤ i ≤ nr, we have
‖e′i(Ŷ∗ −Y∗Pr)‖F = ‖e ′ i(ÛB̂ ′ R(B̂R B̂ ′ R) −1 −UB′R(BRB′R)−1Pr)‖F
= ‖e′i(Û −U(U ′Û))B̂′R(B̂R B̂ ′ R) −1 + e′i(U(U ′Û)B̂′R(B̂R B̂ ′ R) −1
−U(U′Û)(P ′r(BRB′R)(B′R)−1(U′Û))−1)‖F ≤ ‖e′i(Û −U(U ′Û))B̂′R(B̂R B̂ ′ R) −1‖F + ‖e′iU(U ′Û)(B̂′R(B̂R B̂ ′ R) −1
− (P ′r(BRB′R)(B′R)−1(U′Û))−1)‖F ≤ ‖e′i(Û −U(U ′Û))‖F‖B̂−1R ‖F + ‖e ′ iU(U ′Û)(B̂′R(B̂R B̂ ′ R) −1 − (P ′r(BRB′R)(B′R)−1(U′Û))−1)‖F ≤ √
K‖e′i(Û −U(U ′Û))‖F/ √ λK(B̂R B̂′R) + ‖e ′ iU(U ′Û)(B̂−1R − (P ′ r BR(U
′Û))−1)‖F (i) = √
K‖e′i(ÛÛ ′ −UU′)Û‖FO(
θr,max √
κ(Π′rΠr) θr,min ) + ‖e′iU(U ′Û)(B̂−1R − (P ′ r BR(U ′Û))−1)‖F
≤ √
K‖e′i(ÛÛ ′ −UU′)‖FO(
θr,max √
κ(Π′rΠr) θr,min ) + ‖e′iU(U ′Û)(B̂−1R − (P ′ r BR(U ′Û))−1)‖F
≤ √ KvO( θr,max
√ κ(Π′rΠr)
θr,min ) + ‖e′iU(U ′Û)(B̂−1R − (P ′ r BR(U ′Û))−1)‖F
= O(v θr,max
√ Kκ(Π′rΠr)
θr,min ) + ‖e′iU(U ′Û)(B̂−1R − (P ′ r BR(U ′Û))−1)‖F,
where we have used similar idea in the proof of Lemma VII.3 in [27] such that we apply O( 1
λK(BRB′R) ) to estimate 1 λK(B̂R B̂′R) , hence (i) holds by Lemma A4. Now, we aim to bound ‖e′iU(U′Û)(B̂ −1 R − (P ′rBR(U′Û))−1)‖F. For convenience, set Tr = U′Û, Sr = P ′rBRTr. We have
‖e′iU(U′Û)(B̂−1R − (P ′ rBR(U ′Û))−1)‖F = ‖e′iUTrS−1r (Sr − B̂R)B̂−1R ‖F
≤ ‖e′iUTrS−1r (Sr − B̂R)‖F‖B̂−1R ‖F ≤ ‖e ′ iUTrS −1 r (Sr − B̂R)‖F
√ K
|λK(B̂R)|
= ‖e′iUTrS−1r (Sr − B̂R)‖F √ K√ λK(B̂R B̂′R) ≤ ‖e′iUTrS−1r (Sr − B̂R)‖FO( θr,max
√ Kκ(Π′rΠr)
θr,min )
= ‖e′iUTrT−1r B′R(BRB′R)−1Pr(Sr − B̂R)‖FO( θr,max
√ Kκ(Π′rΠr)
θr,min )
= ‖e′iUB′R(BRB′R)−1Pr(Sr − B̂R)‖FO( θr,max
√ Kκ(Π′rΠr)
θr,min )
= ‖e′iY∗Pr(Sr − B̂R)‖FO( θr,max
√ Kλ1(Π′rΠr)
θr,min ) ≤ ‖e′iY∗‖F‖Sr − B̂R‖FO(
θr,max √
Kλ1(Π′rΠr) θr,min )
By Equation(A4) ≤
θ2r,max √
Kλ1(Π′rΠr) θ2r,minλK(Π ′ rΠr) max1≤k≤K‖e′k(Sr − B̂R)‖FO( θr,maxK
√ κ(Π′rΠr)
θr,min )
= max1≤k≤K‖e′k(B̂R −P ′ rBRU ′Û)‖FO( θ3r,maxK1.5κ(Π′rΠr) θ3r,min √ λK(Π′rΠr) )
= max1≤k≤K‖e′k(B̂RÛ ′ −P ′rBRU′)Û‖FO(
θ3r,maxK1.5κ(Π′rΠr) θ3r,min √ λK(Π′rΠr) )
≤ max1≤k≤K‖e′k(B̂RÛ ′ −P ′rBRU′)‖FO(
θ3r,maxK1.5κ(Π′rΠr) θ3r,min √ λK(Π′rΠr) )
= max1≤k≤K‖e′k(B̂2R −P ′ rB2R)‖FO(
θ3r,maxK1.5κ(Π′rΠr) θ3r,min √ λK(Π′rΠr) ) (A3)
By Lemma A8 = O( K4.5θ14r,maxvκ4.5(Π′rΠr)λ1(Π′rΠr) θ14r,minπr,min ). Remark A4. Similar as Equation (A2), Equation (A3) supports our statement that building the theoretical framework of DiMSC benefits a lot by introducing the DiMSC-equivalence algorithm since ‖B̂2R −P ′rB2R‖2→∞ is obtained from DiMSC-equivalence (i.e., inputing Û∗,2 in the SVM-cone algorithm obtains ‖B̂2R −P ′rB2R‖2→∞). Then, we have
‖e′i(Ŷ∗ −Y∗Pr)‖F ≤ O(v θr,max
√ Kκ(Π′rΠr)
θr,min ) + ‖e′iU(U′Û)(B̂−1R − (P ′ rBRU ′Û))−1)‖F
≤ O(v θr,max √
Kκ(Π′rΠr) θr,min ) + O( K4.5θ14r,maxvκ4.5(Π′rΠr)λ1(Π′rΠr) θ14r,minπr,min )
= O( K4.5θ14r,maxvκ4.5(Π′rΠr)λ1(Π′rΠr)
θ14r,minπr,min ). • for ‖e′iY∗‖F, since Y∗ = UU−1∗ (Ir, :), by Lemmas (A3) and (A4), we have
‖e′iY∗‖F ≤ ‖U(i, :)‖F‖U −1 ∗ (Ir, :)‖F ≤
√ K‖U(i, :)‖F√
λK(U∗(Ir, :)U′∗(Ir, :)) ≤
θ2r,max √
Kλ1(Π′rΠr) θ2r,minλK(Π ′ rΠr) . (A4)
• for ‖ Ĵ∗‖F, recall that Ĵ∗ = diag(Û∗( Îr, :)Λ̂V̂′(Îc, :)), we have
‖ Ĵ∗‖ = max1≤k≤K Ĵ∗(k, k) = max1≤k≤Ke′kÛ∗( Îr, :)Λ̂V̂ ′(Îc, :)ek
= max1≤k≤K‖e′kÛ∗( Îr, :)Λ̂V̂ ′(Îc, :)ek‖ ≤ max1≤k≤K‖e′kÛ∗( Îr, :)‖‖Λ̂‖‖V̂ ′(Îc, :)ek‖ ≤ max1≤k≤K‖e′kÛ∗( Îr, :)‖F‖Λ̂‖‖V̂
′(Îc, :)ek‖ = max1≤k≤K‖A‖‖V̂′(Îc, :)ek‖ = max1≤k≤K‖A‖‖(e′kV̂(Îc, :))
′‖ = max1≤k≤K‖A‖‖e′kV̂(Îc, :)‖ ≤ max1≤k≤K‖A‖‖e′kV̂(Îc, :)‖F ≤ ‖A‖‖V̂‖2→∞ = ‖A‖‖V̂sgn(HV̂)−V + V‖2→∞ ≤ ‖A‖(‖V̂sgn(HV̂)−V‖2→∞ + ‖V‖2→∞). By Lemmas (A6) and (A5), ‖A‖ = ‖A−Ω + Ω‖ ≤ ‖A−Ω‖+ σ1(Ω) ≤ ‖A−Ω‖+ θr,maxσ1(P)σ1(Πr)σ1(Πc) = O(θr,maxσ1(Πr)σ1(Πc)). By Lemma (A5) and Equation (A1),
‖V̂sgn(HV̂)−V‖2→∞ ≤ C √ θr,maxKlog(nr+nc) θr,minσK(P)σK(Πr)σK(Πc) . By Lemma A3, ‖V‖2→∞ ≤ √ 1 λK(Π′cΠc) ,
which gives ‖V̂sgn(HV̂)− V‖2→∞ + ‖V‖2→∞ = O( √ 1 λK(Π′cΠc) ) (this can be seen as
simply using ‖V‖2→∞ to estimate ‖V̂‖2→∞ since √
1 λK(Π′cΠc) is the same order as
√ θr,maxKlog(nr+nc)
θr,minσK(P)σK(Πr)σK(Πc) ). Then, we have ‖ Ĵ∗‖ = O(θr,maxσ1(Πr)κ(Πc)), which gives that
‖ Ĵ∗‖F = O(θr,max √
Kσ1(Πr)κ(Πc)). • for ‖ Ĵ∗ − P ′r J∗Pr‖F, since J∗ = NU(Ir, Ir)Θr(Ir, Ir), we have ‖J∗‖ ≤ NU,maxθr,max ≤
θ2r,max √
Kλ1(Π′rΠr) θr,min , which gives that ‖J∗‖F ≤ θ2r,maxKσ1(Πr) θr,min . Thus, we have ‖ Ĵ∗−P ′r J∗Pr‖F =
O( θ 2 r,maxKσ1(Πr)
θr,min ). Combining the above results, we have
‖e′i(Ẑr − ZrPr)‖F ≤ ‖e′i(Ŷ∗ −Y∗Pr)‖F‖ Ĵ∗‖F + ‖e′iY∗‖F‖ Ĵ∗ −P ′r J∗Pr‖F
= O( K4.5θ14r,maxvκ4.5(Π′rΠr)λ1(Π′rΠr)
θ14r,minπr,min )O(θr,max
√ Kσ1(Πr)κ(Πc))
+ θ2r,max
√ Kλ1(Π′rΠr)
θ2r,minλK(ΠrΠr) O( θ2r,maxKσ1(Πr) θr,min ) = O( K5θ15r,maxvκ4.5(Π′rΠr)κ(Πc)λ1.51 (Π ′ rΠr) θ14r,minπr,min ). Appendix E.1. Proof of Theorem 2
Proof. We bound ‖e′j(Π̂c −ΠcPc)‖1 first. Recall that Zc = Πc, Πc(j, :) = Zc(j,:) ‖Zc(j,:)‖1 , Π̂c(i, : ) = Ẑc(j,:)‖Ẑc(j,:)‖1 , for 1 ≤ j ≤ nc, since
‖e′j(Π̂c −ΠcPc)‖1 = ‖ e′jẐc
‖e′jẐc‖1 − e′jZcPc ‖e′jZcPc‖1 ‖1 = ‖ e′jẐc‖e′jZc‖1 − e′jZcPc‖e′jẐc‖1 ‖e′jẐc‖1‖e′jZc‖1 ‖1
= ‖ e′jẐc‖e′jZc‖1 − e′jẐc‖e′jẐc‖1 + e′jẐc‖e′jẐc‖1 − e′jZcP‖e′jẐc‖1
‖e′jẐc‖1‖e′jZc‖1 ‖1
≤ ‖e′jẐc‖e′jZc‖1 − e′jẐc‖e′jẐc‖1‖1 + ‖e′jẐc‖e′jẐc‖1 − e′jZcPc‖e′jẐc‖1‖1
‖e′jẐc‖1‖e′jZc‖1
= |‖e′jZc‖1 − ‖e′jẐc‖1|+ ‖e′jẐc − e′jZcPc‖1
‖e′jZc‖1 ≤ 2‖e′j(Ẑc − ZcPc)‖1 ‖e′jZc‖1
= 2‖e′j(Ẑc − ZcPc)‖1
‖e′jΠc‖1 = 2‖e′j(Ẑc − ZcPc)‖1 ≤ 2
√ K‖e′j(Ẑc − ZcPc)‖F,
by Lemma A9, we have
‖e′j(Π̂c −ΠcPc)‖1 = O( √ K‖e′j(Ẑc − ZcPc)‖F) = O(vKκ(Π′cΠc) √ λ1(Π′cΠc)). For row nodes 1 ≤ i ≤ nr, recall that Zr = Y∗ J∗ ≡ N−1U NMΠr, Ẑr = Ŷ∗ Ĵ∗, Πr(i, :) = Zr(i,:) ‖Zr(i,:)‖1 and Π̂r(i, :) = Ẑr(i,:) ‖Ẑr(i,:)‖1 , where NM and M are defined in the proof of Lemma 1 such that U = Θr M ≡ ΘrΠrBr and NM(i, i) = 1‖M(i,:)‖F , similar as the proof for column nodes, we have
‖e′i(Π̂r −ΠrPr)‖1 ≤ 2‖e′i(Ẑr − ZrPr)‖1
‖e′iZr‖1 ≤
2 √
K‖e′i(Ẑr − ZrPr)‖F ‖e′iZr‖1 . Now, we provide a lower bound of ‖e′iZr‖1 as below
‖e′iZr‖1 = ‖e′i N−1U NMΠr‖1 = ‖N −1 U (i, i)e ′ i NMΠr‖1 = N−1U (i, i)‖NM(i, i)e ′ iΠr‖1 = NM(i, i) NU(i, i)
= ‖U(i, :)‖F NM(i, i) = ‖U(i, :)‖F 1
‖M(i, :)‖F = ‖U(i, :)‖F 1 ‖e′i M‖F
= ‖U(i, :)‖F 1
‖e′iΘ −1 r U‖F
= ‖U(i, :)‖F 1
‖Θ−1r (i, i)e′iU‖F = θr(i) ≥ θr,min. Therefore, by Lemma A9, we have
‖e′i(Π̂r −ΠrPr)‖1 ≤ 2 √
K‖e′i(Ẑr − ZrPr)‖F ‖e′iZr‖1
≤ 2 √
K‖e′i(Ẑr − ZrPr)‖F θr,min
= O( K5.5θ15r,maxvκ4.5(Π′rΠr)κ(Πc)λ1.51 (Π ′ rΠr)
θ15r,minπr,min ). Appendix E.2. Proof of Corollary 1
Proof. Under conditions of Corollary 1, we have
‖e′i(Π̂r −ΠrPr)‖1 = O( K5.5θ15r,maxvκ4.5(Π′rΠr)κ(Πc)λ1.51 (Π ′ rΠr)
θ15r,minπr,min ) = O(
θ15r,maxv √
nr θ15r,min ),
‖e′j(Π̂c −ΠcPc)‖1 = O(vKκ(Π′cΠc) √ λ1(Π′cΠc)) = O(v √ nc). Under conditions of Corollary 1, Lemma A7 gives v = O( √
Pmaxθr,maxlog(nr+nc) θr,minσK(P) √ nrnc
), which gives that
‖e′i(Π̂r −ΠrPr)‖1 = O( θ15r,maxv
√ nr
θ15r,min ) = O(
θ15.5r,max √
Pmaxlog(nr + nc) θ16r,minσK(P) √ nc ),
‖e′j(Π̂c −ΠcPc)‖1 = O(v √ nc) = O( √
Pmaxθr,maxlog(nr + nc) θr,minσK(P) √ nr ). By basic algebra, this corollary follows.