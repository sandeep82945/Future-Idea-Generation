Currently in the petroleum industry, operators often flare the produced gas instead of commodifying it. The flaring magnitudes are large in some states, which constitute problems with energy waste and CO2 emissions. In North Dakota, operators are required to estimate and report the volume flared. The questions are, how good is the quality of this reporting, and what insights can be drawn from it? Apart from the company-reported statistics, which are available from the North Dakota Industrial Commission (NDIC), flared volumes can be estimated via satellite remote sensing, serving as an unbiased benchmark. Since interpretation of the Landsat 8 imagery is hindered by artifacts due to glow, the estimated volumes based on the Visible Infrared Imaging Radiometer Suite (VIIRS) are used. Reverse geocoding is performed for comparing and contrasting the NDIC and VIIRS data at different levels, such as county and oilfield. With all the data gathered and preprocessed, Bayesian learning implemented by Markov chain Monte Carlo methods is performed to address three problems: county level model development, flaring time series analytics, and distribution estimation. First, there is heterogeneity among the different counties, in the associations between the NDIC and VIIRS volumes. In light of such, models are developed for each county by exploiting hierarchical models. Second, the flaring time series, albeit noisy, contains information regarding trends and patterns, which provide some insights into operator approaches. Gaussian processes are found to be effective in many different pattern recognition scenarios. Third, distributional insights are obtained through unsupervised learning. The negative binomial and Gaussian mixture models are found to effectively describe the oilfield flare count and flared volume distributions, respectively. Finally, a nearest-neighbor-based approach for operator level monitoring and analytics is introduced. acknowledgments In the very first place, I want to express my deepest appreciation to my advisor, Dr. Jennifer L. Miskimins. Dr. Miskimins has been my MS/PhD advisor and mentor since 2011. Since I returned to Mines to start my PhD in 2017, Dr. Miskimins has been providing me with the best guidance, the greatest support, and the most opportunities that I could imagine. During the first semester, I worked as a lab assistant in the High Bay; in a later semester, I worked as a teaching assistant in her well stimulation course; ever since I started to become interested in machine learning, she has provided me with a huge number of opportunities to connect with different groups of people, for brainstorming and pursuing my research interest. To a certain extent, I feel like I finally become a “qualified” FAST student member, thanks to all of these precious experience. What I have achieved, including this dissertation, would have never been possible without the guidance and support from Dr. Miskimins. Her world-class technical expertise, attitudes toward work/life, and art of managing different teams at various levels are what I hope I can learn from in my career and personal life. I am deeply grateful to my dissertation committee members: Dr. Soutir Bandyopadhyay, Dr. Alfred W. Eustes III, Dr. Yilin Fan, and Prof. Jim Crompton. My competency in my research field, as well as the shape of this dissertation are built with the help of those fruitful discussions and insightful comments from them. I am indebted to my mentors, colleagues, and friends from the Payne Institute for Public Policy. Especially, I want to thank Dr. Mikhail N. Zhizhin, Dr. Christopher D. Elvidge, and Dr. Morgan D. Bazilian. It is such an eye-opening experience for me to work with these world-class experts in remote sensing and satellite imagery. I would particularly like to thank Dr. Zhizhin for his help, insights, and time. I am really grateful to Dr. Bandyopadhyay and Dr. Luis Tenorio from the AMS Department, for their fantastic teaching, knowing me personally and motivating me to work hard. Looking
xix
back at what I have learned in machine learning which makes this dissertation possible, taking their classes are definitely the most important resources for myself (excuse me for not being a probabilist at this moment). By taking their statistical methods classes, I started to appreciate what really is machine learning, and falling in love with mathematics, more specifically, probability theory and statistical modeling. The TA experience at Mines makes me a better PhD student. What I have learned, technical or non-technical, made their way into this dissertation. I want to thank Prof. Crompton, Dr. Eustes, Dr. Mark G. Miller, Dr. Linda A. Battalora, and Dr. Miskimins for providing me with those valuable TA opportunities. I am grateful to all of my students for their support and feedback. I want to thank Dr. Yu-Shu Wu, Dr. Xiaolong Yin, and Dr. Yilin Fan for their care, support, and encouragement throughout my PhD study. I would like to thank Denise Winn-Bower, Rachel McDonald, and Joe Chen for their help. I really appreciate the feedback from the FAST member companies’ representatives. A lot of the discussions and the reflections following those were incorporated into this dissertation. Especially, I want to thank Ty Woodworth for his time and help, in the process of collecting plunger lift data for me. I got very warm welcomes every time I visited their Windsor office in Northern Colorado. Ty kindly introduced me to the team he led, and I got the great opportunities to ask questions and discuss with many field experts in different areas. Those discussions helped me tremendously. Special thanks go to the open source community. In the process of conducting this research and typesetting this dissertation, I benefited a lot from the ecosystems around Linux/GNU, TEX/LATEX, and Python. Especially, I want to thank the people behind PyMC3, a probabilistic programming language that this dissertation is heavily dependent upon. Last but not least, I would like to thank my family and friends. Thank you to my beloved wife Xiaodan, for all her love, support, and delicious dishes. I also want to thank my parents and parents-in-law for their support, encouragement, and understanding. xx
I dedicate this work to my mother, Dr. Lingying Ni, and my father, Mr. Honggang Lu. 谁言寸草心，报得三春晖。
xxi
CHAPTER 1
INTRODUCTION
Currently in the petroleum industry, for wells which produce both crude oil and natural gas, operators often choose to flare the produced gas instead of commodifying it. The rationales behind such decisions are multifold. Variations in natural gas price can be an important factor, especially when the processing and transportation cost is higher than the value of gas (Srivastava et al. 2019). The amount of gas being flared each year on a national level is huge, and an increasing trend can be observed for the top flaring countries (Figure 1.1). Source: NOAA, Colorado School of Mines, GGFR
The new ranking – top 30 flaring countries (2014 – 2018)
Ranked by 2018 flare volume Million m3 gas/year flared
P ub
lic D
is cl
os ur
e A
ut ho
riz ed
P ub
lic D
is cl
os ur
e A
ut ho
riz ed
P ub
lic D
is cl
os ur
e A
ut ho
riz ed
P ub
lic D
is
cl
os ur
e A
ut
ho
riz ed
Figure 1.1: Top 30 countries ranked by flared gas volume in 2018. United States ranks No. 4 and has a large increase from 2017 to 2018 (World Bank 2019). Due to the boom of unconventional resources (e.g., shale gas reservoirs) development in the recent decade, the United States has been among the top flaring countries in terms
1
of total volume flared. This is backed by the data from the U.S. Energy Information Administration (EIA) (2019) showing North Dakota, which is underlain by the Bakken Formation, and Texas, which houses the Permian Basin and the Eagle Ford Shale, are the top two flaring states since 2013. The two states’ annual flaring volume time series are shown in Figure 1.2. Some flaring sites can be clearly identified from Google Earth’s imagery (Figure 1.3). Natural gas flaring constitutes a problem of energy waste and CO2 emissions. In recent years, various organizations and government agencies have advocated reducing or eliminating routine gas flaring. For example, the North Dakota Industrial Commission (NDIC) introduced a gas flaring regulatory policy (Order 24665) in 2014, with goals of reducing flaring in different aspects (e.g., volume of gas flared). The World Bank launched the “Zero Routine Flaring by 2030” initiative in 2015. To monitor and benchmark flaring activity’s magnitude, a precise and accurate method to obtain quantitative flaring information is desirable. However, in certain situations, this information is only available through self-reporting mechanisms. 2
Inaccuracies might be introduced either intentionally or unintentionally. Satellite remote sensing is one unbiased approach for solving this problem. It can help detect active flares especially during nighttime and can be used to calibrate the estimation for flared gas volume. For this work, two different types of sensors are considered, including the Landsat 8 (L8)’s Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS), as well as the Visible Infrared Imaging Radiometer Suite (VIIRS) that is on the Suomi National Polar-orbiting Partnership (NPP) and NOAA-20 satellites. In the remainder of this dissertation, they are referred to as L8 and VIIRS, respectively. An example of detecting flaring with VIIRS low light imaging data is shown in Figure 1.4. 1.1 research goal This research is undertaken to achieve the following goals:
• Evaluate the methodology for estimating flared gas volume leveraging satellite imagery;
and,
3
• Find insights into operators’ gas flaring behavior. 1.2 dissertation objectives To achieve the goals outlined in Section 1.1, more specific objectives are listed below:
1. Compare and contrast the flaring data from VIIRS and NDIC. • Compare the VIIRS flared volumes to the NDIC, using the NDIC as a benchmark. 2. Evaluate the effectiveness of using Landsat 8 nighttime images to improve flare detection
and volume estimation. • Determine the detection limits of Landsat 8 and compare it with VIIRS’ capabili-
ties. Investigate operator approaches for gas flaring. 4
• Determine the correlation between gas price / oil price / oil production and flared
gas volume. • Evaluate if the North Dakota regulatory policy (Order 24665) achieved its goals. • Develop a model that can predict flared gas volume at a state level. Find any hidden structure/clusters from all the producing entities. 1.3 outline and contributions The main contribution of this dissertation is demonstrating that Bayesian learning
implemented by Markov chain Monte Carlo methods is very effective in flaring data analytics. A series of parametric and nonparametric machine learning models are developed for various analytics goals and granularities, providing direct guidance for future modeling endeavors. To demonstrate the effectiveness and robustness, they are all tested with real data. The superiority of this approach is based on the fact that the inference stage is entirely probabilistic, in that the parametric uncertainties arising from probable models as well as the stochastic uncertainties arising from noisy observations are all properly characterized and quantified. It makes the extracted insights robust and interpretable for decision- and policy-making by, for example, a state government. In Chapter 2, a literature review is given for the state of the art in satellite imagery
processing, Bayesian inference, Markov chain Monte Carlo methods, and machine learning. In Chapter 3, the data gathering processes are discussed. Results from some exploratory
data analysis are presented. In Chapter 4, county level models are built to study the correlations between VIIRS and
NDIC, and to explore the heterogeneity among the counties in North Dakota. In Chapter 5, flaring time series analytics is presented for the purposes of revealing trends
and patterns at different levels. In Chapter 6, unsupervised learning is applied on flaring data to characterize the latent
structures. 5
In Chapter 7, a method of operator level monitoring and analytics is introduced, and
some discussions about applying Bayesian learning are given. In Chapter 8, major conclusions drawn are presented. Recommendations based on this
work are given. A number of future research areas are outlined. 6
CHAPTER 2
LITERATURE REVIEW
In the 1990s, the World Bank started gathering nighttime satellite images, from which big cities and oilfields were both bright and needed to be sorted using extra information. The situation changed in 2012 when infrared data became available from VIIRS (Rassenfoss and Zborowski 2018). One of the data products, VIIRS Nightfire (VNF) specializes in natural gas flaring observation and is even able to distinguish between biomass burning and gas flaring (Elvidge et al. 2017). VNF’s development was based upon VIIRS imagery. To improve the performance of flare detection and gas volume estimation, other sources of information, such as L8 imagery, can be leveraged. Table 2.1 presents a comparison of L8 and VIIRS spatial and temporal resolutions (NASA 2019; Wikipedia 2019). Figure 2.1 illustrates L8’s spatial resolution. In addition, L8 collects data in 11 different spectral bands of the electromagnetic spectrum. VIIRS has 22 bands. Both L8 and VIIRS are in near-polar orbits of the earth and can reveal rich features in the landscape. Therefore, L8 should be able to identify smaller gas flares compared to VIIRS’ capability, although its longer satellite revisit time poses a challenge to identify less persistent flares. More details on the processing steps of VNF are discussed in Section 2.1, the essence of which will be applied to L8. Nowadays, one resource which is more than abundant is data. For a certain discipline or research field, new sources of data bring in new dimensions of information, such as satellite images are now playing a role in gas flaring analytics. How to analyze data effectively and intelligently to gain insights is a central problem. In the petroleum engineering domain, for example, data driven approaches have been proposed to analyze stimulation treatments (Kazakov and Miskimins 2011) and predict screenouts (Yu et al. 2020). Machine learning is a powerful tool for this purpose. It is at the core of artificial intelligence and data science, and lies at the intersection of statistics and computer science (Jordan and Mitchell 2015). Frameworks in computational learning theory, such as the PAC learning proposed by Valiant (1984), help provide a theoretical backbone for some learning algorithms. One subset of machine learning, deep learning (DL), had its debut in 2006 when Hinton and Salakhutdinov introduced Deep Belief Networks (DBN), but it did not gain wide acceptance until 2012 when AlexNet showed the breakthrough performance on classification accuracy in the ImageNet competition (Krizhevsky et al. 2012). AlexNet is a DL-based model (more
8
specifically a convolutional neural network) and achieved an error rate of 15.3 %, which is more than 10 % lower than the runner-up. DL dominated the competition thereafter, and DL-based models finally surpassed human performance on the classification data set in 2015 (He et al. 2015). Although neural network-based models have gained much success in recent years, it should be noted that no one type of model can always be the best candidate for all problems. This has been formally shown by Wolpert (1996), and is usually referred to as the “no free lunch” (NFL) theorem. More recently, Olson et al. (2017) empirically assessed 13 classification algorithms on 165 different problem sets, and the results aligned with the theorem: even the union of the top five best performing algorithms cannot dominate all of the problem sets. In the following sections, a detailed review is given for the aspects below, which serve as
the foundation and inspiration for this work:
1. Satellite image processing
2. Bayesian inference
3. Markov chain Monte Carlo
4. Machine learning
5. Analytics toolset 2.1 satellite image processing Satellite images are utilized to estimate flared gas volume. The fire detection algorithm based on Planck curve fitting and physical laws, known as VIIRS Nightfire (VNF) due to Elvidge et al. (2013), serves as a starting point for analyzing L8 images in this research. The method consists of several major steps:
1. Detection of hot pixels
During nighttime, the sensors mainly record instrument noise which approximately follows a Gaussian distribution, except for the few pixels that contain an infrared
9
emitter such as a gas flare. Therefore hot pixels can be identified by setting a cutoff on the tail of the distribution, e.g., those pixels with digital numbers exceeding the mean plus four standard deviations. 2. Noise filtering
Hot pixels that are detected in only one spectral band are treated as noise and filtered out. Atmospheric correction
Losses in radiance due to scattering and absorption effects can be corrected. MODTRAN ® 5 (Berk et al. 2006), parameterized with atmospheric water vapor and temperature profiles, is used to derive the correction coefficients for each spectral band. Planck curve fitting
Planck curves are modeled for gas flares, which appear as gray bodies because they are sub-pixel sources. Therefore the output of the fitting is an estimate of the temperature and an emission scaling factor (the emissivity term in the Planck function). The latter is used subsequently to estimate the source area. Calculation of source area
The source area S is calculated using
S = εA , (2.1)
where ε is the emission scaling factor and A is the size of the pixel footprint. 6. Calculation of radiant heat
The radiant heat is calculated using the Stefan–Boltzmann law:
RH = σT 4S , (2.2)
10
where RH is the radiant heat in MW, σ is the Stefan–Boltzmann constant, T is the temperature in K, and S is the source area in m2. Once RH is obtained, previous work by Elvidge et al. (2015) developed a calibration for estimating flared gas volume, utilizing nation-level flaring reporting provided by Cedigaz (2015) and state-level reporting from Texas and North Dakota. The developed calibration can then be applied to each individual flaring site worldwide for estimation of flared gas volume, etc. 2.2 bayesian inference Bayesian inference leverages conditional probability theory to establish a formal procedure for learning from data (Betancourt 2018). Bayesian models provide full joint probability distributions p(D,θ) over observable data D and unobservable model parameters θ. The essence of Bayesian analysis is to obtain the posterior distribution p(θ | D), which characterizes the conditional probability of parameters θ given some data D. It can be derived through Bayes’ theorem:
p(θ | D) = p(D | θ) p(θ) p(D) (2.3a)
= p(D | θ) p(θ)∫ p(D | θ′) p(θ′) dθ′ (2.3b) ∝ p(D | θ) p(θ) , (2.3c)
where p(D | θ) is the likelihood (also referred to as the observation model) which denotes how likely the data is given a certain set of parameters, and p(θ) is the prior which models the probability of the parameters before observing any data. The prior encodes domain expertise. Once some observations are given, it is updated into a posterior which quantifies how consistent the model configurations are with both the domain knowledge and the observed data (Betancourt 2018). After the posterior is obtained, most if not all inferential questions can then be answered with posterior expectation values of certain functions (Betancourt
11
2019):
Ep[g(θ)] =
∫ g(θ) p(θ | D) dθ , (2.4)
where g(θ) is the function encoding some inferential question (e.g., where in the model configuration space the posterior concentrates). Predictions can be made in the form of a posterior predictive distribution: p(y∗ | x∗,D) = ∫ p(y∗ | θ,x∗) p(θ | D) dθ , (2.5)
where y∗ is the predictions based on the training set D for a test input x∗. Essentially this is integrating the prediction p(y∗ | θ,x∗) over the posterior distribution of parameters (Rasmussen and Williams 2006). Note that by giving the final results in terms of a probability distribution, richer information and more reliable inferences are accessed compared to merely giving a point estimate through MLE or MAP (as some machine learning models do under the frequentist framework). This is achieved by incorporating into the inference process the uncertainty in the posterior parameter estimate. Other benefits include posterior predictive checks, which are conducted by checking for auto-consistency between generated data (y∗) and observed data (y). 2.3 markov chain monte carlo Many of the integration problems central to Bayesian statistics, including those in Equations 2.4 and 2.5, are analytically intractable. A class of sampling algorithms, known as Markov chain Monte Carlo (MCMC), can be applied to approximate these (Andrieu et al. 2003). Suppose for some function of interest f(x), the objective is to obtain its integral, with respect to a non-standard target distribution p(x) from which samples cannot be drawn directly:
I(f) = ∫ f(x) p(x) dx . (2.6)
By constructing Markov chains that have p(x) as the invariant distribution, MCMC samplers, while traversing the sample space X , are able to generate samples x(i) that mimic samples
12
drawn directly from the target distribution p(x). In other words, this mechanism makes it possible to draw a set of samples {x(i)}Ni=1 from p(x). Then, by the Monte Carlo principle, the integral I(f) can be approximated with a sum
IN(f):
IN(f) = 1
N N∑ i=1 f(x(i)) a.s.−−−−→ N−→∞ I(f) = ∫ f(x) p(x) dx . (2.7)
That is, the estimate IN(f) is unbiased and by the strong law of large numbers, it will converge almost surely (a.s.) to I(f). That’s why MCMC is a powerful tool in Bayesian analysis. In practice, the Metropolis-Hastings (MH) algorithm and Gibbs sampling have been popular MCMC methods (Andrieu et al. 2003), but only when the parameter space is not too high-dimensional (McElreath 2020). Due to limited computing resources, it is impossible to run Markov chains infinitely long. In other words, inference has to be made based on finitely many draws. One approach, which is effectively leveraged in this research, is to run multiple chains in parallel and monitor various statistics for diagnosing non-convergence. Besides the effective sample size per transition of the Markov chain, the Gelman-Rubin statistic (Gelman and Rubin 1992), denoted by R̂, is used in this dissertation. The R̂ statistic quantifies whether the ensemble of Markov chains initialized from diffuse points in parameter space finally converge to the same equilibrium phase (Betancourt 2017b). When R̂ is sufficiently close to 1 (for example R̂ < 1.05), convergence is declared to be achieved. As an example, Figure 2.2 presents how four chains are started in different corners but approach stationarity and convergence after a certain number of iterations. For many of the problems in practice, including the models in this dissertation, the parameter space is very high-dimensional and involves highly curving regions. The MetropolisHastings algorithm and Gibbs sampling are far from efficient in these situations. Hamiltonian Monte Carlo (HMC), originally proposed by Duane et al. (1987), really outshines the other algorithms at this point and is the main sampling strategy adopted in this dissertation. 13
Specifically, No-U-Turn Sampler (NUTS) introduced by Hoffman and Gelman (2014), which is an extension to HMC, is employed for sampling from posterior distributions. 2.4 machine learning Machine learning was defined by Mitchell (1997) as computers improving automatically through experience. It can also be viewed as a function estimation problem (Vapnik 2000), or as the process of extracting important patterns and trends from data (Hastie et al. 2009). In terms of tasks, common types of learning consist of supervised, unsupervised, semi-
supervised, and reinforcement (Burkov 2019). Let xi ∈ X ⊆ Rd represent input, and yi ∈ Y represent target, then the goals of the first two types are:
• Supervised learning aims to use the dataset, consisting of X = {xi}ni=1 and y = {yi}ni=1,
to produce a model that is able to predict an output (yj) given some new/unseen input (xj), i.e., learning the underlying mapping f : X → Y . • Unsupervised learning is used to find the hidden patterns in X; in this case there does
not exist any labels (y) or predefined targets. 14
Another variation of learning is online learning, in which case training data is fed to the algorithm continuously or one example at a time (Abu-Mostafa et al. 2012). In other words, streaming data is available that the algorithm has to process on the run. This is different from batch learning, where data is provided beforehand and “frozen” during the learning process. Online learning can be applied to the different tasks as discussed above (supervised and others). In terms of model characteristics, machine learning models can be categorized into parametric and nonparametric models. Parametric models are characterized by a fixed number of parameters, whereas nonparametric models have an infinite-dimensional parameter space. For example, in the latter case the parameter space can be the set of continuous functions in a regression setting (Orbanz and Teh 2010). In this dissertation, supervised and unsupervised learning are leveraged while exploiting both parametric and nonparametric models. From Bayesian’s perspective, machine learning is essentially computing the posterior (de Freitas 2013), which is then used for inference and prediction tasks. This is conducted exactly through Equation 2.3a. In practice, machine learning conducted under Bayesian’s framework follows a principled workflow (Figure 2.3), which is adapted for the modeling in this dissertation. 2.5 analytics toolset For the past five to ten years, prosperity in contributions and progress in the open source community has been witnessed. Ecosystems around Python, R, and Julia have been prototyped, tested, and deployed in production environments in various industries. Powerful probabilistic programming languages (PPL), for example Stan (Carpenter et al. 2017) and PyMC3 (Salvatier et al. 2016), have become the workhorse for Bayesian machine learning. The majority of this work is implemented in Python. Specifically, Bayesian learning is performed by leveraging PyMC3. Some analytic visualizations are produced employing ArviZ (Kumar et al. 2019). Geospatial operations are performed with the help of GeoPan-
15
das (Jordahl et al. 2020). Satellite imagery is processed and analyzed in MATLAB, with implementations mainly following Elvidge et al. (2013). 16
CHAPTER 3
DATA PREPROCESSING AND EXPLORATORY DATA ANALYSIS
In this chapter, an overview of the flaring data is given. Some other variables which might be correlated with the flaring statistics are also considered. Exploratory data analysis is performed for choosing the subset of the variables as the focus in this dissertation. A state level model is developed in the end which motivates the work in the next two chapters. 3.1 data gathering Four sources of data, L8 satellite images, VIIRS estimated flared volumes, NDIC monthly production reports, and county/oilfield shapefiles for North Dakota were gathered for the analysis used in this research. 3.1.1 landsat 8 images In total, 167 images (since 2013) were downloaded from Google Cloud using the criteria
below:
• From five Path/Row’s: 126/216, 126/217, 126/218, 127/216, and 127/217. According to the Worldwide Reference System (WRS), the satellite imagery of any portion of the world can be queried using Path and Row numbers. These five Path/Row’s cover the majority of the areas in North Dakota that have production and flaring activities. • Nighttime images. Only nocturnal Landsat 8 imagery are used for the purpose of flare detection. • Cloud cover less than 10 %. Images with low cloud cover percentages reveal more clearly land features including gas flares, and thus are ideal for validating the developed methodologies. 17
• GeoTIFF Data Product. Both the georeferencing information and the raw images of all the spectral bands are preserved through the GeoTIFF format, which are necessary for the analysis. 3.1.2 viirs estimated volumes The VIIRS flare inventory and estimated volume dataset obtained from Mikhail N. Zhizhin (personal communication) are used in this dissertation. This dataset includes monthly flare detection records in North America from March 2012 to December 2018 (both inclusive) with their associated:
• Timestamps giving the specific month
• Latitudes and longitudes in WGS 84 coordinates
• Flared volume estimations in bcm 3.1.3 ndic monthly production reports All the monthly production reports from May 2015 to April 2020 (both inclusive) which have flaring information have been downloaded from NDIC. There is one Excel spreadsheet per month; each row corresponds to a well (that was active in that month), and columns are for various types of information, including flared gas volume (estimated and reported by operator), oilfield, oil production, etc. A screenshot of the top ∼50 rows in one of the spreadsheets is displayed in Figure 3.1. 3.1.4 ndic shapefiles The shapefiles for the counties and oilfields in North Dakota are downloaded from the NDIC GIS Map Server. All the polygons are described in NAD 27 coordinates. The shapefiles are for reverse geocoding the satellite detection locations to readable addresses, specifically which county and oilfield is a flare located in. 18
F ig
u re
3. 1:
A sc
re en
sh ot
of th
e to
p ∼
50 ro
w s
in th
e O
ct ob
er 20
18 p ro
d u ct
io n
re p
or t.
E ac
h ro
w co
rr es
p on
d s
to a
w el
l. T
h er
e ar e in to ta l 17 ,1 35 ro w s in th is sp re ad sh ee t, w it h th e fi rs t ro w b ei n g th e h ea d er . 19 3.2 satellite image processing As discussed in Section 3.1.1, all the available L8 images have been downloaded. They are processed in batch, following the workflow as outlined in Section 2.1. To compare and contrast with VIIRS’ performance, specifically the nighttime combustion source detection limits, all the flares detected from all of the L8 images are gathered and used to generate the source area versus temperature scattergram shown in Figure 3.2. Although it is expected that L8 would pick up smaller flares than VIIRS (which is capable of detecting flares around the size of a whole cooktop area), the majority of the detections as indicated on the scattergram are too small for natural gas flaring. To verify if some hot pixels are clustered together and actually representing a single flare or flaring site, HDBSCAN (Campello et al. 2013) with an implementation due to McInnes et al. (2017) is executed on every L8 detection map to find out if large blobs of hot pixels are present. HDBSCAN is a density-based clustering algorithm which keeps all the advantages of the original DBSCAN (Ester et al. 1996), for example the capacity of finding clusters of arbitrary shapes. It also outperforms DBSCAN by being able to build clusters of varying density (Burkov 2019). Further, to get the most accurate results in this case, haversine metric is chosen to handle the great-circle distances between the hot pixels; leaf clustering is used instead of the default Excess of Mass method to produce more fine grained clusters. The clustering results are illustrated in Figure 3.3. To verify whether these clusters are really single flares or they are actually a large number of neighboring wells (in which case each hot pixel still represents an individual flare), they are tracked down by looking further into each detection map (KMZ file). It is found that some large blobs of hot pixels are clustered and indeed represent single (huge) flares. One of the examples is shown in Figure 3.4. This poses a challenge to situations where an accurate estimate of the flare count is needed. The reason for this processing artifact is that, for large flares, there is glow surrounding the flare that was treated as many individual combustion sources. There are potential approaches
20
21
to mitigate this to make the interpretation and estimation out of L8 more accurate. In this work, the flares detected from VIIRS and the gas volumes estimated out of those are the focus for analytics. 22 3.3 reverse geocoding By reverse geocoding, the county information of every VIIRS flare that is in North Dakota can be retrieved. For most of the flares, the oilfield information is also retrievable. Thereafter, the flaring statistics from VIIRS and NDIC can be compared and contrasted at different levels, for a certain point or period of time. Shapefiles as discussed in Section 3.1.4 are used. With the help of GeoPandas, the
procedures for extracting counties and oilfields are the same:
1. Read the VIIRS records into a geospatial data object, with their original coordinates in
WGS 84. 2. Read the shapefile into a geospatial data object, with its original coordinates in NAD
27. Transform all the geometries in the shapefile to WGS 84 coordinates. Perform a spatial join of the two data objects to get the county or oilfield information
for each flare, if a specific county/oilfield’s polygon and the flare intersect, i.e., having any boundary or interior point in common. 3.4 correlational analysis To study the correlations between oil/gas prices, flaring statistics, and production performance, various time series are extracted for May 2015 to December 2018 (both inclusive). The below list describes all the variables used with their associated labels:
VIIRS flared vol monthly flared gas volume from VIIRS
NDIC flared vol monthly flared gas volume from NDIC
WTI oil price WTI crude oil price given by EIA (2020b)
Henry Hub gas price Henry Hub natural gas price given by EIA (2020a)
23
NDIC oil prod monthly oil production from NDIC
NDIC gas prod monthly gas production from NDIC
VIIRS flare count monthly flare detections count from VIIRS
NDIC flaring well count monthly wells count which conduct flaring from NDIC
NDIC GOR ratio of the NDIC gas production to the NDIC oil production
First, the monthly observations are extracted from each time series, and Spearman’s ρ is employed to measure the statistical dependence between the variables. Spearman’s ρ is a rank correlation, which quantifies the correlation between the rankings of two variables. Compared to Pearson’s r, it assesses monotonic relationships which can be nonlinear and is more robust to outliers, therefore is used in this section. The pairwise correlations between the variables are presented in Figure 3.5. Since a correlation matrix is always symmetric with unit diagonals, only the lower triangular part without the diagonal is plotted to minimize the information redundancy. It can be observed that most pairs show positive correlations. Financial factors (i.e., the oil and gas prices) are not among any of the highly correlated pairs (e.g., above 0.80). Nevertheless, it is indicated that the NDIC and VIIRS reportings have a positive correlation, and oil production is positively correlated with flared gas volume. In this analysis, due to the nature of the procedure (i.e., extract the monthly data and then measure the rank correlations), all the information on the time scale is neglected. To explore the correlations in the context of time series, the first differences (i.e., lag-1 differences) are taken for each variable
y′t = yt − yt−1, (3.1)
and then pairwise Spearman’s ρ is evaluated and visualized in Figure 3.6. In this case, there aren’t many pairs of variables which are highly correlated, except the oil and gas production are shown to be monotonically related on the lag-1 differences, which is unsurprising. In the
24
remainder of this dissertation, the focus is put on flaring and production related statistics instead of the financial factors. 25 3.5 state level flaring model In this section, a regression model is built for the purpose of investigating the statistical relationships between the NDIC and VIIRS reportings. Data from both sources are visualized in Figure 3.7, which demonstrate a positive correlation. Assuming a Gaussian observation model for the NDIC reporting with the location parameter encoding VIIRS’ information, the model is specified through Expressions 3.2a–
26
3.2e:
α ∼ Half-Normal(0.2) (3.2a) β ∼ Gamma(2, 2) (3.2b) σ ∼ Half-Cauchy(0.1) (3.2c) µi = α + β × VIIRSi (3.2d)
NDICi ∼ N (µi, σ) (3.2e)
where α is the intercept and β is the slope, both of which are constrained to be non-negative based on the nature of flaring volume; σ is the standard deviation in the Gaussian likelihood function, which has to be non-negative as well; µi is the expected NDIC reporting of month i, while NDICi and VIIRSi are the observed data (i.e., reported volumes) from NDIC and VIIRS in month i, respectively. The notation used in defining this model communicates the data generating process unambiguously and is adopted throughout this dissertation. Priors and hyperpriors are on the top while the observation model is at the bottom. The prior distributions for this model and all the others in this dissertation are chosen following the principles below:
27
1. Prefer weakly informative priors, i.e., choose the priors based on the domain expertise
at hand before observing any data. They should be strong enough to reflect the domain expertise and be weak enough to “let the data speak”, i.e., let the likelihood dominate when there is a decent amount of data. For example, a prior of a gamma distribution with mean Eβ = 2/2 = 1 is placed on β, reflecting the assumption that the satellite interpretation workflow gives the same flared volume as the NDIC reporting, before one observes any data. 2. Prefer priors with soft constraints as opposed to hard constraints, i.e., follow Cromwell’s
rule. For example, α, β and σ all have prior distributions with support on R>0 or R≥0. Counterexamples include using a triangular distribution or a continuous uniform distribution as the prior for such quantities, for which the author does not recommend. Prefer maximum entropy distributions, i.e., make the most conservative assumptions
based on all the information at hand (obeying all the known constraints). For example, the Gaussian and the binomial distributions are maximum entropy distributions and used in this dissertation, the fact of which can be formally shown leveraging the definition of Kullback–Leibler (KL) divergence. Once the priors and likelihood are established, four Markov chains of Hamiltonian Monte Carlo are run in parallel to sample from the posterior. The parameter estimates are reported in Table 3.1, and the posterior distributions and trace plots are presented in Figure 3.8. The four chains are plotted separately with different colors. The x-axis of the trace plot shows the number of iterations. This layout is used consistently for the remainder of this dissertation. Utilizing the model and the trace, posterior predictive samples are generated to construct the intervals (Figure 3.9). Point estimates and point predictions are easy to obtain for a certain machine learning model, however it is the properly constructed intervals that will provide insights into the uncertainty for decision making. The author would like to emphasize the importance of quantifying uncertainties when using machine learning, no matter for inference, prediction, or building intermediate models for integration into physics-based models. This is unfortunately neglected or ignored in some of the applications/publications in the petroleum engineering domain. The importance of properly quantifying the uncertainties will also be stressed in the following chapters. Whenever only one model specification is needed for making point predictions, it can be
recovered by the parameter estimates from Table 3.1:
NDICi = 0.061 + 0.535× VIIRSi , (3.3)
29
where NDICi and VIIRSi are flared volumes in bcm of month i. The model also provides clear interpretations for the NDIC reporting regression mean, on the whole state level:
1. The intercept indicates on average there is 90 % probability that 0.04 bcm to 0.08 bcm
reported volume per month will not be captured by the current VIIRS processing workflow. The posterior mean is 0.061 bcm (≈ 2150 MMcf). 2. The slope indicates on average when satellite estimated volume increases by one unit,
under 90 % probability the NDIC reporting will increase by 0.48 unit to 0.59 unit. The posterior mean is 0.535 unit. This model, while serving as a decent calibration and estimation tool for NDIC reporting on the state level, makes the assumption that the heterogeneity within the state (e.g., among different counties) is negligible and all the monthly observations are conditionally independent
30
and identically distributed (i.i.d.). For the scenarios in which these assumptions do not hold, other types of models can be built and are discussed in Chapter 4 and Chapter 5. 31
CHAPTER 4
COUNTY LEVEL FLARING MODEL
“Multilevel regression deserves to be the default form of regression.”
— McElreath (2015) 4.1 learning the heterogeneity In this chapter, the author explores the heterogeneity in correlations between the statereported and satellite-detected flaring statistics, among different counties in North Dakota. The motivations are threefold:
1. Provide more granular insights than merely investigating the whole state’s flaring
statistics. 2. Compare and contrast different counties’ reporting consistencies with the baseline (i.e.,
the satellite detections). Develop a dedicated model for each county for calibration and prediction purposes. 4.2 hierarchical model A common problem in learning from data is modeling individuals or units of a population. For example, building models for different counties in a state, or for different well pads in an oilfield. Usually from domain expertise, it is expected that the units would demonstrate some differences, however they do not necessarily represent completely independent data generating processes. In other words, the units are different in some ways, while being similar in others. Unfortunately, the following two common modeling approaches are extreme and not ideal:
1. Complete pooling
32
• This ignores heterogeneity and assumes that the observations from all the units are
generated/described by the exact same process. One set of parameters is learned for the whole population. In this situation, the variance might be smaller, however the bias could be huge. 2. No pooling
• This lets each unit learn its own set of parameters from its own data. The
assumption is that the information from each unit tells one nothing about any other unit. In this situation, the bias might be smaller, however the variance could be huge. In practice, neither of these approaches will be able to generalize well for insight extraction or prediction tasks, due to the total generalization error being large. In fact, these two extremes can be compromised by explicitly modeling the entire population of units. That is, in order to investigate the correlations among the individual units, an explicit model is introduced for the population. In the learning phase, the individual posteriors are used to fit some population distribution, while the information of the population is then fed back to the individuals. What happens in this case is that the individuals with diffuse likelihood functions (e.g. with less data) are dragged more towards the population distribution, whereas the individuals which are well informed by their data will have their posteriors mostly unchanged. In this process, dynamic regularization is achieved, i.e., the total generalization error is much smaller by partially pooling the data and balancing between the bias and variance. In the context of county level model development, the question is now how might one model the population. To motivate the choice of a particular class of models, some characteristics of the counties have to be examined. In this work, the counties are considered to be exchangeable, i.e., the joint probability p(θ1,θ2, . . . ,θn) is invariant to permutation of the indices, where θi, i = 1, 2, . . . , n is the parameters for the i-th county. That is, for any permutation π,
p(θ1,θ2, . . . ,θn) = p(θπ1 ,θπ2 , . . . ,θπn) . (4.1)
33
Furthermore, the list of counties can grow, i.e., although one might only look at a few counties at this point, in the future new counties in terms of flaring activities might be considered. If a population being modeled is exchangeable, and the population can grow arbitrarily large, de Finetti’s theorem shows that the only distribution that respects exchangeability is a hierarchical distribution:
p(θ1,θ2, . . . ,θn) = ∫ [ n∏ i=1 p(θi | φ) ] p(φ) dφ , (4.2)
where φ is a population parameter (which can be generalized to multiple population parameters) and p(φ) is a population prior. It asserts an important fact that if exchangeable data is used for analytics, there must exist a population model (Jordan and Broderick 2010). This provides guidance for the development of the county level flaring models in this chapter. Equivalently, the individual and population parameters can be fitted jointly, achieving a
dynamic pooling of the data:
p(θ1,θ2, . . . ,θn, φ) = [ n∏ i=1 p(θi | φ) ] p(φ) , (4.3)
in which process not only the θ’s but also φ are learned. After adding the observations component (D = {(xj, yj) | j = 1, . . . ,m}) to it, the joint model becomes:
p ({ yj,xj,θcounty[j], ψj }m j=1 , φ ) = [ m∏ j=1 p(yj | xj,θcounty[j], ψj) p(θcounty[j] | φ) ] p(φ) , (4.4)
where θcounty[j] stands for the parameters for the j-th observation based on its county assignment, and ψ are some other parameters in the likelihood function that are not necessarily distributed according to a population model. Equation 4.4 characterizes a hierarchical model that fits nicely into the Bayesian framework and is exploited for building the models in this chapter. As a fundamental approach to model heterogeneity, hierarchical models have been depended upon routinely in various fields including ecological science (Bolker 2008), political science (Gelman and Hill 2006), and biological science (McElreath 2015). The author believes
34
that they should be widely accepted and utilized in the petroleum engineering domain as well, where the dataset is usually presented in hierarchies. For example, the shale gas wells in a given basin were completed by different oilfield service companies. The information can then be pooled among the service companies. A further discussion is given in Section 7.3. One caveat, though, is that de Finetti’s theorem is based on the assumption that the population (of units) is exchangeable and can grow arbitrarily large. Just like every other assumption in machine learning, it should not be taken for granted and does not always hold. In the context of county level flaring model development, one might argue that there are currently 53 counties in North Dakota and there might not be many new counties (as administrative divisions) in any finite amount of time. In that regard, the author agrees with the claim of Box et al. (2009) that, since assumptions “are never exactly true”, what shall be sought is the useful models as opposed to the correct ones. That is the goal for applying the hierarchical models in this chapter. It is worth noting that the terminologies are not consistent when referring to these types of models: some argue that hierarchical model and multilevel model are different names for the same modeling technique (Bolker 2008; McElreath 2015), while others tried to differentiate them (Carpenter 2019). In this dissertation, the model assumptions are communicated via the mathematical structures instead of the terminologies, by writing out the full model definitions whenever possible. 4.3 data description After performing the reverse geocoding as outlined in Section 3.3, there are twelve counties found to have reported flaring activities from both VIIRS and NDIC. For each county’s historical data from May 2015 to December 2018 (both inclusive), only the months that have reported volumes from both sources are extracted. A scatterplot for each of the 12 counties is presented in Figure 4.1, where the county abbreviations follow the convention from the NDIC monthly production reports. Table 4.1 lists the full county names associated with each abbreviation. 35
It can be seen that the flaring magnitudes in terms of the flared volumes are quite diverse for the different counties. To better visualize all of them, a zoomed-in view for each county is shown in Figure 4.2. It becomes clear that most of the counties except SLP and GV have more than ∼12 data points; however, only the four counties in the top row (i.e., MCK, DUN, WIL and MTL) have the largest amount of data and indicate stronger positive correlations between VIIRS and NDIC. For the purpose of building county level models and investigating the heterogeneity among the counties, the no pooling option discussed in the previous section will fail. Especially with counties SLP (which has 3 observations) and GV (which has 2 observations), if a linear
36
model such as Equation 3.2d is fitted, the learned slope parameters βcounty will have point estimates β̂slp ≈ 0 and β̂gv 0 with their associated samples. The interpretation of the slope parameter (which was discussed right after Equation 3.3) implies that such inferences are never possible. Some other counties, even with more data points (e.g., MCL), suffer from the noise levels in their observations. Using their own dataset will frustrate accurate inferences. Therefore, in order to build models robustly at a county level, the hierarchical model discussed in the previous section is exploited. 4.4 model specification Motivated by the discussions in Section 4.2, partial pooling is performed by explicitly modeling the entire population of counties. In this way, the counties such as MCL can leverage the information from other counties to learn their own parameters. Counties with
“strong data” (i.e., very informative data which makes the likelihood dominate the structure
of the posterior), such as those in the top row of Figure 4.2, indicate a positive correlation between VIIRS and NDIC. Therefore, a similar strategy as in Model 3.2 is adopted for the counties, i.e., one set of slope and intercept is learned for each county. 37
Since the slope and intercept are very interpretable, the meanings of which were discussed right after Equation 3.3, partial pooling is also enabled across parameter types (i.e., intercepts and slopes). In other words, knowing how much flared volume is missed from VIIRS (i.e., the information carried by the intercept) might improve learning how VIIRS and NDIC will covary (i.e., the information carried by the slope). Specifically, a population model with a multivariate normal density is used for the different counties’ parameters. The hierarchical model is specified through Expressions 4.5a–4.5j:
µα ∼ Half-Normal(0.1) (4.5a) µβ ∼ Gamma(2, 2) (4.5b)
38
σα ∼ Half-Normal(0.1) (4.5c) σβ ∼ Half-Normal(0.1) (4.5d) σ ∼ Half-Normal(0.05) (4.5e) R ∼ LKJcorr(2) (4.5f)
Σ = ( σα 0 0 σβ ) ·R · ( σα 0 0 σβ ) (4.5g)
[ αcounty βcounty ] ∼ MVNormal [µα µβ ] ,Σ  (4.5h) µj = αcounty[j] + βcounty[j] × VIIRSj (4.5i)
NDICj ∼ N (µj, σ) (4.5j)
where:
µα is the average intercept for all the counties; µβ is the average slope for all the counties; σα is the standard deviation among different counties’ intercepts; σβ is the standard deviation among different counties’ slopes; σ is the the standard deviation in NDIC reporting within the counties; R is the correlation matrix distributed according to an LKJ distribution. It is 2-by-2
in size and encodes the correlation between the intercepts and slopes;
Σ is the covariance matrix for the population model, which is constructed by multi-
plying the correlation matrix from both sides by a diagonal matrix of standard deviations;
αcounty and βcounty are the intercept and slope for each county, whose prior distributions
are defined by a two-dimensional Gaussian population model;
county[j] (in the subscript) denotes the county index, i.e., county[j] ∈ {k ∈ N0 |
k ≤ 11}, such that αcounty[j] and βcounty[j] are the intercept and slope for the j-th observation based on its county assignment;
VIIRSj is the VIIRS reported volume of the j-th observation;
39
µj denotes the underlying flared volume of the j-th observation; NDICj is the NDIC reported volume of the j-th observation. The LKJ distribution due to Lewandowski, Kurowicka, and Joe (2009) is a distribution over positive-definite symmetric matrices with unit diagonals, i.e., correlation matrices. In the model specification above, it directly influences the prior for the covariance matrix. Before it was introduced and when HMC was not widely applicable, the usual choices for modeling covariance matrices were Wishart or inverse-Wishart distributions, due to their nice conjugacy properties. However, LKJ is better suited for modern Bayesian computational settings (Betancourt 2015; Lambert 2018) and therefore employed in this work. LKJ has a single parameter η, which can be interpreted as the shape parameter of a symmetric beta distribution (Gelman et al. 2013). As η gets larger, the prior is more skeptical of large correlations in the matrix, i.e., providing regularizing effects. The probability density of LKJ with a few η values are displayed in Figure 4.3. In this work, LKJcorr(η = 2) is chosen to define a weakly informative and regularizing prior. Model 4.5, while being expressive in the data generating process, is a centered parameterization of the hierarchical structure (Papaspiliopoulos et al. 2007). In this parameterization,
40
the hierarchical parameters (such as βcounty) and the lower-level parameters in the prior (e.g., µβ and σβ) are tightly coupled, and they are highly correlated in the posterior. Since this model involves complex geometries and interactions in the posterior, HMC is leveraged for sampling. When there is not a lot of data (which is the case for the current NDIC and VIIRS reportings), this parameterization leads to very inefficient sampling and nonconvergences (Stan Development Team 2020). The noncentered parameterization is preferable in these cases and therefore employed for building the county level models. 4.5 model reparameterization Reparameterization of hierarchical models can be applied to any distribution in the location-scale family, for which the normal distribution is a good candidate. In the case of reparameterizing a multivariate normal prior, suppose the prior for θ is a multivariate normal with mean vector µ and covariance matrix Σ (such as Expression 4.5h), then a noncentered parameterization is given by:
θ̃ ∼ MVNormal(0n, In) (4.6a) ϕ = µ+ L · θ̃ (4.6b)
where θ̃ has the same dimensions as θ and all of its elements i.i.d. according to N (0, 1), L satisfies L · L> = Σ, and ϕ recovers the exact same prior distribution for θ. This reparameterization leads to more efficient sampling by reducing the dependence between µ, L, and θ̃. One choice for L is the Cholesky factor of Σ, which provides implementation convenience for the multivariate normal cases (Stan Development Team 2020) and is adopted in this work. The noncentered county level model is specified through Expressions 4.7a–4.7j, with the
reparameterized part (corresponding to Model 4.5) highlighted in blue:
µα ∼ Half-Normal(0.1) (4.7a) µβ ∼ Gamma(2, 2) (4.7b)
41
σα ∼ Half-Normal(0.1) (4.7c) σβ ∼ Half-Normal(0.1) (4.7d) σ ∼ Half-Normal(0.05) (4.7e)
L ∼ LKJCholeskyCov ( η = 2, [ σα σβ ]ᵀ) (4.7f)[
zα zβ
] ∼ MVNormal [0 0 ] , ( 1 0 0 1 ) (4.7g) [ αcounty βcounty ] = [ µα µβ ] + L · [ zα zβ ] (4.7h)
µj = αcounty[j] + βcounty[j] × VIIRSj (4.7i) NDICj ∼ N (µj, σ) (4.7j)
where:
L is the Cholesky factor of the covariance matrix which has LKJ distributed correla-
tions;
zα and zβ are the standardized intercept and slope for each county. The rest of the symbols have the same meaning as in Model 4.5. The noncentered model imposes the exact same probabilistic structure as in Model 4.5, and is implemented for making inference on each county’s parameters. 4.6 model fitting Four chains are sampled from the posterior distributions. The posterior distributions and trace plots for the slopes and intercepts are presented in Figure 4.4 and Figure 4.5, respectively. Well mixing and convergence have been achieved as shown by the trace plots. To better compare and contrast the different counties’ parameters, the forest plots of 90 % highest density intervals (HDI) for the slopes and intercepts are given in Figure 4.6 and Figure 4.7, respectively. In both figures, counties are ordered by the VIIRS reported volumes, and those with the least amount of estimated volumes (such as SLP and GV) are at the bottom. The thin lines present the 90 % HDI’s and the thicker line segments stand for
42
43
44
the interquartile ranges (IQR). The points represent the posterior means. In the case of the slopes (Figure 4.6), it can be seen the top four counties are quite diverse. MTL has the largest point estimate in the entire population (β̂mtl > 0.6) while
45
DUN has the smallest one (β̂dun < 0.5). Furthermore, the HDI’s for DUN and MTL rarely overlap, indicating that it is almost certain that MTL has a larger slope than DUN. The counties with fewer observations (remaining eight counties) have greater uncertainties in their parameter estimates, while all of their point estimates are pulled towards the partially-pooled mean which is between 0.5 and 0.6. When there is not enough data for some counties, the hierarchical model strives to reinforce information sharing among different counties, thus providing more sensible results and also quantifying the uncertainties in such processes. From domain expertise, these results make more physical sense than the no-pooling estimates discussed in Section 4.3 (i.e., β̂slp ≈ 0 and β̂gv 0). In the case of the intercepts (Figure 4.7), there is also heterogeneity among the counties. In particular, by plotting a dotted line labeling the zero intercept, some counties are found to likely have zero intercept (e.g., zero is covered by the IQR or HDI) while others have intercepts that are significantly different from zero. It might not be surprising to get close-to-zero intercepts and greater uncertainties for those counties with less data (such as SLP and GV), however it is interesting to obtain the HDI for MTL that covers zero. Recall that the intercept parameter can be interpreted as the NDIC reported volume which is not captured by VIIRS. This finding for MTL, along with the fact that MTL has the largest slope point estimate (where a larger slope denotes closer proximity to the satellite estimation), convinces the author that MTL used to have persistent and stronger gas flares. They kept VIIRS from missing the flaring events in general, and lead to the reported volumes from NDIC and VIIRS being closer to each other. On the contrary, DUN’s smaller slope and larger intercept characterize its flares as sporadic and weaker. One thing worth mentioning is that, with the current interpretation of the intercept, it does not make much physical sense to have negative intercepts. Although every county has positive point estimates for their intercepts, some counties’ HDI’s show coverage over the negative values. This is a limitation of choosing a 2D Gaussian population model for the intercepts and slopes. Since the 2D Gaussian is supported on R2, in the context of some counties having “weak data”, negative values make
46
an appearance in their HDI’s. The discussions above naturally lead to the question of whether the slopes and intercepts are correlated. It turns out that, by partially pooling the different types of parameters, a probable negative correlation between the slopes and intercepts is revealed (Figure 4.8). The correlation is learned from the heterogeneity in flare characteristics among the counties:
• Persistent flares yield smaller intercepts and larger slopes. • Sporadic flares yield larger intercepts and smaller slopes. In other words, intercepts and slopes covary in the entire population of counties. By pooling information across parameter types, what the model learns in the intercept can improve learning about slopes, and vice versa. With this “experience” or “knowledge”, the hierarchical model will be able to quickly update its expectation for any new counties’ parameters even with just a few observations in the beginning. It should be noted that there is also some probability mass for the positive correlation values, i.e., the negative correlation is not very strong. This could be due to that some counties do not have a lot of data at this time. The posterior will be updated as more data is brought in. Finally, the parameter estimates are reported in Table 4.2, from which the parametric model for each county can be recovered, and then deployed in calibration and prediction usage scenarios. 4.7 model extensibility Looking back at the hierarchical model and the reparameterization strategy from the
previous sections, there are four potential deployment scenarios that are worth discussing. They demonstrate the extensibility and flexibility of the chosen approach in the context of flaring data analytics:
1. New counties are present in terms of the reported flaring statistics from both VIIRS
and NDIC. 47
1.0 0.5 0.0 0.5 1.0 Correlation
0.0
0.2
0.4
0.6
0.8
1.0
D en
si ty
prior
posterior
HDI’s would become narrower and narrower as more and more data are available, and since the hierarchical model pools information among the counties, these counties will contribute to updating the population model’s and other counties’ parameters. Similar to Item 1 above, Model 4.7 does not need modifying and can be re-fitted with the new data. Sample sizes among counties become more unbalanced. In general, when there is a lot of data for each county, the centered parameterization (Model 4.5) is more efficient. When the sample size is not large, which is the case for the current VIIRS and NDIC reportings, the noncentered parameterization (Model 4.7) is better. However, the parameterization for hierarchical models is not a monolithic tactic. If the reported flaring data becomes very unbalanced across counties, e.g., some counties have a huge amount of data whereas others have very little data, then each county can be parameterized differently. More specifically,
• For the counties that have strong data such that their likelihood functions dominate,
centered parameterization can be applied through Expressions 4.5f–4.5h. • For the counties that have weak data such that their prior models dominate,
noncentered parameterization can be applied through Expressions 4.7f–4.7h. All in all, this is still one hierarchical model which defines the exact same probabilistic structure as Model 4.5 or Model 4.7, but avoids inefficiencies and non-convergences in the sampling from posteriors. Oilfield level heterogeneity needs to be examined. Under the assumptions that the oilfields in North Dakota are exchangeable and the population of oilfields (which conduct flaring) can grow, the hierarchical model developed in this chapter can be directly applied to investigate the heterogeneity in different oilfields’ parameters. Following the reverse geocoding as discussed in Section 3.3, there
50
are 258 oilfields that have both NDIC and VIIRS reportings for the same study period as in this chapter. Some oilfields have very few observations and can benefit from the hierarchical model through pooling information among the entire population of oilfields. Furthermore, due to the number of oilfields being relatively large, the population model could be learned with more ease (because more information is available for the population). In the case of the county level model developed in this chapter, since there are only 12 individuals (counties) in the population, some uncertainties about the population are inevitably present and reflected through the posteriors. The models developed in this chapter, while capturing the heterogeneity among the different counties in North Dakota, rely on the assumption that all the monthly observations within a certain county are conditionally i.i.d. For situations where the temporal structure has to be taken into consideration, other types of models can be built and are discussed in the next chapter. 51
CHAPTER 5
FLARING TIME SERIES ANALYTICS
“Were neural networks over-hyped, or have we underestimated the power of smoothing methods? I think both these propositions are true.”
— MacKay (2003) 5.1 learning the flaring pattern and behavior In this chapter, the author develops a generic framework for revealing flaring patterns
and behaviors. The main challenges are fourfold:
1. Observed data are noisy. • Companies estimate the flaring volumes and conduct self-reporting. Satellites
could miss some events. However, having knowledge about the underlying process is vital in lots of situations including when the state and local governments need to make key decisions based on the data. In the meantime, understanding the underlying process helps with anomaly detection by differentiating between true anomalies in reporting and ordinary noise or stochasticity. 2. A probabilistic approach is desirable to be adopted. • A set of most probable functions (characterizing the underlying process) are
preferable over one single best fit function. The observations of a certain entity are time series. • The temporal structure is intrinsic to the dataset and thus must be harnessed. The framework should be generic enough for automated insights extraction. 52
• There are more than 200 operators and 500 oilfields operating in North Dakota. Choosing a specific parametric form of model (e.g., ARIMA or LSTM) for each entity and then fitting the model to the data is not only time consuming, but also prevents easy integration into automation pipelines (for extracting insights for example). It is striking that the elegant properties of Gaussian process make it a natural choice to
tackle all of these challenges and is therefore employed in this chapter. 5.2 gaussian process A Gaussian process (GP) can be viewed as a distribution over infinite-dimensional Hilbert space of functions. It is formally defined as “a collection of random variables, any finite number of which have a joint Gaussian distribution” (Rasmussen and Williams 2006). Gaussian processes are extremely powerful nonparametric learning techniques, which provide a composite of flexibility and interpretability. They are well suited to problems which necessitate principled handling of uncertainty and interpretation, in the presence of noisy and dynamic datasets. Such scenarios include smoothing (Deisenroth et al. 2012) and time series modeling (Roberts et al. 2013). They are also well established in different fields under various names, for example kriging in geostatistics and Kalman filters both correspond to Gaussian processes (MacKay 1998). In this work, the motivation is to develop a generic framework for recognizing the underlying unknown processes f(x) which reflect flaring strategies and behaviors. Thus inference is conducted directly in the function space employing GP as a prior. A Gaussian process is completely specified by its mean function m(x) and covariance function k(x,x′) (Bandyopadhyay 2018), which are defined as:
m(x) = E[f(x)] , (5.1) k(x,x′) = E[(f(x)−m(x))(f(x′)−m(x′))] , (5.2)
53
and the function distributed as a Gaussian process is denoted by
f(x) ∼ GP ( m(x), k(x,x′) ) . (5.3) 5.2.1 mean function In this work, the mean functions are always chosen to be zero, since there is no prior knowledge on the mean of the latent processes. In the meantime, for GPs with a zero mean function, the mean of the posterior process is not confined to be zero (Rasmussen and Williams 2006). All the latent functions modeled with a GP prior in this dissertation follow
f(x) ∼ GP ( 0, k(x,x′) ) , (5.4)
where k is some covariance function. 5.2.2 covariance function Covariance function, also known as kernel, is the crucial ingredient in a GP, as it encodes one’s assumptions about how the function should behave by defining similarity. The fundamental assumption is that data points with inputs x which are close would have similar target values y. This assumption is usually very reasonable in areas including time series modeling, and it is theoretically backed by Tobler’s first law of geography. The covariance functions used in this dissertation include:
1. The Matérn class of covariance functions, which is given by:
kν(r) = 21−ν
Γ(ν)
( √
2ν r
`
)ν Kν ( √ 2ν r
`
) , (5.5)
where Γ(·) is the gamma function, Kν is a modified Bessel function of the second kind of order ν, r =‖x− x′‖, and ` is the lengthscale controlling the smoothness from one perspective: large ` characterizes functions which change slowly and can be reliably extrapolated further away. The Matérn covariance functions can be written as a product of an exponential and a polynomial of order p, when ν is half-integer: ν = p+1/2, p ∈ N0. The hyperparameter
54
ν controls the smoothness from another perspective: when ν = 1/2, the Matérn kernel becomes the exponential kernel (continuous but not differentiable); as ν → ∞, it becomes the exponentiated quadratic kernel (infinitely differentiable). Rasmussen and Williams (2006) argued that the most interesting cases for machine learning would be ν = 3/2 and ν = 5/2. For gas flaring time series, as operators might change flaring strategy at any given time due to policy changes, gas processing facility deployment, gas price fluctuation, etc., the latent process might not be as smooth as infinitely differentiable. Instead the Matérn kernel is harnessed which is capable of inducing non-smooth function realizations to handle those discontinuities. Specifically the Matérn kernel with ν = 5/2 is chosen for this dissertation with the input space X ⊆ R1:
kmatérn52(x, x ′; `) := ( 1 + √ 5(x− x′)2
` + 5(x− x′)2 3`2
) exp [ − √
5(x− x′)2 `
] , (5.6)
where x vary over the time domain. 2. The standard periodic kernel due to MacKay (1998):
kperiodic(x, x ′;T, `) := exp ( −sin 2(π|x− x′| 1 T )
2`2
) , (5.7)
where T denotes the period. This kernel is used for modeling seasonal behaviors. The white noise kernel, which is given by:
kWhiteNoise(x, x ′; δ) := δ2In, (5.8)
where δ2 is the variance of the noise. In this dissertation, the usage of the white noise kernel is for stabilizing the computation of the covariance matrix. Adding a small value of diagonal shift will try to guarantee the resulting covariance matrix is always positive semi-definite. 55
A nice property is that the sum and product of the established kernels are still valid
kernels. This fact is also exploited in the model building process in this work. 5.2.3 inference and model reparameterization In practice, one always works with a dataset of finite size. In such situations, a multivariate
normal prior distribution is placed on the vector of function values f ,
f ∼ MVNormal(mx, Kxx) , (5.9)
where the vector mx and the matrix Kxx are the mean function and covariance function evaluated over the inputs x. A key question which has significant impact on the inference is how to learn the hyperparameters from data. A natural (and popular) approach is to conduct maximum likelihood estimation, i.e., generating point estimates leveraging the data. However, as Betancourt (2017a) showed with experiment results, both regularized and unregularized maximum marginal likelihood have limited performance in terms of fitting robustly and recovering the true data generating process. Technically, given a particular kernel with particular hyperparameters, a GP does not support an entire Hilbert space but only a slice through that space; changing the hyperparameters by an infinitesimal amount yields a different slice which has no overlap with the original one. Therefore in this dissertation, a full Bayesian approach is taken for the GP inference, i.e., the entire Hilbert space of functions is considered by taking into account all of the possible hyperparameters for a specific kernel. For the class of problems which have Gaussian observation models, GP has nice closed-form posterior results. However, for the situations which do not have Gaussian observation models, for examples the ones in this dissertation which employ Student-t or Poisson likelihood, there does not exist analytical solutions. HMC as discussed in Section 2.3 is used to sample from the posteriors. Specifically, the noncentered parameterization of the latent multivariate Gaussian is
exploited. The reparameterized model is
56
f̃ ∼ MVNormal(0n, In) (5.10a) L = Cholesky(Kxx) (5.10b)
f = mx + L · f̃ (5.10c)
which defines the same distribution as Expression 5.9 but induces a nicer posterior geometry for HMC to explore and sample from (Betancourt 2017a). Once the learning on hyperparameters is done, posterior predictive distribution of the
latent function values which are not part of the original dataset is obtained by f∗ | f ∼ MVNormal ( m∗ + K > x∗K −1 xx (f −mx), K∗∗ −K>x∗K−1xxKx∗ ) , (5.11)
where m∗ is the mean function evaluated at the new inputs, K∗∗ is the covariance between the new inputs, and Kx∗ is the covariance between the original inputs and the new inputs. 5.3 suite of models for pattern recognition This section presents models built from various angles, with the goal of providing a
coherent framework for learning the flaring pattern and behavior in a principled manner. Each model is tested on real flaring data from North Dakota. Whenever more granular analytics capabilities are demonstrated through investigations at oilfield level or operator level, the data from a major producing field, the Blue Buttes Oilfield (Alexeyev et al. 2017), and one operator, denoted by ‘Operator A’ are used. 5.3.1 modeling proportion of gas flared The proportion of gas production that is flared is an indicator of flaring intensity and energy efficiency. It is interesting to investigate whether the proportion has changed over a period of time for certain operators and oilfields. The model is specified through Expressions 5.12a– 5.12i:
` ∼ Gamma(2, 1) (5.12a) η ∼ Half-Cauchy(5) (5.12b)
57
ν ∼ Gamma(2, 0.1) (5.12c) σ̂2 ∼ Half-Cauchy(5) (5.12d) k = η2 × kmatérn52(x, x′; `) (5.12e) f ∼ GP(0, k) (5.12f) πi = logit
−1(f(xi)) (5.12g) µi = πi ×Gi (5.12h) Fi ∼ Student-t(ν, µi, 1/σ̂2) (5.12i)
where:
` is the lengthscale for the Matérn kernel; η is the marginal deviation parameter controlling how strongly the latent functions
vary in the output space;
ν is the degrees of freedom for the Student-t likelihood; σ̂2 controls the inverse scaling parameter of the Student-t likelihood (analogous to
the precision of a Gaussian distribution);
k is the covariance function for the GP; f denotes the latent process, which is distributed according to the GP; πi is the underlying flaring gas proportion of month i. Since proportion is bounded
between 0 and 1, the inverse-logit function is applied to the latent process;
Gi is the total gas production of month i; µi denotes the underlying flared volume of month i; Fi is the reported flared volume, which is modeled using a Student-t observation
model. The reasoning behind choosing a Student-t observation model is to make the model specification be able to generalize to as many entities as possible and be robust to (potentially many) outliers and noisy data points. This is due to the fact that at this time, operators have to estimate the flared volume by their own procedures and conduct reporting, in which case inaccuracies are introduced unintentionally or intentionally. The heavier tail of Student’s
58
t-distribution is a natural decision in modeling to deal with those phenomena. This line of thought, i.e., design models that are generic and robust, is indeed reflected in choosing the half-Cauchy priors (which are heavy-tailed and very weakly informative) and GP as a nonparametric regression technique. To demonstrate this model’s capability on real data, both the Blue Buttes Oilfield and Operator A are tested. The production and flared volumes coming from NDIC are used. For the oilfield, the posterior distributions and trace plots of the hyperparameters are presented in Figure 5.1. The posterior predictive samples for the underlying process of gas flaring proportions (πi) are demonstrated in Figure 5.2, which depict the trend very clearly. The colored bands have the below coverage for the posterior samples:
• The darkest colored band (in the center at a certain x location) represents the 49th
percentile to 51st percentile;
• The lightest colored band (characterized by the widest interval at a certain x location)
represents the 1st percentile to 99th percentile. Additionally, 30 random samples are drawn from the GP posterior and plotted on the same figure, showing as thin lines. The latent functions do not go through all the observed data points, in which case the model would have been overfitted; instead they present the possible functions which are most compatible with the data as well as the assumptions inherent in the model. On one hand, the insights are already obtained, i.e., the underlying process is inferred. On the other hand, this serves as an anomaly detection tool. For example, the state government might be interested to look into that observed data in the second half of 2019 which deviated quite a lot from the “true” process, e.g. to audit the reporting for that month or to investigate what had happened that led to a sudden huge drop in flaring in just one month. With the exact same model specification, the model is also run with the operator’s data. The posterior distributions and trace plots of the hyperparameters are presented in Figure 5.3. 59
60
The posterior predictive samples for the underlying process of gas flaring proportions (πi) are demonstrated in Figure 5.4. It can be seen this operator’s flaring proportion time series is more jagged than the Blue Buttes Oilfield (which is operated by more than five companies). A operator can change flaring strategies more swiftly which can be captured as well. Nevertheless the long-term trend is also available. Comparing Figure 5.1 and Figure 5.3, it can be seen the posterior distributions are very different. However the priors for them were specified in the exact same way. This showcases the power of Bayesian approach. Taking ` as an example, a Gamma(2, 1) prior is placed on it. However, after conditioning on the data, the operator model reports smaller lengthscale values on average (indicating jagged processes), whereas the oilfield model reports larger lengthscale values (suggesting smoother processes). 61
Order 24665, which is established by the North Dakota Industrial Commission, defines
the gas capture percentage pcap as
pcap = Gsold +Gused +Gproc
Gprod , (5.13)
where:
Gsold is the monthly gas sold; Gused is the monthly gas used on lease; Gproc is the monthly gas processed; Gprod is the monthly gas produced. Since North Dakota bans the venting of natural gas (U.S. Department of Energy 2019b), it is obvious the model developed in this section provides a powerful tool for NDIC to evaluate compliance with the gas capture goals: at a given month i, pcap = 1− πi. Furthermore, when looking at the model specification, there is nothing special that encodes the data sources and location information. A user of this model is free to use satellite estimation as the observed data or apply it to the Permian Basin, and conduct inference on the flaring proportion. This is a benefit from using nonparametric and interpretable models as opposed to black box
62
models (such as the neural networks, in which case the learned weights and bias inside the network provide little or no domain insights). The author hopes this section provides a comprehensive view in terms of how and why to use GP, with real data. Models built and presented in later sections follow a similar flow. 5.3.2 modeling proportion of wells flaring The proportion of wells that conduct flaring in a month can reflect a company’s flaring strategy and is an indicator of flaring magnitude. It is interesting to investigate how this indicator varies for a certain entity in a certain time period. The model is specified through Expressions 5.14a–5.14f:
` ∼ Gamma(2, 1) (5.14a) η ∼ Half-Cauchy(5) (5.14b) k = η2 × kmatérn52(x, x′; `) (5.14c) f ∼ GP(0, k) (5.14d) pi = logit
−1(f(xi)) (5.14e) Wi ∼ Binomial(Ni, pi) (5.14f)
where pi is the unobserved “true” proportion of wells that conduct flaring in month i, Ni is the total number of active wells in month i, and Wi is the observed (i.e., estimated and reported by company) number of wells that conduct flaring in month i. The rest of the symbols have the same meaning as in Model 5.12. To demonstrate this model’s capability on actual data, both the Blue Buttes Oilfield and Operator A are tested. For the oilfield, the posterior distributions and trace plots of the hyperparameters are presented in Figure 5.5. The posterior predictive samples for the underlying process of well flaring proportion (pi) are demonstrated in Figure 5.6. The visualization strategy (different colors represent different percentiles, etc.) is the same as in Section 5.3.1. 63
With the exact same model specification, this model is also tested with the operator’s data. The posterior distributions and trace plots of the hyperparameters are presented in Figure 5.7. The posterior predictive samples for the underlying process of well flaring proportion (pi) are demonstrated in Figure 5.8. Comparing the two sets of figures from the oilfield and the operator, it can be seen:
64
1. With the same prior placed on the lengthscale `, the oilfield model learns from the data
and gives a posterior mode around 5.5, whereas the operator model gives a posterior mode around 10.0. This is also reflected in the posterior samples time series plot: the oilfield experienced some well flaring proportion changes in relative shorter time periods, whereas the operator underwent changes on a longer time span. 2. The oilfield’s posterior samples time series show narrower percentile bands while the
operator’s show wider percentile bands. This is due to the fact that the operator chosen here had smaller number of wells than the oilfield. Since the binomial observation model is used for each month’s flaring well count, this naturally represents and quantifies the uncertainties (i.e., binary data contains less information especially when the sample size is small), as well as aligns with the expectation that when there is more data, there should be less uncertainties; when there is less data, there should be more uncertainties. This really showcases how and why to encode domain expertise in flaring data analytics while exploiting machine learning models, which is also the reason to choose the Bayesian approach. One could fit a black box model either with target values Wi ∈ R, or without any probabilistic view (e.g., to optimize for the best deterministic function mapping in the
65
hypothesis space). But either of those would be fundamentally flawed. Domain expertise indicates the well count has to be a non-positive integer, i.e., Wi ∈ N0. Furthermore, neither the NDIC reporting nor the satellite estimation is ever produced in a noise-free environment, and therefore probabilistic modeling is a must. Compared to frequentist machine learning, Bayesian learning is entirely probabilistic and gives one the capability and freedom to encode his/her domain expertise. 5.3.3 modeling flare detection count Satellite detected flare count provides an unbiased indicator of flaring intensity. How this
indicator varies in a certain time period for a certain entity is valuable information to obtain. The model is specified through Expressions 5.15a–5.15f. Essentially the latent process is modeled as a Gaussian Cox process (Adams et al. 2009), where the Poisson process has varying intensity across time domain and a GP prior is placed on this intensity. ` ∼ Gamma(2, 1) (5.15a) η ∼ Half-Cauchy(5) (5.15b) k = η2 × kmatérn52(x, x′; `) (5.15c)
66
f ∼ GP(0, k) (5.15d) λi = exp ( f(xi) ) (5.15e) Ci ∼ Poisson(λi) (5.15f)
where λi is the unobserved flaring intensity (“true” count) in month i and Ci is the reported VIIRS detection count in month i. Since λi is bounded to be positive, the natural exponential function is applied to the latent process. The rest of the symbols have the same meaning as in Model 5.12. For the task of flaring pattern recognition, the author believes this approach (leveraging a Gaussian Cox process) is a nicer surrogate than a popular change point model presented in (Davidson-Pilon 2015; Salvatier et al. 2016; Stan Development Team 2020), which is specified by:
e ∼ Exponential(re) (5.16a) l ∼ Exponential(rl) (5.16b) s ∼ Uniform(1, T ) (5.16c) Ci ∼ Poisson(i < s ? e : l) (5.16d)
where e and l are the early and late rates respectively, re and rl controls the priors for the early and late rates, s is the change point, T is the total time period, and the rate in the Poisson likelihood is decided through a ternary conditional operator (?:). The reason is that, although this model could be generalized to more than one change point, its usage is restricted by the assumption that any period between two adjacent change points has a constant rate. This limitation becomes obvious when analyzing the actual flaring data in the discussions below, and is a major disadvantage of the change point model. The Gaussian Cox process model is tested with the Blue Buttes Oilfield’s data. Since only VIIRS data is used, the whole time series is analyzed beginning in 2012. The posterior distributions and trace plots of the hyperparameters are presented in Figure 5.9. The posterior predictive samples for the underlying process of flare count (Ci) are demonstrated
67
in Figure 5.10. The visualization strategy (different colors represent different percentiles, etc.) is the same as in Section 5.3.1. From the time series plot, it can be seen the observations from 2014 to 2017 can possibly be described by a change point model (with late 2015 being a potential change point), but the steady growth before and after that time span will frustrate accurate inference with such a model. 68
This model’s inference results serve as a type of confirmation, if not evidence, in terms of whether or not an entity achieves the goal/target in reducing the number of wells flaring, when the detection count is used as a surrogate for the number of wells flaring. In practice, reducing the number of wells flaring is exactly the second goal of the regulatory policy introduced by the North Dakota Industrial Commission in 2014. If the state government is interested in this order’s effectiveness from a macroscopic standpoint, the model can also be used to conduct inferences with the state level data. In this case, the posterior distributions and trace plots of the hyperparameters are presented in Figure 5.11. The posterior predictive samples for the underlying process of flare count (Ci) are demonstrated in Figure 5.12. The percentile bands in this case are quite narrow, which indicate greater confidence in the inferences about the data generating process given the model assumptions. By not (over)fitting to each and every observation, interesting patterns are discovered, for example in every year there is one and only one peak that happened around June. It is worth pointing out that there is no model that can tell the modeler if his/her assumptions are good, only domain expertise might. This model employing a Poisson observation model could be considered “rigid” due to the fact that a Poisson likelihood has only one parameter λ (to
69
control both the mean and variance) and, furthermore, when λ is large as in this scenario, a Poisson distribution is well approximated by a normal distribution. Whenever the state government believes that overdispersion might exist, other observation models such as the negative binomial distribution could be considered. In such cases, only Expression 5.15f needs to be changed to the negative binomial likelihood, with a prior added for the overdispersion parameter. The specific parameterization is given by Equation 6.4 in Section 6.3. This really showcases both the flexibility and interpretability of taking a Bayesian approach for high-stakes decision making areas including flaring data analytics. 5.3.4 modeling proportion of oil flared As crude oil (as opposed to natural gas) is the main commodity at this time, the amount of gas in a barrel of oil equivalent (BOE) that is flared provides an indicator of production efficiency due to flaring. In this work, the normalized quantity, proportion of oil production being flared, is used such that the model specification is generic for large and small entities. The model is specified through Expressions 5.17a–5.17j:
` ∼ Gamma(2, 1) (5.17a)
70
η ∼ Half-Cauchy(5) (5.17b) ν ∼ Gamma(2, 0.1) (5.17c) σ̂2 ∼ Half-Cauchy(5) (5.17d) k = η2 × kmatérn52(x, x′; `) (5.17e) f ∼ GP(0, k) (5.17f) πi = logit
−1(f(xi)) (5.17g) µi = πi ×Oi (5.17h)
c = 6 Mcf
1 BOE (5.17i)
Fi/c =: Ei ∼ Student-t(ν, µi, 1/σ̂2) (5.17j)
where:
πi is the underlying flaring BOE proportion of month i; Oi is the total oil production of month i; µi denotes the “true” flared BOE of month i; c denotes the conversion factor that 6 Mcf equals 1 BOE, given by the United States
Geological Survey (2000);
Ei is the reported flared BOE, which is modeled using a Student-t observation model. The rest of the symbols have the same meaning as in Model 5.12. To test this model’s performance on real data, both the Blue Buttes Oilfield and Operator A are used. For the oilfield, the posterior distributions and trace plots of the hyperparameters are presented in Figure 5.13. The posterior predictive samples for the underlying process of BOE flaring proportion (πi) are demonstrated in Figure 5.14. The visualization strategy (different colors represent different percentiles, etc.) is the same as in Section 5.3.1. With the exact same model specification, this model is also tested with the operator’s data. The posterior distributions and trace plots of the hyperparameters are presented in Figure 5.15. The posterior predictive samples for the underlying process of BOE flaring proportion (πi) are demonstrated in Figure 5.16. Comparing the two sets of figures from the oilfield and the operator, it can be observed:
71
72
1. With the same prior placed on the lengthscale `, which has a mean of 2 (months), both
models have updated the posterior to move away from this mean, reflecting a long range variation. The oilfield has a posterior mode about 1 year while the operator has a mode around 15 months. The operator has much larger reporting variability, shown by the parameter σ̂2. 2. With a Student-t likelihood, both models demonstrate robustness to outliers and
overfitting. This can be seen from the oilfield’s late 2019 observations and the operator’s early 2016 observations. For the posterior function samples, shown as the thin lines, some of them are indeed pulled towards those “outliers”. However, the percentile plots
73
(shown as the colored bands) are not impacted and those really can be interpreted as the trend which is most compatible with the data and the assumptions. This built-in Occam’s razor of the Bayesian approach when choosing appropriate priors is very impressive. In many of the frequentist machine learning methods, if the regularization strategy is not implemented well especially when the sample size is not huge enough for the asymptotic properties to kick in, outliers become “influential observations” that will have a huge undesirable effect on the inference results. 5.3.5 modeling scale factor between viirs and ndic Both NDIC and VIIRS reporting give (estimated) flared gas volume. The scale factor
between the two sources provides insights into whether NDIC reporting is consistent:
1. for different entities (e.g., among a group of operators), and
2. for one entity when looking at a certain time period. This is based on the fact that the satellite detection processing algorithm is unbiased and consistent. Item 2 is particularly interesting in terms of time series analytics. The model is specified through Expressions 5.18a–5.18n:
74
`mat ∼ Gamma(8, 2) (5.18a) ηmat ∼ Half-Cauchy(5) (5.18b) T ∼ N (12, 1) (5.18c) `per ∼ Gamma(4, 3) (5.18d) ηper ∼ Half-Cauchy(5) (5.18e) ν ∼ Gamma(2, 0.1) (5.18f) σ̂2 ∼ Half-Cauchy(5) (5.18g) kmat = η 2 mat × kmatérn52(x, x′; `mat) (5.18h) kper = η 2 per × kperiodic(x, x′;T, `per) (5.18i)
kwn = kWhiteNoise(x, x ′; δ = 1e−6) (5.18j)
f ∼ GP(0, kmat + kper + kwn) (5.18k) βi = exp ( f(xi) ) (5.18l)
µi = βi × VIIRSi (5.18m) NDICi ∼ Student-t(ν, µi, 1/σ̂2) (5.18n)
where:
`mat is the lengthscale for the Matérn kernel; ηmat is the marginal deviation for the Matérn kernel; T is the period for the periodic kernel; `per is the lengthscale for the periodic kernel; ηper is the marginal deviation for the periodic kernel; kmat is the Matérn kernel (component); kper is the periodic kernel (component); kwn is the white noise kernel (component); f denotes the latent process, which is distributed according to a GP whose covariance
function is the sum of 3 kernels;
βi is the underlying scale factor between VIIRS and NDIC of month i. Since this scale
factor is bounded to be positive, the natural exponential function is applied to
75
the latent process;
VIIRSi is the VIIRS reported volume of month i; µi denotes the underlying flared volume of month i; NDICi is the NDIC reported volume of month i, which is modeled using a Student-t
observation model. The rest of the symbols have the same meaning as in Model 5.12. The reason for adding a periodic kernel is to investigate if there are any seasonal patterns. Maintaining a proper Bayesian workflow lets the data speak for itself, i.e., whether there exists seasonal behaviors or not, as shown by the two case studies in this section. The model is first fitted with the state level data to investigate the macroscopic reporting consistency. The posterior distributions and trace plots of the hyperparameters are presented in Figure 5.17. The posterior predictive samples for the underlying process of the scale factor variations (βi) are demonstrated in Figure 5.18. The visualization strategy (different colors represent different percentiles, etc.) is the same as in Section 5.3.1. From the posterior time series plot, it can be seen in general the volumes from NDIC reporting is smaller than that of VIIRS reporting, except for the times when the total flaring magnitude was small (indicated by the smaller points). More importantly, within each and every year from 2015 to 2018, there is a decreasing trend in the values of the scale factor (βi) around midyear. Each year’s latent process from Q2 to Q3 can be viewed as a “seesaw”, with July being the middle pivot point and the months after July always going down. Note that within each year, the NDIC reporting of flared volumes might increase steadily or a lot (which was actually happening from the time series plot in Figure 3.7), however this scale factor declining trends indicate the satellites observed much greater flaring activities than what was reported by the companies! This finding suggests that the NDIC reporting is very likely not consistent throughout the year, and the state government should be concerned that some companies might underreport their flared volumes especially in the second half of the year. 76
77
A interesting question arises: is this seasonal behavior universal across all the entities? The answer is unfortunately no, which indicates some operators likely reported their flared volume in an inconsistent manner throughout the entire year. In fact, if the Blue Buttes Oilfield data is used to fit the model, rather consistent behavior is observed. In this case, the posterior distributions and trace plots of the hyperparameters are presented in Figure 5.19. The posterior predictive samples for the underlying process of the scale factor variations (βi) are demonstrated in Figure 5.20. With the exact same model specification incorporating the periodic kernel, no apparent seasonal behaviors are discerned by the inference process. There are much uncertainties around the time of early 2016, where the point sizes indicate the overall flaring magnitudes were small as observed from VIIRS, and the NDIC reported volumes were actually larger than that of VIIRS. This could be due to the truncation effects instead of the reporting inconsistencies, i.e., when the flares are sporadic and weaker, they are not easily captured by the satellites, resulting in a truncated sample for the VIIRS processing workflow. By applying this model and workflow to the other major producing fields, it will likely pick up the ones who have the “seesaw” behaviors in their reporting. 78
79 5.3.6 predicting ndic flared volume GP is not only fully capable of making predictions once the model hyperparameters are learned, but it can provide rigorously constructed intervals quantifying uncertainties as well through Expression 5.11, for which many of the frequentist machine learning methods fail to do. The author chooses to present one particular prediction case study, that is to predict NDIC reported volume based on the projected scale factor between VIIRS and NDIC. This will be a particular interesting deployment scenario once fast satellite detection/estimation is available, which takes less time than waiting on company reports followed by compiling everything into an analytics-ready format. The predictions are generated in the form of posterior predictive samples. Along with the historical observations, the predictions of the scale factor for the next six months are presented in Figure 5.21. The very wide percentile bands in the forecasting indicate that the seasonal behaviors will likely take effect again, however with great uncertainties. If point predictions (i.e., without the prediction intervals) are needed, one can always use the posterior mean, mode, etc. to construct that “best” function; however this showcases why predicting
80
the future is generally very difficult and uncertainties should always be properly characterized. 5.3.7 a look back at the prior choices Looking back at the suite of models developed, the set of priors for the latent functions have been the same (except the scale factor model where a periodic kernel is added). However the posteriors are all updated (i.e., “learned”) based on each dataset and modeling goal. This means the below set of priors
` ∼ Gamma(2, 1) (5.19a) η ∼ Half-Cauchy(5) (5.19b) k = η2 × kmatérn52(x, x′; `) (5.19c) f ∼ GP(0, k) (5.19d)
serves as a generic framework and can be recommended for flaring time series analytics in general, in a GP context. Notice this prior choice gives latent function values in the unconstrained space, i.e., f(x) ∈ R. However, in many situations, the domain expertise
81
indicates the quantities of interest live in constrained space, such as:
• R>0 for Poisson rate parameter when modeling count data, and
• [0, 1] for binomial success probability when modeling flaring well proportion. To better reflect the domain expertise, the link functions can be leveraged. For the above scenarios, the log link function and the logit link function can be applied, respectively. Although this prior configuration is the result of several design iterations and tested with real data, there is no reason to think that it is optimal for every entity. Indeed, the model for scale factor between VIIRS and NDIC has bespoke components in its priors. The Stan Development Team (2020) also gave some general prior choice recommendations for GP. The whole suite of models demonstrate full capability of harnessing the temporal structure in flaring time series at different levels for different entities. This provides huge potential for extracting insights from noisy monthly data streams. For the situations where cross-sectional data analytics is desirable, for example when the latest monthly data is available and the state government needs insights from merely that month (before appending it to the whole historical data for a longitudinal study), other types of models can be built. Such is discussed in the next chapter. 82
CHAPTER 6
UNSUPERVISED LEARNING FROM MULTIPLE PERSPECTIVES
“Estimation of densities is a universal problem of statistics (knowing the densities one can solve various problems).”
— Vapnik (2000) 6.1 learning the distribution In this chapter, the author studies how to describe the flaring related quantities’ distribution among the oilfields in North Dakota in a cross-sectional setting. That is, data collected for one point or a period of time (such as a certain month or quarter) is analyzed. In this setting, the data used for learning is unlabeled:
U = {x1, x2, . . . , xN} , (6.1)
where xi, i = 1, 2, . . . , N , are the observations for the i-th oilfield. Thus unsupervised learning is naturally applied. The model to be learned is in the form of a conditional probability distribution Pθ(x | z) where z is some latent structure and θ represents the parameters. This has many application scenarios in practice. When the latest month’s or quarter’s data is available, the government of North Dakota might need distributional insights of the population (of oilfields), preferably beyond some forms of the order statistics (such as the five-number summary). This cross-sectional study is especially valuable and worth conducting when a direct comparison with previous months/quarters (which can be either the immediately previous one, or the same month/quarter in previous years) is desirable, or deeper understanding of the population is needed, such as looking for potential clusters among the entities. 83 6.2 probability model estimation The task of learning distributions is a probability model estimation problem in unsupervised settings (Li 2019). It sometimes takes the form of density estimation, which is considered by some statisticians as the most fundamental topic in probabilistic machine learning (Yu 2017). A basic and common technique, the histogram, can be easily misused which leads to biased understanding of the dataset (Figure 6.1). In general, assuming that the data is generated by a probability model, the structure and parameters of that model are learned from the data. The type of the structure, i.e., the set of possible probability models is usually given (assumed), while the specifics of the structure and the parameters have to be learned. The goal is to find the model structure and the parameters which are most likely to have generated the data. The probability model can be a mixture model or a graphical model. In this dissertation, the mixture model is considered, where the assumption is that data comes from a mixture of distributions. Mathematically, mixture models describe a distribution p(x) by a convex combination of K base distributions:
84
p(x) = K∑ k=1 πkpk(x) (6.2a)
K∑ k=1 πk = 1, πk ≥ 0, (6.2b)
where pk are the components in the mixture and πk are the mixture weights. Mixture models can be interpreted as the overall population being a combination of distinct subpopulations. Mixture models can be generalized to the continuous cases as well. For example, both the negative binomial distribution and Student’s t-distribution can be thought of a mixture of some continuous distributions (Martin 2018). In the model representation Pθ(x | z), x stands for the observations which can be discrete
or continuous quantities; z represents the latent structure which is a discrete random variable. The model is parameterized by θ. When the model is assumed to be a mixture type, z represents the different components. The knowledge of the model structure and parameters are learned from the data U = {x1, x2, . . . , xN}, where in this work xi ∈ X ⊆ R1, i = 1, 2, . . . , N , is the observation for the i-th oilfield. 6.3 modeling viirs detection count In Section 5.3.3, methods are developed for analyzing the time series of VIIRS detection count for any given oilfield. This section tackles the problem of how to extract insights from any given month’s flare detection count in North Dakota’s oilfields. Specifically, by learning from each oilfield’s detection count, the population of the oilfields is summarized, through which the state government can gain distributional insights. Following the general form in Section 6.2, this problem becomes a special case that the latent structure z does not exist, i.e., satisfying Pθ(x | z) = Pθ(x), where x represents the detection count. It is when estimating conditional probability distributions becomes estimating probability distributions, therefore, only estimating the parameters of Pθ(x) is enough. Density estimation in classical statistics, for instance the Gaussian parameters
85
estimation, is an example of such scenarios. Since the count data is modeled, the author compares the four observation models below
with many randomly chosen months’ data:
1. Poisson likelihood
2. Negative binomial likelihood
3. Zero-inflated Poisson (ZIP) likelihood
4. Zero-inflated negative binomial (ZINB) likelihood
Items 3 and 4 above are experimented with because many of the oilfields in North Dakota did not have detection records from VIIRS for a given month. Therefore, zero-inflated models are tried as well. Through the posterior predictive checks, it is found that the negative binomial observation model fits data in the most compatible manner, which is employed in this work. The model is specified through Expressions 6.3a–6.3c:
µ ∼ Gamma(2, 1) (6.3a) φ ∼ Exponential(1) (6.3b) Ci ∼ NegBinomial(µ, φ) (6.3c)
where Ci denotes the detection count for the i-th oilfield. The probability mass function of the negative binomial likelihood is parameterized by a location parameter µ ∈ R>0, and an overdispersion parameter φ ∈ R>0, in the following way:
P (X = n | µ, φ) = Γ(φ+ n) n! Γ(φ)
( µ
µ+ φ
)n( φ
µ+ φ
)φ for n ∈ N0 , (6.4)
where Γ(·) is the gamma function. Through this parameterization, the expectation and variance of a random variable X ∼ P are:
E[X] = µ and V[X] = µ+ µ2
φ . (6.5)
86
As the negative binomial distribution describes a Poisson random variable whose rate parameter is gamma distributed, and due to the fact that Poisson(µ) has variance µ, the learned parameters provide nice interpretations for the state government:
• µ indicates a mean intensity from the detection count’s perspective, just like the
interpretation of a Poisson’s rate parameter. The larger the value of µ, the more flare detections are present on average at an oilfield level. • φ indicates the heterogeneity among the oilfields in North Dakota. Specifically, µ2/φ is
the additional variance above that of a Poisson with rate µ. The smaller the value of φ, the more oilfields with extreme detection counts (away from µ) are present. To demonstrate this model’s compatibility with the observations, the data from October 2018 is used. There are 506 oilfields in total. The distribution of the detection count for all the oilfields is illustrated in Figure 6.2. 87
After fitting Model 6.3, the posterior distributions and trace plots of the hyperparameters
are presented in Figure 6.3. The parameter estimation results are reported in Table 6.1. Parameter Variable Point Estimate 90 % CI
µ Intensity 1.005 (0.814, 1.200) φ Heterogeneity 0.168 (0.135, 0.202)
The point estimate for the intensity parameter µ is relatively small (µ̂ ≈ 1), which possibly results from the model being overwhelmed by the large number of zero counts. However, by inspecting the histogram from Figure 6.2, the tail of the distribution definitely extends far beyond µ̂. Therefore, posterior predictive checks are performed to scrutinize Model 6.3’s compatibility with the observations. These types of checks substantially harness the information from the samples drawn from the posterior distributions. By combining the uncertainty about the parameters, as described by the posterior, with the uncertainty about the outcomes, as described by the likelihood, the generative model is employed to simulate the implied observations. Subsequently, posterior predictive plots are generated to display the model-based predictions along with the raw data. Such a plot for the detection count distribution model is given in Figure 6.4. 88
In Figure 6.4, the histograms for the original VIIRS observations, as well as all of the posterior predictive simulations are displayed. Each set of the parameter values (of µ and φ) are used in simulating one synthetic snapshot of the oilfields in North Dakota for October 2018, and there are in total 12,000 snapshots (constructed by the samples from the four Markov chains, each of which was setup for 3000 sampling iterations). Every histogram is visualized through an unfilled line chart, i.e., rendering the “step” histogram. Through Figure 6.4, it appears that the model is very compatible with the observations from October 2018, in that there is no obvious and consistent discrepancy between the observed and simulated data. To delve into the tail behaviors, i.e., beyond the zero count, a zoomed-in view is depicted in Figure 6.5. A few discrepancies are observed from this view, for example, when the count Ci = 11 and Ci = 12. One thing to note is that, with such a low mean (µ̂ ≈ 1), even with a relatively large overdispersion (φ̂ ≈ 0.2), the model would still be surprised by the high detection count, e.g., when Ci ≥ 20. 89
The thorough performance of Model 6.3 that is characterized by a negative binomial likelihood, and the complicatedness of the real data manifest themselves through the posterior predictive checks. As discussed earlier in Section 6.3, the negative binomial likelihood was compared with three other likelihoods (Poisson, ZIP and ZINB) on many randomly chosen months, and found to outperform them in terms of the compatibility with the data in general. In fact, there are some months’ data that are distributed in a “cleaner” way, i.e., almost perfectly described by Model 6.3. The author chooses not to cherry-pick those data, in the hope of not misleading the readers about the performance of the developed model. Nevertheless, the simplicity, interpretability, and effectiveness of Model 6.3 proves itself in the mission of modeling detection count distribution. In practice, the state government can benefit from this model in the two use cases below:
1. When the latest month’s data becomes available, Model 6.3 can be fitted to obtain an
estimate for µ and φ. These parameter estimates along with the credible intervals can be compared with those from the earlier times. In the case of the discussions above, the
90
learned parameters can be compared either with August/September from 2018, or with October from 2016/2017. From the comparison, it provides insights into whether there are more detection counts on average (characterized by a larger µ), or if more oilfields with an atypical number of detections are spotted (characterized by a smaller φ). 2. After the model is fitted, it is recommended to perform the posterior predictive checks
as demonstrated in Figure 6.4 and Figure 6.5, to identify any issues of the fits. The list of the oilfields which have large deviations from the simulated data, especially those on the far tail (e.g., when Ci ≥ 20), are worth tracking. That is, to investigate whether the “anomalies” from each month are random samples from the population or do not change from month to month. This provides further understanding of how the oilfields population behave, from the perspective of the detection count. A distributional summary of the detection counts exhibits only one facet of the flaring landscape, while the flared volumes distribution provides another crucial one, which is discussed next. 6.4 modeling flared volume In this section, the VIIRS estimated flared volumes for different oilfields are studied from a distributional point of view. The dataset from a three-month period is analyzed for demonstration purposes. Specifically, following the reverse geocoding as discussed in Section 3.3, all the oilfields’ cumulative flared volumes during Q4 2018 are computed and compiled for analysis. There are in total 152 oilfields that have VIIRS reported volumes in this time span. The data is highly skewed (Figure 6.6). Therefore, for each oilfield, the order of magnitude of the flared volume (in bcm) is computed for the analysis, instead of working with the original absolute volumes. From an applied perspective, taking the log of a measure converts the measure into
magnitudes (McElreath 2015), which is applied to each oilfield’s flared volume:
91
Li = log(Fi), (6.6)
where Fi is the original flared volume in bcm, and Li is the flared volume magnitude, both of which are for the i-th oilfield. In this dissertation, base e is always used for the logarithm (i.e., natural logarithm). A univariate distribution of the magnitudes is visualized in Figure 6.7. Among the three approaches used to visualize the distribution, only the rug plot does not lead to subtleties due to the hyperparameters used. However, as a 1D scatter plot, its representation ability is naturally limited. The histogram suffers from the problem as illustrated in Figure 6.1. The curve is generated by kernel density estimation (KDE). For a given dataset as defined in Equation 6.1, KDE represents the underlying distribution as:
p̂(x) = 1
Nh N∑ i=1 K (x− xi h ) , (6.7)
where K(·) is a kernel function and h is a bandwidth parameter. To generate Figure 6.7, the Gaussian kernel is used, which is given by:
92
K(z) = 1√ 2π exp
( −z 2
2
) , (6.8)
and h is chosen based on Scott’s rule. Since the bandwidth plays a similar role as the bin size in histograms, KDE can also lead to the same issue as in histograms. Nevertheless, all three (the rug plot, histogram and KDE) agree that a single Gaussian approximation of the density which generates this data would be a poor approximation. Therefore, Gaussian mixture model (GMM) is employed to represent the data, i.e. the base distributions in Model 6.2 are chosen to be Gaussians. GMM provides more expressive modeling capabilities and also possibilities for clustering. 6.4.1 model specification As discussed earlier, since the flared volume is a continuous quantity, density estimation is applicable and tackled with GMM. At first, the data generating process is considered, which paves the way for potential clustering applications. That is, each data point Li (defined in
93
Equation 6.6) is assumed to be generated by exactly one mixture component. The number of components, K, is unknown, and up to seven components are tried to fit the dataset visualized in Figure 6.7. A relatively small number of components are experimented, because as the number of components increases, it becomes more difficult to interpret the modeling results. The model is specified through Expressions 6.9a–6.9i, ∀K ∈ {2, . . . , 7}:
α = (α1, . . . , αK) = 6 · 1K (6.9a) p ∼ Dirichlet(α) (6.9b) zi ∼ Categorical(p) (6.9c) l1 = min{L1, . . . , Ln} (6.9d) l2 = max{L1, . . . , Ln} (6.9e)
µ̃k = l1 + (k − 1) ( l2 − l1 K − 1 ) , k = 1, . . . , K (6.9f) µk ∼ N (µ̃k, 2), k = 1, . . . , K (6.9g) σk ∼ Half-Normal(2), k = 1, . . . , K (6.9h)
Li | (zi = j) ∼ N (µj, σj) j ∈ {1, . . . , K} (6.9i)
where:
α is the vector of concentration parameters for the Dirichlet distribution, which is a
multivariate generalization of the beta distribution;
p is the simplex of probabilities for the mixture components, which is assigned
a Dirichlet prior. This prior with each value inside α being 6, is a weakly informative prior, expecting any pk inside p could be bigger or smaller than the others. Ten random draws from Dirichlet([6, 6, 6, 6, 6, 6, 6]) are displayed in Figure 6.8;
zi is the probable mixture component that the i-th oilfield belongs to; l1 and l2 are the lower and upper bound for {Li}ni=1, respectively; µ̃k is used in “initializing” the location of the k-th mixture component, and {µ̃k}Kk=1 essentially represent the K evenly spaced points between [l1, l2];
94
µk is the mean for the k-th Gaussian component; σk is the standard deviation for the k-th Gaussian component; Li is the flared volume magnitude of the i-th oilfield, which is generated by the
mixture component zi. Model 6.9, while unambiguously expressing the assumed generative process, relies on sampling the discrete latent variables zn, which is controlled by a categorical mixing distribution. This reliance causes slow mixing and ineffective exploration of the posterior distribution. An equivalent parameterization which addresses these problems is to marginalize out the z parameter. The marginalized model is specified through Expressions 6.10a–6.10h, ∀K ∈ {2, . . . , 7}:
α = (α1, . . . , αK) = 6 · 1K (6.10a) w ∼ Dirichlet(α) (6.10b) l1 = min{L1, . . . , Ln} (6.10c) l2 = max{L1, . . . , Ln} (6.10d)
µ̃k = l1 + (k − 1) ( l2 − l1 K − 1 ) , k = 1, . . . , K (6.10e)
95
µk ∼ N (µ̃k, 2), k = 1, . . . , K (6.10f) σk ∼ Half-Normal(2), k = 1, . . . , K (6.10g) Li ∼ K∑ j=1 wj N (µj, σj) (6.10h)
where w are the mixture weights (i.e., mixing proportions), and the rest of the symbols have the same meaning as in Model 6.9. The likelihood function, defined in Expression 6.10h, corresponds with the density of a mixture model expressed in its general form (Equation 6.2a). Model 6.10 is implemented and fitted six times (∀K ∈ {2, . . . , 7}) to compare the inference results with different number of components specified. For each K, rapid mixing and fast convergence of the Markov chains are obtained. The modeling results are displayed in Figure 6.9, where the KDE (same as in Figure 6.7) and the Gaussian components inferred are plotted along with the posterior samples. It can be observed that, when using a mixture of Gaussians, the multimodal features can be represented in a relative effortlessly way, and all the mean fits are quite close to the one obtained with KDE. As the number of components increases, for example when K = 6 or K = 7, the mean density estimation using GMM resembles KDE more closely, but the samples from the posterior show more stochasticity, which is an indicator of potential overfitting. This naturally leads to the question of how to decide the number of components for this dataset. 6.4.2 model comparison Choosing the best K is a model comparison problem, for which there does not exist a silver bullet. In this dissertation, the author chooses to take the information criteria approach, specifically leveraging the widely applicable information criterion (WAIC) introduced by Watanabe (2010). Information criteria provide a theoretical estimate of the relative out-ofsample KL divergence (McElreath 2020), and thus a lower value is better. Following Martin (2018) and McElreath (2020), WAIC is computed by:
96
WAIC(y,Θ) = −2× lppd(y,Θ) + 2pwaic (6.11a)
= −2 n∑ i=1 log
( 1
S S∑ j=1
p(yi | Θj) )
+ 2 n∑ i=1 VΘ[log p(yi | Θj)] , (6.11b)
where:
y denotes the observations and yi is the i-th observation; Θ is the posterior distribution and Θj is the j-th set of sampled parameter values;
97
S is the number of posterior samples; lppd(·) calculates the log pointwise predictive density; pwaic is the penalty term given by summing up the variance in the log-likelihood over
the S posterior samples, for each observation i. Fundamentally, model comparison is performed by leveraging Occam’s razor, i.e., parsimonious models are preferred in light of predictive performance. The models are compared based on their WAIC values, which are summarized using Figure 6.10. It can be seen that the model with two Gaussian components are the best (smallest WAIC), however, there are considerable overlaps among all of the models when the estimated standard error is taken into consideration. Considering the fact that K = 2 gives the simplest model, also that there are only 152 observations (oilfields) in this dataset, the GMM with two components would be the best choice. 98 6.4.3 clustering When looking at the developed model from a latent variable perspective (Model 6.9), it becomes obvious that the mixture model serves as a natural candidate for solving clustering tasks, in that every observation (Li) can be drawn from one of the K data generating processes, each with its own set of parameters, N (Li | µk, σk). Since a probabilistic model is built, for the purpose of clustering, a reasonable choice is to assign a data point to the mixture component (i.e., cluster) with the highest posterior probabilities (which are also interpreted as the responsibilities). In the case of the 2-component GMM trained from the previous sections, for a particular observation x, the probability that it belongs to cluster one (z = 1) can be computed using Bayes’ theorem (Equation 2.3a):
p(z = 1 | x) = p(z = 1)N (x | µ1, σ1) p(z = 1)N (x | µ1, σ1) + p(z = 2)N (x | µ2, σ2) , (6.12)
where every part in the formula can be obtained from the posterior samples (e.g., using the posterior means). Clustering, as an unsupervised approach, can be used to reveal the hidden groups in the observations. In the case of the oilfield flaring magnitudes data in this chapter, the two clusters can be directly mapped to concepts such as major and minor flaring fields. However, it is usually the deeper insights into what caused these clusters that the state government is mostly interested in, for the sake of decision- and policy-making for example. If the oilfields belonging to the major flaring cluster seem to be a volatile membership when more months/quarters data are analyzed, the variations in flared volumes are possibly tied more closely to company strategies and movements. On the other hand, if there exists a group of oilfields that are found to join the major flaring cluster on a regular basis, this could provide a perspective in regards to where to construct the next natural gas processing plants, i.e., the locations/capacities of the new gas plants should be optimized based on those oilfields’ situations. 99
In this chapter, the dataset compiled for unsupervised learning is univariate, i.e., xi ∈ X ⊆ R1. GMM are also suitable for the density estimation and clustering tasks when the data goes beyond 1D. As an example, for the same oilfields studied for Q4 2018, if their oil production volumes are extracted from NDIC, a scatterplot of gas flaring versus oil production magnitudes is shown in Figure 6.11. It is very possible that the density of the underlying distribution can be modeled by a bivariate normal distribution or a 2D GMM. In such cases, the mixture components become multivariate normal distributions, and the component covariance matrices can be constructed with the help of the LKJ distribution (which is employed in Models 4.5 and 4.7). The developed density model can be used, for example, in anomaly detections, looking for any oilfields which have a tendency to creep toward the upper left corner (characterized by very little oil production and a huge flaring magnitude). Similar to all the inferences presented throughout this dissertation, one advantage of doing such is that the decision making can be based on some consistent metrics (such as probability scores), instead of some criteria based on human eyeballing or improvising. 102 103 104 105 106 107 100
This concludes the statistical modeling journey of this dissertation. In the next chapter, discussions are presented on one extension scenario and one bigger picture viewpoint, from applying Bayesian learning to flaring data. 101
CHAPTER 7 DISCUSSION
This chapter discusses the possibility of operator level monitoring and analytics, potential result inconsistencies, and relates the endeavors of learning from flaring data to the larger process of applying machine learning in the petroleum engineering domain. 7.1 operator level monitoring and analytics Up till this point, the satellite-detected flaring statistics have been applied to the state, county, and oilfield levels. This is made possible by the reverse geocoding discussed in Section 3.3. An ideal application scenario is operator level monitoring and analytics by leveraging the information from the satellite detections. Unfortunately, assigning flares to corresponding companies is not a straightforward operation. One possible solution is to make use of the shapefiles of the leases, which are not provided by NDIC. Some data vendors have such files in their database. However, after spending some effort investigating the lease shapefiles from one vendor, the author believes it is possible to create more problems than solving the existing ones, when bringing in such information. In particular, some reasons include:
• Multiple companies exist on a single lease. • The company names from the lease shapefiles do not always correspond with those on
the NDIC monthly production reports. • Some leases in the vendor’s database miss start date or end date data. • It takes time for the vendor to compile and digitize such information, which makes the
available lease shapefiles not up to date. 102
Nevertheless, for such an important use case, the author managed to develop a nearestneighbor-based approach which partly solves the problem (Algorithm 7.1). The essence of this approach is to cautiously assign the closest well’s operator to each satellite-detected flare. The closest wells are found based on the corresponding time window. For example, for the flares detected in January 2016, only the active wells reported on the NDIC production report from the same month are looked up. The function FindClosestOperator() returns the closest operator (OPj) for each VIIRS detection, as well as the calculated distance (dj) between each pair (of flare and well). The distance is calculated based on the haversine metric, i.e., the great-circle distance, thus the Earth radius (RE) is needed. The function is essentially performing the k-nearest-neighbors (k-NN) search for k = 1. When the sample is as large as in this case, i.e., there are usually a few hundred VIIRS detections and more than 15,000 wells for each month, linear scanning each well’s location for each VIIRS detection is too slow. Therefore, in this work, the function internally depends on a ball tree implementation from scikit-learn (Pedregosa et al. 2011) for speedup on the k-NN search. Once the 2-tuple, (OPj, dj), is obtained for each VIIRS detection, some logics are implemented to decide whether to drop or keep the operator assignment. The idea is straightforward: the assignment is immediately kept or discarded, when dj is very small or very large, respectively. If dj is mid-range, i.e., dsecure ≤ dj ≤ dcutoff, the assignment will be in effect, only if the flare and the operator are found to be located on the same township/range/section. The township/range/section shapefiles, as part of the input for Algorithm 7.1, are available from the NDIC GIS Map Server. The reverse geocoding follows the exact same procedure as in Section 3.3. After the processing is completed, a small portion of the VIIRS detections are not used for operator level analytics, because either they are too far away from the reported well locations, or the townships/ranges/sections fail to match. It should be noted that, the pseudocode for Algorithm 7.1 is written in a way that illustrates the precise details in the data processing logics. For the implementation in this work, some of the for-loops are replaced by the vectorized operations for enhanced performance. 103
Algorithm 7.1: Nearest-Neighbor-Based Flare Owner Assignment
Input: both VIIRS and NDIC reportings in WGS 84 coordinates, the township/range/section shapefiles for North Dakota, dsecure, dcutoff, RE Output: operators being assigned to most VIIRS detections
1 n← number of months 2 for i← 1 to n do 3 VIIRSi ← the i-th month’s observations from VIIRS 4 NDICi ← the i-th month’s reportings from NDIC 5 (OP, d)← FindClosestOperator(VIIRSi, NDICi, RE) 6 m← number of records in OP or d 7 for j ← 1 to m do 8 OPj ← the closest operator found on the j-th record 9 dj ← the distance between the flare and the closest well, for the j-th record
10 if dj > dcutoff then 11 drop OPj 12 else if dj < dsecure then 13 keep OPj 14 else 15 if township/range/section agree then 16 keep OPj 17 else 18 drop OPj 19 end 20 end 21 end 22 end
The developed approach is tested with real flaring data from North Dakota. For the
demonstrated cases in this section, the values below are chosen for Algorithm 7.1:
dsecure = 300 m (7.1a) dcutoff = 800 m (7.1b)
RE = 6371 km (7.1c)
Some operators are found to show positive correlations between the NDIC and VIIRS reported volumes. Examples of two operators, denoted by Operator B and Operator C, are shown in Figure 7.1. The axes’ meanings are the same as in the right panel of Figure 3.7. The
104
legend shows the results of fitting Equation 3.2d by ordinary least squares (OLS). R2adj stands for the adjusted R2. Although the differences in β̂operator indicate that there is heterogeneity among the different companies, these operators show some consistency in terms of their own reporting and have good matches with the VIIRS data up to a scale factor (as the intercepts are very close to zero). However, some operators (e.g., Operator D and Operator E) show discrepancies between their reportings and the satellite-detected flaring statistics, which are manifested through the poor fits (Figure 7.2). Certainly, a poor fit with the linear model does not indicate much on its own. Nonetheless, there exists a pattern in both scatterplots that, some points seem to be “pushed down” towards the x-axis. If the time series of these two operators are drawn, it shows that this behavior is due to company-reported volumes leveling off for a certain period of time (Figure 7.3). The VIIRS curves in the time series imply that there were flaring intensity variations for those times. This workflow, driven by Algorithm 7.1, is capable of raising a flag when it comes across datasets like these, and can serve as a powerful monitoring
105
and analytics tool, however, strong cautions need to be applied. The introduced approach, although it looks promising, is by no means a one-stop solution and has the potential for being misapplied. First, there is the possibility of misassigning the satellite-detected flares to the operators. Whenever the concern is raised, further investigations can be conducted by looking into the detection maps as well as the satellite imagery of the operators’ production sites. In addition, this method is more effective for the relatively large producing/flaring operators, because when a company conducts very little flaring, the truncation effects discussed for the peak in Figure 5.20 are magnified. 7.2 warnings regarding inconsistencies Given the resolution of the satellite imagery, assigning specific flaring volumes to a given operator is fraught with challenges. Although the VIIRS processing workflow is capable of picking up flares with areas around 1 m2 (Figure 3.2(a)), the pixel footprint is much larger (Table 2.1). Since the latitude and longitude of the pixel center is stored for each individual VIIRS observation (Elvidge et al. 2015), when multiple operators have sub-pixel combustion
106
sources, it makes flare owner assignment extremely challenging. In such situations, conclusions reached by merely benchmarking company reporting against VIIRS reporting would likely be inaccurate. In fact, in the realm of NDIC reporting, warnings must be issued regarding any inconsistencies in those results, with considerations from three aspects. First, the report from the U.S. Department of Energy (2019a) presents data supporting that North Dakota
107
shows closer agreement between the NOAA estimations and state reportings (of flared gas volumes), when compared with Texas and New Mexico. Second, flaring is preferred over venting because methane (the main component of natural gas) is more potent than carbon dioxide which is the main product of flaring (EIA 2019b). Since North Dakota bans venting, the massive flaring magnitude indicates that the direct release of gas into the atmosphere is minimized. Third, estimation of flaring volumes is inherently a difficult task. When it is not practicable to meter the flared gas, the Canadian Association of Petroleum Producers (2002) gives guidelines on available volume estimation methods. Every category of methods, no matter using rules of thumb, or experimentally determined correlations, or process simulators, has its own limitations and accuracy issues. Considering the fact that the VIIRS volumes used in this work were largely calibrated using the Cedigaz reported data (Section 2.1), which has its own error bars (Elvidge et al. 2015), the difference between company reporting and VIIRS reporting is inconclusive and unsurprising, especially when the standard error of the difference is larger than the difference itself. By inspecting a more comprehensive profile of time series, both Operator D and Operator E from the previous section are self-consistent in their reportings to the NDIC. Their time series are displayed in Figure 7.4 and Figure 7.5, respectively. The variables and associated labels (shown in the legends) follow the same definitions from Section 3.4. The units for all the variables are given in Table 7.1. Clearly, the reported flared volumes show good correspondence with the gas production and GOR profiles. Some rapid variations in their flared volumes match the fluctuations in the gas prices, i.e., when the gas price drops, the operators tend to flare more, whereas when the gas price reaches peak, there is little flaring. In summary, to nail down the decisions and conclusions with regard to operator reporting quality, better resolution satellite data and a more comprehensive review of the time series profiles are required. 108
109
110 7.3 caveats in petroleum data analytics As a petroleum engineer, the author is thrilled to witness the oil and gas industry and academia are embracing data-driven mindsets and solutions, while being part of it through writing this dissertation. However, there are certainly areas that could be continuously improved, and this section provides a discussion on one of those. That is, extending a cautious welcome to some black box models. The pervasive influence of some black box models in the recent years can be seen by performing a rough search on OnePetro (Table 7.2). One thing to note is that, from an algorithmic point of view, these methods are rather “glass boxes” as opposed to “black boxes”, i.e., everything under the hood in terms of implementation is well understood. For example, backpropagation, which is the core of neural network training, is based on the chain rule. However, for a given task, the learned parameters inside the network provide little or no insights for the problem domain. Therefore, it is considered a black box. The wide adoption of such models is largely due to the availability of the open source libraries, for example in the Python ecosystem, construction and training of neural networks become much simpler thanks to TensorFlow and PyTorch, and gradient boosting models can be built within a few lines of code with the help of XGBoost, LightGBM, or CatBoost. In other words, with the mathematical details of those statistical routines abstracted away, for a practitioner, implementing those models is almost as easy as pushing a Learning button on
111
Table 7.2: Publication Count Rise on OnePetro
Exact Phrase Searched
Year Method Introduced
Publication Count
2010–2014 2015–2019
neural network 1958† 843 2044 gradient boosting 2001‡ 1 110 random forest 2001§ 9 245
† Based on (Rosenblatt 1958) ‡ Based on (Friedman 2001) § Based on (Breiman 2001)
a GUI. Unfortunately, easiness in the implementation does not imply appropriateness for the
problem. In particular, those black box models face the challenges below:
1. How to incorporate domain expertise. A lot of the black box models in the frequentist framework make the assumption that the observations are conditionally i.i.d. The hope is that by feeding a huge number of i.i.d. samples to a universal approximator, such as a neural network, some function for prediction can be optimized with a certain accuracy. For some applications, the domain expertise is often encoded in the feature selection process. For example, to train a model to predict oil production, the analyst might choose some completion parameters other than the API well number or well name, as input features. However, in the author’s opinion, this way of incorporating domain expertise is still a shallow one, which is far from what the oil and gas industry have accumulated in many decades. For example, the phenomena of well interference through fracture hits leave the assumption of some neighboring wells being i.i.d. in an unfavorable position. Another example would be, when looking at a populations of wells from one basin that are completed by N oilfield service companies, domain expertise might indicate that, each company deserves its own model while each company is not completely
112
independent from others in terms of the completion technologies, etc. In this situation, the hierarchical model employed in Chapter 4 might be a better choice, in which case a lot of the prior knowledge about the different service companies can be incorporated into the population model. 2. How to interpret the results. As discussed earlier, the black box models suffer from the interpretability issues. Using the shale gas wells example from Item 1 above, if a black box model is trained, it is impossible (at this point) to attribute the failure in capturing the well interference effects to a certain part of the neural network, or to a certain portion of the decision trees (in the case of gradient boosted trees or random forest). Rudin (2019) asserted that people should “stop explaining black box machine learning models” and use interpretable models for high-stakes decisions. In the petroleum industry, there are a number of high-stakes decision scenarios, such as real-time well integrity anomaly detection and production forecasting in a high well cost context. Blindly applying black box models to those scenarios might involve serious losses. In terms of providing interpretability, the Bayesian approach employed throughout this dissertation is much more effective. Each and every assumption is expressed in the generative model through either the priors or the likelihood. How to quantify the uncertainties, especially in the context of risk management and
decision making. Along the lines of Item 2 above, error bars are vital, especially in high-stakes prediction applications. In the case of predicting oil production using a trained data-driven model, point prediction results such as 1000 bbl/day are not really insightful. In fact, if the 95 % prediction interval (PI) is 1000± 50 bbl/day, that point prediction becomes more informative. However, if the 95 % PI is 1000± 1500 bbl/day, that same point prediction is unhelpful or misleading. What shall be reported instead is either the considered
113
model yields much uncertainty in this given task, or there is possibility that the entity will not produce anything at all. It should be noted that, the ‘95’ in the CI/PI is not a “magic number”. A state government or an oil company might want to make decisions based on 73 % or 99.6 % confidence, or any other arbitrary choices. What really matters is the necessity of a principled way to quantify the uncertainties in machine learning-based estimations/predictions, such that any intervals can be computed. As presented throughout this dissertation, the Bayesian approach provides full capacity and flexibility is this regard. In fact, for parameter estimates, the author chooses to give 90 % CI instead of the “conventional” 95 %, to emphasize that this should be a domain’s consideration rather than a statistical one. A lot of the black box models in the frequentist framework, however, fall short of this requirement. Maximum likelihood estimation (MLE), which is fundamentally relied upon by some frequentist learning methods, enjoys really nice properties and is capable of quantifying uncertainties, but only when a massive amount of data is at hand such that the asymptotic properties could take effect. Unfortunately, that is not the case in many scenarios for the petroleum engineering domain, which is discussed next. How to mitigate overfitting when the data is not “big”. Two aspects are worth discussing here. For one thing, the big data is not everywhere. Indeed, the author believes that the claim of Gelman (2015) that, “sample sizes are never large”, applies to a lot of problems in the petroleum industry. The reason is that, if the data were large, the analyst would already be on to the next problem for which more data is needed. For example, a sample of 500 producing wells in the Bakken Formation could make some general study possible. When the analyst has access to a dataset of more than 15,000 wells, some granular insights are desirable. Especially, if partial pooling is needed among the different service companies/operators, different
114
members of the formation, or different completion technologies, data for some units of the population could be very small (which happens for the analysis in Chapter 4). On the other hand, the sample size should be inspected in the light of model complexity. The number of parameters provides one measure of such. For example, consider a hypothetical classification problem, whose goal is to determine if a given well will deliver good or average or poor production performance. Ten completion parameters (features) are available to train the multilayer perceptron illustrated in Figure 7.6. In this (small) neural network, the number of parameters np is given by:
np = 11× 20 + 21× 10 + 11× 3 = 463, (7.2)
when considering a single bias node for every layer except the last one. To train this model, a dataset of 500 wells would definitely be a small sample. There is still possibility to train such a model with a small sample, however, great efforts in regularization have to be made, in the hope that the neural network will learn something that can be generalized, instead of merely memorizing the observed samples (i.e., overfitting). 115
By utilizing the regularizing priors, the Bayesian approach’s built-in Occam’s razor greatly mitigate the risk of overfitting. In particular, Bayesian nonparametric models, such as the Gaussian processes employed in Chapter 5, are very attractive in a sense that the sizes of models are allowed to grow with the size of data (Orbanz and Teh 2010). This makes the developed model flexible while being robust to overfitting. Although the Bayesian learning models (such as the ones developed in this work) have outstanding merits and deserve wider utilization in petroleum data analytics, they are not cure-alls. Recently researchers have started to stress the necessity of bespoke statistical models (Andorra 2020; McElreath 2020). The argument is that, off-the-shelf models, no matter neural networks or generalized linear models, interrupt the incorporation of domain expertise. This is especially relevant in the field of petroleum engineering. For instance, when conducting data-driven analysis for hydraulic fracturing performance, it makes sense to bring in the fracture propagation models to the machine learning workflow. That way, statistical models are motivated by the physically informed models. The Bayesian framework, as employed throughout this dissertation, readily embraces this strategy, in that the domain knowledge, which is represented by differential equations for example, can be inserted into the generative model. One advantage is that a lot of the parameters will have direct scientific meanings, and more informative priors can be placed based on scientific constraints, field experience, etc. The final outcome should be better inferences and predictions. 116
CHAPTER 8
CONCLUSIONS AND RECOMMENDATIONS
In this dissertation, the effectiveness of a full Bayesian approach has been observed in learning models from natural gas flaring data. The author hopes this work contributes to the understanding of the options and considerations when applying data-driven approaches to gas flaring. In closing, this chapter presents the major conclusions and recommendations for future work. 8.1 conclusions The major conclusions are:
1. Bayesian learning implemented using Hamiltonian Monte Carlo can be effectively
applied to real problems in gas flaring analytics, in both supervised and unsupervised settings. The advantages of the Bayesian approach indicate it deserves wider usage in the petroleum engineering domain in general; these advantages are listed below:
(a) Petrotechnical domain expertise can be incorporated in a principled way. (b) Model interpretability is drastically improved, facilitating communications with
petroleum engineers. (c) Quantification of uncertainty leads to more robust decision making, which is
important for oil exploration and production companies. (d) The built-in Occam’s razor makes the model less prone to overfitting, in the
context of noisy field measurements. 2. The development of a suite of models (Table 8.1), with both parametric and nonpara-
metric techniques, provides guidance on how insights can be extracted from various
117
angles. The presented models are designed and tested to be able to generalize to different entities at various levels. To investigate the heterogeneity among the different entities (such as counties or
oilfields), partial pooling is recommended, because some entities have very little data. Gaussian processes demonstrate very attractive traits in revealing the patterns and
trends from flaring time series. A set of priors with the Matérn 5/2 kernel works very well across different modeling goals, observation models, and data sources. From a distributional point of view, the negative binomial and Gaussian mixture models
are good representations of the oilfield flare counts and flared volumes, respectively. The learned parameters and structures are very interpretable. Hidden clusters are found by fitting Gaussian mixture models. 6. A nearest-neighbor-based approach for operator level monitoring and analytics is
introduced. Its performance is tested on real data and defendable results are obtained. However, better resolution satellite data is needed for the scenario of multiple operators’ wells being very close to each other. 7. All the dissertation objectives (Section 1.2) have been achieved. In particular, the flared
volumes missed from VIIRS for the state and each county are estimated via fitting the intercept parameter and reported in Table 3.1 and Table 4.2. The nighttime combustion source detection limits of Landsat 8, without being corrected for artifacts due to glow, are determined and reported in Figure 3.2(b). Correlations between financial factors, production performance, and flared volumes at a state level are computed using Spearman’s ρ and reported in Figure 3.5 and Figure 3.6 for the original data and lag-1 differences, respectively. Most pairs of the variables do not show strong correlations on the lag-1 differences. Robust Gaussian process modeling serves as a generic framework for addressing the rest of the objectives, including demonstrating operator approaches,
118
evaluating if the goals of the North Dakota regulatory policy (Order 24665) have been achieved, and predicting NDIC flared volumes. 8.2 future work 3. The models in Chapter 5 are learned from each entity’s own data. Can pooling across different entities via hierarchical Gaussian processes improve the inferences? One step further from Item 3 above, the efficacy of spatial-temporal models (which allow for pooling information across time and space) are worth investigating. Are neighboring entities exhibiting close resemblance in flaring behaviors? The model comparison for GMMs in Chapter 6 depends on specifying the potential numbers of clusters a priori. In fact, Dirichlet process, as an infinite-dimensional generalization of the Dirichlet distribution, is nonparametric and allows for automatically choosing the number of necessary clusters. 120