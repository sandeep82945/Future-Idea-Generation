In this paper, we present a simple yet effective semi-supervised 3D object detector named DDS3D. Our main contributions have two-fold. On the one hand, different from previous works using Non-Maximal Suppression (NMS) or its variants for obtaining the sparse pseudo labels, we propose a dense pseudo-label generation strategy to get dense pseudolabels, which can retain more potential supervision information for the student network. On the other hand, instead of traditional fixed thresholds, we propose a dynamic threshold manner to generate pseudo-labels, which can guarantee the quality and quantity of pseudo-labels during the whole training process. Benefiting from these two components, our DDS3D outperforms the state-of-the-art semi-supervised 3d object detection with mAP of 3.1% on the pedestrian and 2.1% on the cyclist under the same configuration of 1% samples. Extensive ablation studies on the KITTI dataset demonstrate the effectiveness of our DDS3D. The code and models will be made publicly available at https://github.com/hust-jy/DDS3D ii. related work 3D Object Detection In the past few years, there have been numerous detection methods to deal with 3D object detection [4], [24], [7], [16], [17]. In terms of point cloud representations: point-based [31], [42], [46], [49], [13] and voxel-based [52], [3], [41]. Point-based methods directly consume irregular point clouds to extract features. PointRCNN [31] adopts PointNet++ [23] as the backbone to process point clouds, binary classification to obtain foreground points and regression to generate proposals on obtained foreground points, then the proposals are refined in the second stage with the semantic and spatial information. Voxel-based methods adopt voxelization to make point clouds regular, making it possible to extract features using asymmetric functions, e.g., convolution. VoxelNet [52] adopts 3D convolution to extract features from regular voxels. SECOND [41] replaces the conventional 3D convolution with a 3D submanifold sparse convolution to speed up. TANet [18] adopts a triplet attention module to make the detection model more robust by considering the feature-wise relationship. CenterPoint [44] implements the center-based method on 3D Object Detection, which considers objects as key points at the heatmap derived from the bird’s-eye-view (BEV) feature. Although the Voxel-based methods are more computationally efficient, the inevitable information loss degrades the fine-grained localization accuracy. Therefore, some methods consider how to combine point clouds and voxels. PV-RCNN [30] proposes the Voxel Set Abstraction (VSA) that integrates multi-scale voxel features into key points, and then the keypoint features are aggregated to the RoI-grid points to learn proposalspecific features for fine-grained proposal refinement and confidence prediction. Semi-Supervised Learning (SSL) Compared to supervised learning, SSL method can only use a small amount of data, which increases the difficulty of the task. Previous works are mainly divided into two categories of methods, consistency regularization [29], [28], [36], [25], [35] and pseudo-labeling [9], [2], [32]. Temporal Ensembling [29] first proposes consistency regularization and many SSL methods [2], [1] leverage consistency regularization. Mean Teacher [35] takes the teacher model as the exponential moving average (EMA) of the student model and then adopts the consistent regularization to enforce the predictions
on unlabeled data to be consistent under different data augmentations. Others adopt pseudo-labeling [9], [2], [32], [6], [45], which is another popular method of SSL that can be treated as a variant of consistent regularization. MixMatch [2] uses a series of data augmentations and applies consistency regularization on unlabeled data. FixMatch [32] sets a confidence threshold to filter the low quality pseudolabels. Some SSL methods [28], [29], [32] believe that data augmentations are very important to SSL. SSL for 2D and 3D Object Detection Recently, there has been lots of works [14], [50], [40], [33], [10], [15], [34], [51], [11], [27] in 2D Semi-Supervised Object Detection (SS-OD). Previous works have transferred a great deal of experience from SSL works to the SS-OD domain. STAC [33] uses Faster-RCNN [26] as its detector and trains the teacher model with the labeled data and generates pseudo-labels on unlabeled data as a static teacher. But with the accuracy of the student model, improving static pseudo-labels might lead to the opposite effect. Unbiased Teacher [14] solves the pseudo-labels bias problem caused by the class imbalance in real labels and the overfitting problem caused by the lack of labeled data. Unbiased Teacher v2 [15] aims at solving the ineffectiveness of the default regression supervision in semi-supervised, focusing on optimization by predicting the uncertainty of the boundary and achieving high performance under both anchor-based and anchor-free frameworks. Soft Teacher [40] proposes a soft teacher and box jitter mechanism, the former can directly assess all the box candidates from the student model, and the latter can select accurate pseudo boxes for the unlabeled regression. Dense Teacher [50] proposes a united form of pseudolabels named DPL to fit the semi-supervised setting better. PseCo [10] proposes multi-scale feature alignment, which can be regarded as a kind of data augmentation. As for 3D Semi-Supervised Object Detection [48], [38], [20], [47], [43], [39], [12], SESS [48] is the pioneer in applying the SSL framework to point-based 3D objection detection. It uses an EMA teacher and a student on top of VoteNet [21], asymmetric data augmentations, and three kinds of consistency losses between the predictions of the teacher and the student. 3DIoUMatch [38] proposes a multiple threshold filter strategy based on the SESS [48]. ATF-3D [47] proposes adaptive thresholds based on distance and confidence. DetMatch [20] jointly leverages the information of RGB images and point clouds with the Hungarian Matching algorithm [8] to get higher performance. Compared with the above methods, our DDS3D does not require additional image information. iii. method As shown in Fig. 2, we present the proposed framework of DDS3D, which contains three components: a) TeacherStudent network, b) dense pseudo-label generation, c) dynamic threshold selection. Before introducing the technical details of our DDS3D, we first provide the basic definitions for semi-supervised 3D object detection. In semi-supervised 3D object detection, the total dataset includes a small part of labeled data { xli ,y l i }Nl i=1 and a large amount of unlabeled
data {xui } Nu i=1, where Nl and Nu are the number of labeled and unlabeled data, respectively. xli and y l i represent the input point cloud data and the corresponding ground truth annotations. a. framework of dds3d 1) Teacher-student Network: Our DDS3D employs a teacher-student framework, where both the teacher network and the student network use the same 3D detector PVRCNN [30] except for weight parameters and asymmetric data augmentation. More concretely, the teacher first feeds the unlabeled data with weak augmentation into the trained detector to produce the pseudo-labels, which are then utilized to supervise the student network. To ensure the effectiveness of semi-supervised learning, the pseudo-labels generated by the teacher network in the training stage are usually more accurate than the predictions of the student network. To this end, we adopt the EMA strategy during the training process. θT = αθT +(1−α)θS (1)
where α is the EMA momentum and θT and θS are the teacher and student model parameters respectively. b. dense pseudo label generation For obtaining pseudo labels, the previous methods [38], [48], [33] usually introduce NMS operation to remove redundant boxes and obtain high-quality pseudo labels. However, these approaches might be sub-optimal through employing NMS to deduplicate teacher predictions since some beneficial boxes may be removed in this process. The reason for this is the inconsistency of the classification scores and the quality of regressed boxes. Fig. 3 (a) and (b) illustrate the relationship among the classification score, IoU prediction
score and ground-truth 3D IoU in our teacher network (PVRCNN). Although the IoU prediction is more reasonable than the classification score prediction, the prediction is still unsatisfactory compared with the ground truth. Thus, directly adopting pseudo-labels to supervise this IoU estimation branch will lead to inaccurate estimation and poor performance. (c) Teacher Predictions (d) after NMS (e) Dense Pseudo Label Generation
(a) (b)
Fig. 3. Comparison of classification confidence and IoU estimation with true 3D IoU on KITTI validation set, PV-RCNN is trained with 1% labeled data; Pictorial illustration of NMS and Dense Pseudo Label Generation. To alleviate this problem, we propose a simple and effective dense pseudo-label generation strategy. More concretely, we retain all proposals instead of filtering out many redundant boxes by NMS operation. As a result, a ground-truth object might be detected by multiple proposals in this setting,
which naturally improves the recall of detection to provide more potential supervision information than these sparse pseudo-labels obtained from NMS operation. To illustrate this process better, we provide the visualization in Fig. 3, where (c) represents the proposals from the teacher. When NMS is used to process these proposals shown in Fig. 3 (d), this does not guarantee the generation of a high-quality box, which might lead to an unsatisfactory result. However, our dense pseudo label generation (see Fig. 3 (e)) can capture dense boxes to provide more potential information. c. dynamic threshold selection The main gap in detection performance between these two networks in the teacher-student framework is from the different EMA weights and the data augmentation strength. Although the teacher network is usually more powerful than the student network, this does not guarantee that the teacher’s prediction is always more accurate than the student’s. A naive method is to use a fixed threshold to filter out lowquality pseudo-labels from the teacher network. As shown in Fig. 4, the threshold is set at a high value (e.g., 0.7 and 0.9), leading to more false negative examples. Conversely, when the threshold is set at a low value (e.g., 0.1 and 0.3), the performance of the model drop drastically due to a large number of false positive. To avoid this phenomenon, different from the fixed threshold methods [38], [32], we propose a dynamic threshold strategy. Specifically, we set a higher threshold to filter out most false positives to ensure the accuracy of the initial optimization direction. Then, as the number of iterations increases, we gradually reduce the threshold to retain more potential true positives due to the stronger detection performance. Especially at the end of the training, the model usually performs better on object localization. Therefore, a lower threshold is reasonable to cover more hard objects to further boost the performance on these challenging objects. Finally, the process of the dynamic threshold selection can be formulated as:
σcls(t) = min ( σstart −α× ⌊
t steps
⌋ ,σend ) where σcls(t) is the threshold for classification confidence at the number of iterations t, σstart is the starting threshold, σend is the end threshold, steps is step length and α is the attenuation coefficient default as 0.1. d. loss function and final processing of pseudo-label In the pre-training stage, we can use a small amount of labeled data { xli ,y l i }Nl
i=1 to train the student network. The total loss in this stage Ll is composed of RPN losses and RCNN losses, as:
Ll = Llrpn cls +L l rpn reg +L l rcnn iou +L l rcnn reg (4)
where Llrpn cls is classification loss, L l rcnn iou is IoU estimation loss, Llrpn reg and L l rcnn reg are box regression losses. In the semi-supervised training stage, we keep the same proportion for the input labeled data and unlabeled data on each batch. For labeled data, we supervise the student with
GT (same with the pre-training stage). But for unlabeled data, given that the asymmetric data augmentation on unlabeled data for teacher and student, pseudo-labels need to go through additional geometry transformation T to enable the alignment with outputs of the student network. T is equal to the multiplication of the inverse weak augmentation and strong augmentation. Considering Iou is hard to optimize over the network, we remove the IoU estimation branch in the semi-supervised training stage. Thus, the total unsupervised loss Lu is composed of classification loss and box regression losses, which can be computed as follows. Lu = Lurpn cls +L u rpn reg +L u rcnn reg (5). Finally, the total loss L for semi-supervising framework is as follows. L = Ll +λLu (6)
Where λ is the balance weight of the unsupervised loss. iv. experiments  a. experimental setup Dataset and Evaluation Metrics. KITTI [5] dataset is a common dataset for autonomous driving, which contains 7481 training samples and 7518 testing samples. Following F-PointNet [22], the training samples are further divided into a train split (3712 samples) and a val split (3769 samples). For a fair comparison, We select the 1% and 2% labeled samples from train split following [38] to verify the effectiveness of our method. The mean Average Precision (mAP) with 40 recall positions is regarded as our evaluation metric. The IoU threshold for cars, pedestrians, and cyclists is set as 0.7, 0.5, and 0.5, respectively. Implementation Details. We implement the basic detector PV-RCNN [30] based on the open-source framework OpenPCDet [37] codebase. In detail, the detection range is within [0, 70.4], [-40, 40], [-3, 1] meters along the X, Y, Z axes and the voxel size is (0.05, 0.05, 0.1) meters. The training process contains a pre-training stage and a training stage, and we train all models on 4 NVIDIA RTX 2080Ti GPUs. short for pedestrian and cyclist. * denotes the reproduced results. For the pre-training stage, we use the labeled data to train our model for 80 epochs with batch size of 8 (default 2 samples per GPU), and we train the data ten times per epoch so that the model converges better. The detector is optimized by AdamW [19] optimizer with a max learning rate of 0.01. For the training stage, we run 100 epochs with batch size of 8 for 4 GPUs (each GPU loads 1 labeled sample and 1 unlabeled sample in each batch). Besides, we lengthen the number of traverses in each epoch to five times the origin following [38]. Similar to prior works [48], [38], we warm up the EMA momentum from 0.99 to 0.999. For our Dynamic Threshold Strategy, we set σstart to 0.6, σend to 0.4 and steps to 1000. Further, for a fair comparison, we adopt the same data augmentations as 3DIoUMatch, including the GT Sampling for labeled data and basic geometric transformations for unlabeled data. The basic geometric involves random flip along the X axis, random global scaling with a scale factor sampled from [0.95,1.05], and global rotation around Z axis with a random angle sampled from [ −π4 ,+ π 4 ] . b. results on kitti As shown in Table I, we provide a comparison with the superior semi-supervised 3D object detection method 3DIoUMatch under the settings of 1% and 2% labeled data on the KITTI [5] val split. For a fair comparison, we use the same detector PV-RCNN [30] as the labeleddata-only baseline. In Table I, our method outperforms the labeled-data-only baseline by 2.7%, 5.8% and 8.0% on cars, pedestrians and cyclists under 1% labeled data. Besides, our DDS3D achieves 3.1% and 2.1% mAP improvement over 3DIoUMatch [38] on pedestrians and cyclists, which illustrates the superiority of our DDS3D by considering the dense pseudo-label generation and the dynamic threshold strategies. Similar conclusions for 2% of the labeled data. For cars, our DDS3D achieves similar performance to 3DIoUMatch. The reason behind is that the detector on the category of the car has already achieved satisfactory results in the pre-training stage, which is difficult to improve by pseudo-labels in the semi-supervised framework. c. ablation study We present ablation studies with 1% labeled data to analyze the effectiveness of our proposed components in DDS3D the ablation of the improvement of each component on the  learning, fixed threshold, dynamic threshold and dense  pedestrian and cyclist. on KITTI [5] val split. Table II summarizes the ablation results on our dense pseudo-label generation mode (DPLG) and dynamic threshold module (DT). We adopt the labeleddata-only PV-RCNN as the baseline (Exp (a) of Table II). To validating the effectiveness of the dynamic threshold filter, we add a fixed threshold (FT) manner as a comparison. SSL stands for paradigm using semi-supervised learning. Effect of dense pseudo label generation module. Compared with the naive pseudo-label baseline with a fixed threshold (Exp (c) of Table II), our dense pseudo-label generation (Exp (e) of Table II) achieves 0.4%, 1.1% and 1.4% mAP on car, pedestrian and cyclist, respectively. The main reason is that some boxes with high IoU with GT but low scores are filtered out by NMS as redundant boxes. Our dense pseudo-label generation mode can effectively deal with this case. Effect of dynamic threshold. For a fair comparison, we extensively search for the fixed confidence threshold, as shown in Table III. It is worth noting that different optimal thresholds might be required for different object classes. To reduce the number of hyperparameters, we use the same threshold to filter all categories. As shown in Table III, setting the fixed threshold to 0.4 is a reliable choice. Thus, we choose the value of 0.4 as the fixed threshold Table II. Compared with our baseline with the fixed threshold Table II (exp (c)), our dynamic threshold strategy shown in Table II (exp (d)) achieves 0.4% mAP improvement on cars and 0.9% mAP improvement on pedestrians. Moreover, our dynamic threshold strategy with dense pseudo label generation brings a significant improvement to the fixed threshold, which achieves 0.4% mAP improvement on pedestrians and 2.5% mAP improvement on cyclists. The naive pseudo-label baseline proves that directly using teachers’ proposals as pseudo-labels to supervise the student model gets low-performance improvement. On the other hand, it is proven that supervision with high quality and sufficient pseudo-labels is necessary. Therefore, it demonstrates the effectiveness of our dynamic threshold and more details will be discussed in later sections. Effect of different threshold strategies Table IV shows the performance of our dynamic threshold strategy under different settings, and all experiments are trained for 60 the ablation of the fixed and dynamic threshold. the ablation of the range of the dynamic threshold (dt). epochs with steps as 500. We design some different start and end of the dynamic threshold strategy and different threshold trends: high-to-low (our dynamic strategy) and low-to-high. According to the result, our dynamic threshold strategy always works well no matter how we choose the start and end. Moreover, we proposed high-to-low dynamic threshold strategy brings effective improvement, but the lowto-high strategy drops the performance, which demonstrates the effectiveness of our proposed dynamic threshold strategy and our hypothesis about the SSL that in the early training stage, the teacher needs a higher threshold to filter out wrong predictions that are going to be pseudo-labels, which ensures the accuracy of the initial optimization direction, and in the latter training stage, the teacher needs lower threshold to keep more predictions, which covers more hard objects so as to further boost the performance on these challenging objects. d. qualitative results and analysis Fig. 5 shows the visualizations of the predictions by PV-RCNN [30], 3DIoUMatch [38], and DDS3D with 2% labeled data on the KITTI [5] dataset the bird’s-eye view. As shown in Fig. 5, (a) generates the pseudo labels from the teacher network PV-RCNN before NMS operation. (b) shows the sparse predictions from 3DIoUMatch through filtering out (a) in NMS operation, whose high-quality objects are obtained by the score ranking in NMS. Thus, some highquality but low-score boxes may be eliminated due to the inconsistency between the confidence scores and the quality of the regressed boxes. (c) represents our dense pseudo-label generation strategy, which keeps more high-quality boxes compared to sparse pseudo-labels. (d) shows the final pseudo labels produced by our DDS3D, where all objects can be accurately localized. This effectively illustrates our dense pseudo labels are more reliable than traditional sparse pseudo labels. v. conclusion In this paper, we have presented a novel semi-supervised 3D object detection framework named DDS3D, which involves a dense pseudo-label generation mode and a dynamic threshold strategy. The dense pseudo label generation can retain more beneficial pseudo labels compared with the manner of filtering out a large number of redundant pseudo labels through NMS operation. Besides, the proposed dynamic threshold strategy can effectively adjust a proper threshold to cover more reliable pseudo labels, which is essential for our teacher-student semi-supervised framework. Finally, DDS3D outperforms the state-of-the-art method on 1% labeled data under the same settings, and the experiment results on the KITTI dataset validate the effectiveness of these two components in our DDS3D.