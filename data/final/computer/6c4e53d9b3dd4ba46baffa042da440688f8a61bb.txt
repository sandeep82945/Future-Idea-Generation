Autonomous driving has now made great strides thanks to artificial intelligence, and numerous advanced methods have been proposed for vehicle end target detection, including single sensor or multi-sensor detection methods. However, the complexity and diversity of real traffic situations necessitate an examination of how to use these methods in real road conditions. In this paper, we propose RMMDet, a road-side multitype and multigroup sensor detection system for autonomous driving. We use a ROS-based virtual environment to simulate real-world conditions, in particular the physical and functional construction of the sensors. Then we implement muti-type sensor detection and multi-group sensors fusion in this environment, including camera-radar and camera-lidar detection based on result-level fusion. We produce local datasets and real sand table field, and conduct various experiments. Furthermore, we link a multi-agent collaborative scheduling system to the fusion detection system. Hence, the whole roadside detection system is formed by roadside perception, fusion detection, and scheduling planning. Through the experiments, it can be seen that RMMDet system we built plays an important role in vehicle-road collaboration and its optimization. The code and supplementary materials can be found at: https://github.com/OrangeSodahub/RMMDet ii. simulation environment ROS [21] is used to construct a 1:18 scale simulation model in this paper. We first build models in SolidWorks based on accurate scaling and component connection and import them into ROS through URDF files. Completing the sensor function configuration, finally, it can achieve the information acquisition and transmission of various sensors. a. traffic environment We design a rectangular field of 8m × 10m (width and length) which takes the actual situation into full consideration, including viaduct, ring road, intersection, straight road, turning road, and other common elements. As shown in Figure 2, among them, the detection system is deployed in the circular section, intersection, and viaduct ramps. Considering the traffic flow around the ring road and the intersection is relatively complex and heavy, we set four monocular cameras positioned 90° apart in four directions, and one 32-line lidar centrally suspended for the circular section, two 16-line lidars diagonally located for the intersection, where every four cameras could cover 360° with additional overlaps. At the viaduct sections, vehicles are usually aligned with long lines, not particularly clumpy or clustered distribution, and the directions are not irregularly staggered at multiple angles, where millimeter wave radar plays a greater role in measuring the speed, position, and other information of the target accurately. Therefore, we set the camera-radar detection system, which is also suitable for others with similar characteristics such as the highways [18]. In addition, we also set up the driving environment of the real vehicle (See Experiments Sec.VI) to validate our scheme in real world. b. sensors: camera, radar and lidar Bodies of sensors are modeled according to their lifesizes and the scale. Then they are functionally configured to capture and transmit massive data, specifically using
the camera plug-in in ROS, Robosense lidar plug-in1 and millimeter wave radar plug-in2 for the image acquisition, point cloud generation and radar points acquisition. Table I shows the attributes of sensors and their data stream. ”FPS” and ”Shape” are both in the simulation environment running on CPU intel i7 and GPU RTX3080. iii. overall framework Figure 1 illustrates the main framework of the whole system with three parts: sensor layer, fusion perception layer, and decision planning layer. The sensor layer contains physical devices of three types of sensors which have been constructed in Sensors (Sec. II-B). The fusion detection layer contains multi-type sensor target detection and multi-group sensor fusion detection. After preprocessing the raw data obtained from the sensors, feed them into the separate branches of image detection based on cameras, 2D point cloud detection based on the radars, and 3D point cloud detection based on lidars. This operation could be performed in the respective mobile edge computing platform (MEC). Then different fusion strategies enhance the detection and output final results, including cameraradar group and camera-lidar group. Additionally, the unique information participated in fusion, such as target velocity detected by millimeter wave radar, etc, will also be hold in outputs. Multi-agent Collaborative system, a lightweight module for downstream tasks, is realized in the Decision planning layer.Based on different decision-making agents divided according to the traffic function or spatial-level characteristics, each body detects within its area respectively and carries out the information transmission among the vehicles and bodies. iv. rmmdet: road-side multitype and multigroup sensor detection system  a. coordinate system Four coordinate systems are depicted in Fig.3: camera(pixel), radar, lidar, world coordinates.The subsequent steps involve the transformations of these four coordinate systems to ensure the unity of spatial representation of different data, which can be summarized as following equations:
Chw = [ R T 0 1 ] Chc,r,l (1)
Zc ·Chu,v = PK ·Chc = PKE ·Chw (2)
In eq.1, three types of subscripts of homogeneous coordinates Ch{·} denote the pixel frame (i.e. C h u,v), world frame (i.e. Chw) and reference frame (i.e. C h c,r,l) respectively, where the reference represents the sensors. Other forms of this formula could project the target from any frame to the other one. And Eq.2 enables the projection from world or camera coordinates to the 2-dimensional pixel coordinates, where K and E represents the intrinsic and extrinsic camera parameter matrix. Matrix P transforms the unit from meter to pixel and produces the pixel coordinates Chu,v. b. multi-type sensor detection As mentioned before, an ideal road-side detection system must be efficient and robust to perform high-load real-time detection tasks. Most of detectors [9]-[15] uses the features after fusion as the input of prediction head. Unfortunately, this results in strong correlation and interdependence among different modal data. Once any of them is contaminated or fails, others associated will not contribute to the detection system. Thus we turn to result-level fusion mechanism and build independent multi-type sensor detection in advance. Radar. Inspired by RRPN [2] that generates anchors based on projected radar points on images, and CenterFusion [3] that expands the radar points to pillars to eliminate the variance of data in z-dimension, after the projection from radar coordinates to camera coordinates and producing the points of interests (POIs) on image, we expand the discrete points to the region of interests (ROIs) based on the known
1Robosense lidar plug-in: https://github.com/RoboSense-LiDAR/rslidarsdk
2Millimeter wave radar plug-in: https://github.com/OrangeSodahub/DelphiESR-SSR-gazebo-plugin
target sizes and the distance compensation. We apply the scaling factor to the predefined sizes to complete the distance compensation:
Si = α di +β (3)
In eq.3, Si is the scaling factor, di is the perpendicular distance along the normal detection plane to the target, α and β are two parameters determined by the size of the vehicle. The process of radar ROIs generation is shown in Figure 4. Camera and Lidar. YOLOv4 [22] and PointPillars [5] are our independent 2D detection and 3D point cloud detection tools. According to the report of PointPillars [5], it has excellent detection accuracy especially low latency which enables the real-time applications. And YOLOv4 achieved 43.5% AP (65.7% AP50) for the MS COCO dataset [25] at a real-time speed of ∼65 FPS on Tesla V100. Other advanced algorithms could also be added to the system. Point cloud data from different lidars are merged first to reach the unified spatial representations and enhance the perception information, that is, apply the coordinates transformation based on eq.1 and concatenation. And the whole size of points data in our simulation environment is up to 6000 ∼ 7000, of which invalid points account for a large proportion. To greatly improve the efficiency, following M. Himmelsbach et al. [26], we assign the Gaussian filter and RANSAC to filter the noise, outliers and ground points, which brings reduction by around 40%. Data synchronization. Though multiple detectors work independently, different data stream should be synchronized to ensure the valid results. Specifically, Coordinate system (Sec. IV-A) already achieves the spatial synchronization. And we align the timestamps of data steams released by ROS to get the temporal synchronization. c. multi-group sensor fusion detection Millimeter-wave Radar and Camera Fusion. The fusion layer takes in the ROIs from the radars and the cameras and returns the world coordinates of the detected objects. It includes two steps, IOU matching and target tracking, as shown in Fig.5. The IOU matching step identifies potential objects by the ROIs from radars and cameras, whose correlation is evaluated by the intersection over union (IOU). As shown in Algorithm 1, the program traverses the ROIs, calculates the IOU matrix, separates the raw detection results into pairs (potential objects) by the K-M algorithm, sorts them by IOU and discards those have low correlation (IOU < 0.6). The pixel coordinates of each pair are transformed to the world coordinates and then returned. Algorithm 1: Radar-Camera IOU Matching Input: Radar ROIs and Camera ROIs Output: Concatenated Pixel Coordinates of Potential
Objects 1 for each radar ROI and camera ROI do 2 calculate the IOU of radar ROI and camera ROI; 3 end 4 matched pairs = K-M algorithm (IOU matrix); 5 for each pair of matched pairs do 6 if IOU < 0.6 then 7 remove the pair from matched pairs; 8 else 9 concatenate the pixel coordinates;
10 end 11 end 12 return concatenated pixel coordinates of potential
objects;
The target tracking step checks the potential objects and denoises. As shown in Fig.6, the program calculates the distance between previous detected objects and the potential objects and then categorizes the potential objects as new, lost, and existing ones. The new objects are not returned for stability, while the lost ones will be discarded if not being detected for over 3 consecutive frames. Two nearing objects in two frames are regarded as an existing object, whose coordinates are filtered by Kalman Filter and then returned. The time update equations calculate the prior state estimate
based on the results of the last frame. { x̃−k = Ax̃k−1 P−k = APk−1A T +Q (4)
The measurement update equations calculate the posterior state estimate based on the prior state estimate and the measurement. The Kalman gain K is the ratio of predicted error to measurement error. K = P−k H T(HP−k HT +R)−1 (5){
x̃k = x̃−k +K ( zk −Hx̃−k ) Pk = (I−KH)P−k
(6)
As shown in Eq.4-6, xk is the real world coordinate of an existing object, zk is the observed one. Pk is the state covariance matrix, which we try to minimize. The transition matrix A and the observation matrix H are identity matrices in this article. The transition covariance Q and the observation covariance R are manually set. After that, the world coordinates of new and existing objects are stored in the storage and wait to be loaded in the next frame. Lidar and Camera Fusion. The fusion strategy is shown in Fig.5. The data obtained by the lidar and the camera are processed and then fed into separate detection branches. YOLOv4 [22] and PointPillars [5], two conventional and popular algorithms, are used for different model object detection. We get two types of predicted boxes at the ends of the branches, DC ∈ RN×8 and DL ∈ RL×M×4. To get the last single result, as shown in the Algorithm 2, boxes are concatenated and matched. Specifically, for each 3D detection box, there exists its multiple 2D detection boxes lie on different image frame, due to the overlap of range of cameras, and those with larger areas on the image plane are relatively more accurate, in this regard, we sort them based on the distances from the 3D prediction to the cameras. This operation only depends on the soft physical associations among sensors, which nearly has no influence on the accuracy. We mark the matched box of the first camera (geographically nearest) in the sorted camera list, and mask the matched boxes of others. By traversing the camera list and all of 2D boxes for each 3D box, we calculate the IOU to determine whether each 2D box is marked, masked or discarded, except the marked or masked ones in the former steps. When N 3D detection boxes in the scene are traversed, all the 2D detection boxes
are traversed N times, hence we discard those who are still not matched. Algorithm 2: Lidar-Camera Box Matching Input: Raw detection boxes DC and DL Output: Merged detection boxes
1 matched boxes: ḊC, ḊL = [], [ ]; 2 for dli, i=1,2,3... in D
L do 3 calculate the distances array A = {||dli − c j||}; 4 generate the ordered list Ci and DCi sorted by A; 5 for ci j, j=1,2,3... in Ci do 6 map dli to the image plane of ci j and get d̃ l i ; 7 for dci jk, k=1,2,3... in D C i j do 8 skip if dci jh exist in Ḋ C; 9 calculate IOU of dci jk and d̃ l i and form the
array Vi j; 10 end 11 get the max IOU from the list; 12 if max
j Vi j ⩾ 0.7 then
13 get the corresponding 2d box dci jh; 14 if dli not in ḊL then 15 add dli to ḊL, and add d c i jh to Ḋ
C; 16 else 17 delete dci jh from D
C; 18 end  19 end 20 end 21 end 22 Discard unmatched 2d and 3d boxes;
Merging is after matching. By projecting the center of image box to 3D world given the height (z-axis) of the target, we get finalize the box center based on weighted coordinates. And adjust the azimuth angle of coordinates-fixed 3d box by adding or subtracting angle increments until the aspect ratio of the lidar-based 2d box on the image plane is approximately equal to the weighted mean ratio. Generally, this step does not require much computation. Figure 7 shows the detection results of one frame, where the green boxes represent the results after successful matching and merging, while the white boxes represent the failures ones. Note the only one car in the far right corner, with
the misdetected result successfully removed through multisensor detection. v. multi-agent collaborative scheduling and decision  a. agent division In the study of [23], the author proposed to switch between V2I mode and V2V mode as the traffic flows change to find the lowest delay and the most reliable decisions. This paper further extends its V2I mode, dividing the complex traffic scene into different subjects, each of which contains a roadside unit (RSU) consists of a sensor detection system and an edge computing platform. The bases for dividing the main body are the traffic function, spatial hierarchy, or the distance. So we introduce the agent division to handle the long range of distance and functional differences. See in Fig.9, the agents consist of three parts: agent1—the intersection, agent2—the overpass ramp, and agent3—the ring road. The three subjects complete the perception, fusion, and scheduling within their respective areas. In the decision stage, the information is exchanged and merged on the cloud platform to make a collaborative decision. The overall information transmission nodes and topic framework of the system are also shown in Fig.9. b. cooperative scheduling Given the agents covering the whole field, we link it to the perception and fusion system. For simplicity, we number and label all the roads and areas where vehicles may occur the traffic jam. From the information offered by perception fusion system, agents could directly build a vehicle density and speed distribution map, hence then transmit the messages that point out where to avoid the peak traffic or else. For our follow-up experiments, we make the vehicles led by agents to optimize the route at any time. vi. experiments A. Implementation Details
Datasets. We build our image detection datasets through blender and gazebo. The collections consist of 13985 images all of which are annotated as VOC format. Train, test and validate split account for 6300, 4285 and 3400 respectively. These images are given by various perspectives and distances, untruncated and partially truncated. And it covers almost the states of the vehicles driving under all the roads. For 3D point cloud detection, we shot the frames of them as they run in the environment similar to image shots and annotated as KITTI [27] format using LabelCloud toolbox [28]. Finally we used 1750 frames of the simulation scene and 1250, 550 of them for training and test. respectively. And data augmentations [29] are applied to improve the model generalization performance. Model. We adapted the configurations from PointPillars [5] to train our 3D detector. For our own datasets, we set the detection range to [−12m,12m] for X and Y axes, and [−1m,4m] for Z axis. We use (0.015m,0.015m) as the pillar size for experiments. For other default settings, the readers could refer to the OpenPCDet toolbox [30] used in this work. Sim-to-Real. To valid the comprehensive function of our system, we performed successful sim-to-real transfer and built a real 8m × 10m sand table consistent with the simulation. We use PC as the MEC and communicate with AWS Deepracers via AIoT technology. b. muti-type and muti-group sensor detection In the simulation, gaussian noises are added to evaluate the KF. The performance of KF is shown in Fig.8 (top), where points in different color: green points—observation of single sensor, cyan points—observation of multiple sensors and red points—output of the Kalman filter, showing the KF denoises effectively. In this case, the fusion can balance the noise influence and filter the false detection that may occur irregularly due to the hardware. Once any of detectors fails, the other can ensure the overall detection effectiveness. Fig.8 (bottom) shows the performance of lidar-camera detection of 2.9×104 consecutive frames. Fig.8 (top) has the bird view of the overall scene, which acts as a geographical reference. The thermal map on the left shows the average pose error represented by Euclidean distance. We take the odometry information released by the vehicle itself as the ground truth. It can be seen that the error of most areas is within 0.045m.The map in the middle shows the average orientation similarity (AOS) of detections, using the cosine distance of the azimuth angle difference (Eq.7), and most parts excess 0.9. Similarly, the BEV IOUs of boxes are mostly greater than 0.7. AOS = 1 |D| Σs∈D 1+ cos(2∆θs ) 2 (7) c. scheduling system To apply the detection system to the actual traffic flow and perform the downstream tasks, we attached a lightweight scheduling system with three agents covering the whole scene. We use AIoT to associate real vehicles with simulated environments and conduct experiments simultaneously, thus through the real vehicles we can feel more intuitively and preliminarily verify the feasibility of our scheme in real world. Fig.10 (Left) introduces one scenario, with 6 vehicles heading the same intersection. We focus on the action of one vehicle, located on the right of frame 1. The critical points are the frame 2 and 4,
where the vehicle faced with the possibility of joining the traffic jam. With the perception system offering the global information and the scheduling system offering route plans, most vehicles could always get what they want—to avoid congestion while reach the least time cost. Here we only conduct a simple one—to showcase the functionality and work process of our proposed system, under the limited conditions. In the simulation, we conducted automatic driving of 10 vehicles randomly and schedulingly. Fig.10 (Right) shows the mean speed of all vehicles over 2.9×104 frames, where a great reduction on traffic load can be seen. As depicted in Fig.11, the other proof is the flows of different road intersections over time. Under the help of global detection and scheduling system, the utilization rate of different roads is optimized, with the increased total flow (624 for random and 668 for scheduling, see Table II). Finally, we achieve a success of the whole system, get FPS ∼ 10—the biggest loss lies in time synchronization among the three sensors, yet we reach the real-time run on RMMDet system. vii. conclusions and discussions In this paper, we present RMMDet, a road-side multitype and multigroup sensor detection system for autonomous driving, for more intelligent vehicle-road collaboration. RMMDet uses a virtual simulation counterpart based on ROS, implementing three types sensors detection and lidar-camera, radar-camera fusion detection in the complex traffic scene. Real-time multimodal data streams are captured, processed, fed into model and generate the desired results to construct an end-to-end efficient roadside system. For the detection algorithms, we initially choose YOLOv4 and pointpillars, which could be replaced by any other methods and it’s convenient for further research and experiments. And we link a lightweight scheduling system to showcase the functionality. In general, RMMDet presents an early attempt to build a comprehensive roadside system virtually and actually for autonomous driving. This is a long-line work and we will keep on it.