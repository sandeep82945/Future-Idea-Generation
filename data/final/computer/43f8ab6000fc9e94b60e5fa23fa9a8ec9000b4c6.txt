Multimodal-driven talking face generation refers to animating a portrait with the given pose, expression, and gaze transferred from the driving image and video, or estimated from the text and audio. However, existing methods ignore the potential of text modal, and their generators mainly follow the source-oriented feature rearrange paradigm coupled with unstable GAN frameworks. In this work, we first represent the emotion in the text prompt, which could inherit rich semantics from the CLIP, allowing flexible and generalized emotion control. We further reorganize these tasks as the target-oriented texture transfer and adopt the Diffusion Models. More specifically, given a textured face as the source and the rendered face projected from the desired 3DMM coefficients as the target, our proposed Texture-Geometry-aware Diffusion Model decomposes the complex transfer problem into multi-conditional denoising process, where a Texture Attention-based module accurately models the correspondences between appearance and geometry cues contained in source and target conditions, and incorporate extra implicit information for high-fidelity talking face generation. Additionally, TGDM can be gracefully tailored for face swapping. We derive a novel paradigm free of unstable seesaw-style optimization, resulting in simple, stable, and effective training and inference schemes. Extensive experiments demonstrate the superiority of our method. 1 introduction Talking face generation aims to synthesize talking video from the source face according to the given emotion, mouth movement, and head rotation, which is relevant to several applications, including video production and virtual avatars. Multimodal information could guide the animation in the real scenario, such as text, audio, image, and video. Recently, many attempts [1], [2], [3] have achieved significant progress in these tasks, most of them share the same paradigm, i.e., extracting the intermediate structural representation from given conditions first and then manipulating the source face to the desired expression and pose, which mainly follows the sourceoriented pipeline, as shown in Fig. 1 (a). Specifically, among these approaches, some image- and video-driven [4], [5], [6] methods employ AdaIN-based [7] generators that take vectors as input, which inevitably lead to the information loss and fail to preserve the source identity and background. Others [3], [8], [9], [10], [11] warp the source feature to the target by the explicit motion flows for better visual results, but they appear warping artifacts when the source and driving conditions encompass significant appearance variation. Consequently, the above generators tend to suffer from image degradation when rearranging the source features. Recent audio-driven tasks require more authentic results, subsequent works [12], [13], [14] adopt identity-specific training but cannot generalize across different persons. Besides, existing pipelines generally adopt GANs [15], and its unstable adversarial min-max objective training process further exacerbates unrealistic textures. Due to these constraints of the generator, each task
• C. Xu, S. Zhu, T. Huang and Y. Liu are with APRIL Lab, Zhejiang University, Hangzhou, China (e-mail: 21832066@zju.edu.cn; zhust@zju.edu.cn; 21725129@zju.edu.cn; yongliu@iipc.zju.edu.cn). • J. Zhu, J. Zhang, and Y. Tai are with YouTu Lab, Tencent, Shanghai, China (e-mail: junweizhu@tencent.com; vtzhang@tencent.com; yingtai@tencent.com). • Y. Liu is the corresponding author. Manuscript received April 19, 2005; revised August 26, 2015.
needs a specific design and is unfriendly for practical applications. Thus, one challenge arises, how to accomplish a robust and stable generator for all driving modals to achieve high-fidelity talking face generation. In addition, existing work [16] simply reflects the text on the mouth movements, ignoring the potential of text when under the large-scale pre-trained models. Thus another challenge arises, how to sufficiently use the text modal in this task. To address the above challenges, we first represent the emotion style in the text prompt inspired by the zero-shot CLIP-guided image manipulation, which could inherit rich semantic knowledge and allow flexible emotion control, i.e., unseen emotions could be specified using the text description and precisely reflected on the synthesized faces. Furthermore, to unify the multimodal-driven tasks into the same generator, we frame the talking face generation as a target-oriented texture transfer, instead of the source-oriented feature rearrange, and adopt a multi-conditional diffusion model to avoid unstable training of GANs, termed Texture-Geometry-aware Diffusion Model (TGDM), as shown in Fig. 1 (b). In particular, benefiting from the explainable and disentangled parameter space of 3DMMs [17], we combine the texture-related coefficients from the source face with the geometry-related ones from the driving conditions to construct 3D descriptors, which are projected to the image domain and serve as the target pivot. To further supplement source texture to rendered face, we employ cross attention that accurately models the correspondences between source and target appearance. To this end, TGDM is dedicated to transferring the source texture to the target rendered face, which preserves explicit structural information but avoids complex texture deformations. In contrast to recent diffusion-based methods [18], [19] that only handle mouth area generation, our approach can generate realistic faces with various expressions and poses. Considering the characteristics of the TGDM to model complex texture and semantic transfer, we further connect TGDM with another popular task, face swapping, which aims to transfer the source identity to the target face while preserving the target
ar X
iv :2
30 5. 02 59
4v 2
[ cs
.C V
] 9
M ay
2 02
3
attributes. Recent developments are stuck due to unstable GANbased training schemes and seesaw-style optimization goals. DiffFace [20] first avoids GANs but is still sensitive to identity-related and identity-unrelated hyperparameter settings when sampling. Borrowing the idea from the aforementioned driving framework, we derive a novel paradigm for face swapping built upon the TGDM, which inherits the merits of the diffusion model and requires only reconstruction loss during training, with no extra tricks for sampling either, as shown in Fig. 1 (d). In summary, we make the following four contributions:
• We adopt the text modal as the talking face emotion representation, inheriting rich semantics from large-scale pre-trained models, which allows flexible emotion control and unseen emotion generalization. • We propose a novel TGDM pipeline based on the multiconditional diffusion model to afford complex texture and identity transfer, generating high-quality talking face generation for all driven modals. • We transfer the TGDM to face swapping task and propose a novel training and inference paradigm that is simple, stable, and effective. • Abundant experiments are conducted to demonstrate the superiority of TGDM for several face manipulation tasks over SOTA methods. 2 related works  2.1 talking face generation Face reenactment involves taking the source face and replicating its pose and expression as the target. This can be achieved through two main techniques: instruction-based methods animate the source face instructed by the target structure. Various works [21], [22], [23], [24], [25] adopt landmarks and segmentation maps to indicate the facial attribute. Recently, with the success of AdaIN [7], subsequent works [5], [26] encode the target attributes in the vectorized information and then inject them into the source face. However, the above methods fail to explicitly indicate the movements between the source and target faces. Subsequently,
warping-based methods learn to warp and synthesize the target faces based on the estimated motion fields. These methods [27], [28] usually separate motion estimation and warped source face refinement into two stages. The most representative work is FOMM [3], which uses relative key-point locations to predict flow fields for source appearance driving. Other follow-up works [9], [10] focus on improving the motion flows and warping operation accuracy. Some works [8], [29], [30], [31] introduce 3D information as structure guidance for flow field generation. However, they still suffer from identity degradation under some extreme conditions. Recent UniFace [32] proposes a unified framework to boost the model’s robustness with the help of face swapping. Audio-driven talking head synthesis, a special form of face reenactment, aims to create talking videos with lip movements corresponding to the driving audio [33], [34], [35]. The traditional approaches could be roughly divided into 2D-based and 3D-based ones. 2D-based methods [36], [37], [38], [39] generate a series of 2D points on the face based on audio inputs, while 3D-based methods [14], [40], [41], [42], [43], [44] use audio to predict expression parameters or facial radiance fields. Some works [12], [31], [45] further improve geometry learning and take talking style into account. After achieving the intermediate structure, PCAVS [1] injects the pose and lip information into the generator by implicit modulation. StyleHeat [46] generates high-resolution driven faces with the help of StyleGAN. Besides, emotion is a factor that plays a critical role in realistic animation. MEAD [47] releases a high-quality talking head video dataset with annotations of emotion category and intensity. Subsequent works [2], [11] follow the framework of PC-AVS or FOMM and take emotion as another condition. However, due to the limitations of the generator and unstable GAN-based training, the above methods need to design the specific intermediate representation and generator for each driving modal, thus making it impossible to share the same structure. Nowadays, some attempts based on diffusion models [48], [49] have been made. Stypułkowski et al. [50] leverage a pre-trained audio encoder to add audio embeddings during the denoising process. DiffTalk [19] and Bigioi et al. [18] present crafted conditional diffusion models for generalized talking head synthesis. However, they fail to model the distinct expression and pose variations. 2.2 face swapping Face swapping aims to change the target identity according to the given source but keep other facial attributes constant. Early face swap works [51], [52], [53], [54] mainly focus on 3Dbased methods but suffer from poor visual quality. Recently, GAN-based [15] methods [55], [56], [57] have made significant progress. Specifically, Faceshifter [58] integrates identity and attribute embeddings adaptively from a well-designed learning model. SimSwap [59] introduces a feature matching loss hoping to preserve more attribute embeddings at the cost of sacrificing identity similarity. Hififace [60] and FaceInpainter [61] take 3D face descriptor into consideration for better geometry structure of swapped results. With the success of StyleGAN [62], [63], many works have emerged as a solution for high-resolution face swap. MegaFS [64] first exploits StyleGAN2 as the decoder. The followup works [65], [66] also adopt the pSp [67] framework and design the fusion strategy for better attribute preservation. However, they lack flexibility in application due to the fixed StyleGAN generator. Consequently, some attempts have been made to solve
this problem. StyleFace [68] redesigns the StyleGAN2 module and opens parameters for training. StyleSwap [69] introduces a mask branch and an ID inversion strategy to empower high-fidelity and robust face swapping. As the diffusion model shows excellent performance in many fields, DiffFace [20] makes the first attempt to apply the diffusion model to the face swapping task. Despite the impressive progress achieved by the above methods, it is still a struggle to fully transfer the face identity from the source face while preserving identity-unrelated attributes of the target images due to seesaw-style training losses. One solution [70] is to fully disentangle identity-related and identity-unrelated information, but it is almost impossible in the current implementation scheme. In this paper, we propose a new training paradigm only guided by the reconstruction loss to solve this challenge. 2.3 diffusion model and multimodal generation Diffusion models [48], [49] are recently proposed generative models that can synthesize high-quality images. They are a type of generative probabilistic model that consists of two steps. Firstly, data is destroyed by successively adding small amounts of Gaussian noise to it over a series of time steps. Secondly, a learning algorithm is trained to recover the data by gradually removing the noise over a series of time steps. Diffusion models are trained without discriminators, so they are more reliable and robust during training compared to GANs. Additionally, they do not suffer from common issues such as mode collapse or vanishing gradients, which are inevitable in the training process of GANs. After achieving great success in the unconditional generation, diffusion models are adapted to enable conditional generation. Dhariwal et al. [71] introduce classifier-guided diffusion, which forces the produced noise to approach the desired condition. Ho et al. further [72] develop a Classifier-Free Guidance approach that allows conditional editing without having to pretrain classifiers. Despite these advantages, diffusion models are hindered by their slow sampling speed due to the thousands of times on one sample for complete pixel space-based denoising. To address this issue, Song et al. [73] propose DDIM reduce sample time, and Rombach et al. [74] propose the Latent Diffusion Models (LDMs), which transfer the training and inference processes to a compressed lower-dimension latent space for more efficient computing. Diffusion models have become increasingly popular in multimodal generation incorporated with CLIP [75] due to their ability to generate data with desirable qualities while covering a wide range of distributions. Application fields of the diffusion model vary from text-based image generation [76], [77], [78], [79], textbased video generation [12], [80], [81], [82], [83], text-based audio generation [84], [85], text-based 3D representation generation [86], [87], [88], and many others. In this paper, we build our framework on the diffusion model and focus on animating the source face by multimodal geometry guidance, i.e., text, audio, image, and video, reflecting on facial expression and pose. 3 preliminaries  3.1 denoising diffusion probabilistic models (ddpms) DDPMs follow the idea of latent variable models that consist of a forward diffusion process and a reverse diffusion process. Specifically, a diffusion process gradually adds noise to the data sampled from the target distribution x0 ∼ q(x0) as a Markov chain. Each step q (xt | xt−1) (for t ∈ {0, . . . , T}) is defined as
a Gaussian distribution with a fixed or learned variance schedule βt ∈ (0, 1):
q (xt | xt−1) := N (√ 1− βtxt−1, βtI ) . (1)
By the Bayes’ rules and Markov property, the latent variable xt can be expressed as:
q (xt | x0) = N ( xt; √ ᾱtx0, (1− ᾱt) I ) , (2)
where ᾱt = ∏t s=1 αs, and αt = 1−βt. Then, the reverse process q (xt−1 | xt) can be parametrized by another Gaussian transition:
pθ (xt−1 | xt) := N (xt−1;µθ (xt, t) , σθ (xt, t)) , (3)
where µθ (·) and σθ (·) are predicted by the trained deep neural networks θ , which is optimized under the objective Ex, ∼N (0,1),t [ ‖ − θ (xt, t)‖22 ] . Thus, given xt, xt−1 can be sampled by using:
xt−1 = 1√
1− βt
( xt −
βt√ 1− αt θ (xt, t)
) + σtz, (4)
where z ∈ N (0, I). Furthermore, according to [73], x0 can be approximate derived by xt and θ (xt, t):
x̂0 := xt − √ 1− αt θ (xt, t)√
αt . (5)
This facilitates the use of pixel-level and perceptual losses during the training stage in Sec. 4.3. 3.2 3d morphable models (3dmms) Recent methods estimate the 3D face descriptors of 2D images by optimizing a neural network to extract 3D parameters from a face image. Thus we follow the previous work D3DFR [17] that adopts ResNet50 as the backbone to predict 3DMM coefficients, which consists of identity α ∈ R80, expression β ∈ R64, texture δ ∈ R80, illumination γ ∈ R27, and pose p ∈ R6. Note that the original 3DMM fails to control gaze direction, we explicitly model the gaze like [89], providing the normalized direction vector from the center of the eye to the pupil in four dimensions ω ∈ R4. Therefore, given an input face I , the output coefficients ρ ∈ R261:
ρ = D(I) = {α,β, δ,γ,p,ω} . (6)
With 3DMM, the 3D shape S and albedo texture T could be parameterized as:
S = S̄ + Bidα+ Bexpβ, T = T̄ + Btδ, (7)
where S̄ and T̄ denote the mean face shape and albedo texture. Bid, Bexp, and Bt are the bases of identity, expression, and texture computed via PCA. We project the reconstructed 3D face onto the 2D image plane with a differentiable renderer R according to its illumination γ and pose p:
I3d = R(S,T,γ,p). (8)
We naturally choose the rendered image I3d as the intermediate geometry condition in Sec. 4.2 due to its several appealing properties: 1) Compared with other structural representations, e.g., landmarks and segmentation maps, 3DMMs provide an explainable and disentangled parameter space, which enables direct recombine corresponding factors when conducting the specific face manipulation task. Besides, mapping other cues to 3DMMs is
much easier since no additional spatial information is required. 2) Rendered face images provide more detailed semantic and explicit geometry than vectorized parameters, thus reducing the training difficulty. We conduct extensive experiments in Sec. 5.2. 4 method  4.1 overview As shown in Fig. 2, multimodal-driven talking face generation aims to produce realistic videos according to given source identity and multimodal geometry conditions, i.e., text, audio, image, and video. In Multimodal-driven Geometry Condition, we employ rendered faces projected from 3DMMs as the intermediate structural representation. To further exploit the potential of text in this task, we represent the emotion style in text prompts, which could inherit rich semantics from the large-scale pre-trained models for flexible and generalized emotion control (Sec. 4.2). To enable multimodal conditions to share the same generator, we propose a powerful paradigm, termed Texture-Geometry-aware Diffusion Model (TGDM), which is based on the multi-conditional diffusion model, allowing complex texture transfer for high-fidelity face generation, and avoids unstable GAN-based training (Sec. 4.3). Finally, we extend TGDM to face swapping and derive a new paradigm for stable and effective training and inference (Sec. 4.4). In the following, we will supply more details. 4.2 multimodal-driven geometry condition Image-driven and Video-driven Conditions. For image driving, we combine the appearance-related 3DMM coefficients (identity, texture, and illumination) from the source image Is with the motion-related coefficients (expression, pose, and gaze) from the driving image Id to construct the desired 3D face descriptors ρ̂ = {αs,βd, δs,γs,pd,ωd}, along with its rendered face I3d as the geometry conditions. For video driving {I1,d, I2,d, · · · , IN,d}, we can treat them as isolated N images for processing. However, parameters from a single input frame will cause jitter and instability in the final generated video due to the inevitable prediction errors between consecutive frames. To alleviate this problem, like [8], we introduce a windowing strategy for better temporal consistency, i.e., the parameters of the adjacent frames are also used as descriptors of the central frame to smooth the motion trajectory. In practice, the coefficients of a window with continuous frames ρ̂ ≡ ρ̂i−k:i+k and the rendered frame of the central frame as the geometry conditions, where k is the radius of the window and set to 1 experimentally. Audio-driven and Text-driven Conditions. Audio-driven talking face generation is expected to maintain lip movements synchronized with input speech contents and synthesize natural facial motion simultaneously. Consequently, it raises two challenges, one is precise audio-to-lip mapping, and the other is highly temporal consistent. Unlike previous works that adopt LSTM [8], [90] or GRU [91], [92] to autoregressively deduce expression coefficients, we adopt the non-autoregressive Transformer [93] to capture the short- and long-term audio context and provide the sequencelevel representations for more accurate and temporal-coherent coefficients regression. Besides, emotion style also plays a crucial role in generating vivid talking face. For emotion representation, the one-hot coding [47] is in a fixed pattern and fails to convey the semantics cues contained in the label, while recent methods [2], [11] extract emotion embedding from given images and audio,
lacking generalizing to unseen styles due to the limited semantics. In contrast, we represent the emotion style in the text prompt and borrow help from CLIP to deliver the semantic cues. Thus our method inherits rich semantic knowledge and convenient interaction after various emotion styles are encoded by CLIP. To this end, we propose the Emotional Audio to Expression (EmoA2E) module. Specifically, as shown in Fig. 3, the Mel-frequency Cepstral Coefficients (MFCC) clips A = {A1, . . . ,AN} provide the cues of lip movement, a non-learnable extended token takes identity information α as input to connect the expression motion to the specific person, and the emotion embedding zemo produced from the fixed CLIP text encoder as the emotion condition. Besides, instead of utilizing a one-hot coding [47] to control the emotion intensity, we further prepend a learnable intensity token ϕ, which is the product of the base learnable intensity vector and intensity scalar:
ϕ = ηϕbase, (9)
where η ∈ {1, 2, 3} at the training phase, and it can be a continuous random value range from 1 to 3 during the testing phase. Typically, the audio sequence and identity token are first embedded into the hidden dimension, then together with the prefix token ϕ to be added with standard positional PE and emotion embeddings:
β̄ = T ([ϕ,MLP(α),MLP(A)] + PE + MLP(zemo)), (10)
β̂ = MLP(β̄). (11)
We train EmoA2E independently by two losses. First, we define an expression Reconstruction Loss Lrec to calculate the distance between the predicted β̂ and ground truth β:
Lrec = ∥∥∥β − β̂∥∥∥
2 . (12)
Besides, we further select 68 points from the original 3DMM ρ and modified one ρ̂, obtaining l and l̂, respectively. We define Landmark Loss to measure the similarity between them:
Llm = ∥∥∥l− l̂∥∥∥
2 . (13)
Thus, the total loss is defined as follows:
L = λrecLrec + λlmLlm, (14)
where λrec = 100 and λlm = 0.1. 4.3 texture-geometry-aware diffusion model Most recent GAN-based methods are source-oriented that explicitly model the deformation to animate the source into the driving pose and expression. However, it is still quite challenging to achieve the accurate desired geometry and capture the complex identity appearance when under various extreme conditions, such as large pose, yielding noticeable artifacts and degradation problems. Thus, we revisit this task and propose the target-oriented Texture-Geometry-aware Diffusion Model (TGDM), which focuses on transferring the source texture to the rendered geometry face and inherits the flexibility and fidelity of diffusion models. In this part, we give the descriptions of the network structure and the training details for the denoising process. Architecture. Following the [94], our conditional denoising model θ is designed by the UNet-based backbone, consisting of the encoder ΨE and decoder ΨD . As shown in Fig. 2, TGDM
𝑰𝑖−1,𝑠
𝑰0,𝑠
𝒁𝑖,𝑡−1
𝝐𝜃(𝒁𝑖,𝑡, 𝑰3𝑑 𝑖−1,𝑖, 𝑭𝑖,𝑠, ෝ𝝆 𝑖−1,𝑖, 𝑡) ，
𝝐𝜃(𝒁𝑖,𝑡, ∅, ∅, ∅, 𝑡)
𝝐𝜃(𝒁𝑖,𝑡, 𝑰3𝑑 0,𝑖 , 𝑭0,𝑠,ෝ𝝆 0,𝑖, 𝑡) ， 𝒁𝑖,𝑡
… …
𝑰𝑖−1,𝑠 𝑰𝑖
𝑰𝑖−1,𝑠
𝝐𝜃(𝒁𝑖,𝑡, 𝑰3𝑑 𝑖−1,𝑖, 𝑭𝑖,𝑠, ෝ𝝆 𝑖−1,𝑖,, 𝑡) ， … …
𝑰𝑖−1,𝑠 𝒁𝑖,𝑡 𝒁𝑖,𝑡−1 𝑰𝑖
Next frame 𝑰𝑖:= 𝑰𝑖−1,𝑠
Transformer 𝓣
Linear
Linear
PE
𝑨1 𝑨𝑁𝝋
𝜷1 𝜷𝑁…
…
CLIP
𝒛𝒆𝒎𝒐
“Angry”
Linear
𝜶
Learned
Fixed Next frame 𝑰𝑖:= 𝑰𝑖−1,𝑠
𝑰0,𝑠
𝝐𝜃(𝒁𝑖,𝑡, 𝑰3𝑑 0,𝑖 , 𝑭0,𝑠, ෝ𝝆 0,𝑖,, 𝑡) ， … …
𝒁𝑖,𝑡 𝒁𝑖,𝑡−1𝑰0,𝑠
Denoising Process
𝑰0,𝑠
（a）
（b）
（c）
𝑰𝑖
Resample
Fig. 3. The architecture of EmoA2E module. Removing the emotion embedding zemo and intensity ϕ inputs, this structure is used for emotion-free audio-to-expression learning. is conditioned on three external inputs. First, the texture encoder ΦE provides the multiscale features F s = { F 0s, . . . ,F k s } to provide the desired texture patterns, where k is 1, i.e., we adopt two resolution texture features in 16 × 16 and 32 × 32. To mix the source texture within the noise prediction branch and eliminate the effects of misalignment, we design the Texture Attention-based (TexAtt) module that employs the cross-attention mechanism for better integration. As shown in Fig. 2, each TexAtt receives the source texture feature F is and the noise feature F i d, the query is extracted by one convolution from F id, and the key and value are extracted from F is in the same way, obtaining Qd,Ks,V s ∈ RCi/4×Hi×Wi with reduced channel numbers. Then Qd and Ks are used to calculate the correlation matrix M , which further multiplies V s to obtain F i s→d. A zero-initialized learned scale parameter τ is applied onF is→d to control the source
texture transfer flow when added to the F id:
F is→d = softmax(Qd(Ks) T )V s = MV s, (15)
F̂ id = τF s→d + F i d. (16)
Then, the spatially aligned rendered face I3d is concatenated channel-wise with the noisy faceZT , which is obtained by adding noise to Id according to Eq. 2. They are fed to the first layer of the network to guide the denoising process, ensuring the intermediate noise and the output face follow the given facial geometry. In addition, the modified coefficients ρ̂ further supplement the implicit geometry cues, especially the gaze direction not included in the rendered face. It added with embedded time, forming the last condition C = Linear(ρ̂) + Linear(t), which is injected into the noise predictor via the adaptive instance normalization (AdaIN) [7]:
AdaIN(F id,C) = σc(C) F id − µ(F i d)
σ(F id) + µc(C), (17)
where µ(·) and σ(·) is the average and variance operation of the input feature F id respectively. µc(·) and σc(·) are used to estimate the adapted mean and bias according to the given condition. To this end, all condition information is properly integrated into the network θ(Zt,F s, I3d, ρ̂, t) to predict the noise for talking face generation. Objectives. We first adopt the regular simple Denoising Loss:
Lsimple = ‖ − θ(Zt,F s, I3d, ρ̂, t)‖2 , (18)
where is an added noise on Id. Besides, we estimate the fully denoised face Ẑ0 according to the Eq. 5, which enables further constraints on the image level. Concretely, we measure the difference between Ẑ0 and Id at the pixel and perceptual level by
a Reconstruction Loss Lrec as L2 distance and a Perceptual Loss as the LPIPS loss [95]:
Lrec = ∥∥∥Ẑ0 − Id∥∥∥
2 , (19) Lp = ∥∥∥φvgg(Ẑ0)− φvgg(Id)∥∥∥
2 , (20)
where φvgg(·) represents the pre-trained VGG16 [96] network. Thus, the total loss is defined as follows:
L = λsimpleLsimple + λrecLrec + λpLp, (21)
where λsimple = 10, λrec = 1, and λp = 1. 4.4 a novel face swapping paradigm built on tgdm Despite the impressive progress of recent methods, GAN- and diffusion-based face swapping methods still suffer from the dilemma that the improvement of source face identity consistency at the expense of sacrificing target attribute preservation. For example, DiffFace [20] employs identity and attribute expert models to guide the noise prediction, and the balance between them is critical to producing high-quality swapped faces. However, it is complex and needs many experimental attempts. We attribute this phenomenon to the training phase playing the seesaw-style game, which struggles to balance all identity-unrelated attributes preservation and the source identity fusion. Since our proposed method for talking face is able to transfer complex textures, we derive a novel paradigm for face swapping built upon the TGDM. Specifically, as shown in the top of Fig. 4, there are two modifications. First, we completely mask the face region of the source texture image with the help of the mask predictor M [97] to ensure that the ground truth identity information is not visible to the network. Second, because of the low-dimensional linear representation of 3DMMs, the rendered images often lack
photo-realism and fine texture details like wrinkles. We further supplement the identity embedding from the expert identity model G [98]. In this way, the renderer image I3d, identity embedding zid, and Linear(ρ) focused on affording identity cues and identity-unrelated attributes of the face region, while Im makes up for the absence of hair and background. Notably, the mouth area is also served as the background, which is discussed in the Sec. 5.4. During training, as Eq. 21, our scheme does not require complex losses. Instead, the reconstruction loss is sufficient. The hyperparameter setting is the same as Eq. 21 either. For inference, given the source Is and the target It, we first render the I3d with the identity factor of the source and the remaining parameters of the target. As shown in the bottom of Fig. 4, I3d is sensitive to the geometric structure, exhibiting the exact desired face shape, and zid contains source identity semantics. Combining both of them guarantees identity similarity. To this end, following the standard denoising process, our method successfully transfers the source geometry- and semantic-aware identity information to the target, while fully keeping the identity-unrelated attributes without any complex sampling tricks. 5 experiment  5.1 datasets and implementation details Datasets. For talking face generation, we leverage the VoxCeleb1 [99] dataset, which contains over 20K videos. Among them, We select the high-resolution (720P) ones and follow the preprocessing method in FOMM [3] to crop the videos and resize them to 256 × 256, obtaining 17,927 training videos and 491 testing videos. For emotional talking face generation, we adopt the MEAD [47] dataset, which contains eight emotion types (neutral, angry, contempt, disgusted, fear, happy, sad, and surprised) and three intensity levels (levels 1, 2, 3). We randomly select 36 identities of front-view video clips for training and the rest for testing. For face swapping, we utilize the high-quality CelebAMask-HQ [100] dataset, which has 30,000 images with fine-grained mask annotation. FaceForensics++ [101] is used for testing, which is a forensics dataset consisting of 1000 videos. Metrics. For face reenactment, we use PSNR and LPIPS [95] to evaluate reconstruction quality. Exp, Angle, and Gaze are used to calculate the average Euclidean Distances of corresponding coefficients between the generated and target faces. We employ ID embeddings extracted by Curricularface [98] (ID-C) and Arcface [102] (ID-A) to measure identity cosine similarity. We further use FID [103] to evaluate the realism of the generated faces. For talking face generation, in addition to the metrics mentioned above, we use Landmarks Distance (LMD) [104] around the mouth, and the confidence score (Sync) proposed in SyncNet [105] to measure the accuracy of mouth shapes and lip synchronization. We further use Emotion Feature Distance (EFD) to measure the accuracy of the emotion representation, which is extracted by [106]. For face swapping, we adopt Exp, Angle, ID-A, and FID for evaluation. We do not adopt ID-C since Curricularface has been used in training and inference. Implementation Details. For EmoA2E, we randomly sample consecutive K = 32 clips for emotion-condition training in MEAD and emotion-free training in VoxCeleb1. We use a learning rate of 0.0002 and 128 batch sizes with the Adam optimizer on one V100 GPU for 200K iterations. For TGDM, we randomly sample the source and target faces from the same video in MEAD and VoxCeleb1 for training. It takes about 4 days by using 4
V100 GPUs with 8 batch sizes and a 0.0002 learning rate for 200K iterations. For face swapping, we train its model as the aforementioned setting for approximately 3 days. For the diffusion model, the length of the denoising step T is set to 1000, and a linear noise schedule is adopted for both the training and inference process. Notably, to stale the training procedure, only MSE loss of noise is used at the beginning of the training. Only when it has been decreased below 0.05, MSE loss of image, and LIPIS loss then start to work. Besides, the UNet of TGDM receives 256×256 resolution images and performs 16 down-sample ratios. 5.2 face reenactment  5.2.1 comparison with baselines Qualitative Results. We perform qualitative comparisons with FOMM [3], PIRenderer [8], NTHS [107], HifiHead, TPSM [9], and DAM [10] in the Cross-Identity setting, where the source and the target are of different identities. We do not compare with StyleHeat [46] since it requires the aligned inputs due to the fixed StyleGAN generator. As shown in Fig. 5, we sample nine pairs from VoxCeleb1 for visualization. First, the top three pairs have a significant difference in face size. It can be seen that FOMMbased methods, e.g., TPSM and DAM, produce over-smooth facial textures and suffer from noticeable warping artifacts. HifiHead could generate realistic faces, but their poses are inconsistent with the target. By contrast, the results of our method are of high quality and with the desired attributes. Second, the target faces of the middle ones show rich micro-expressions. Recent methods just imitate mouth shape and head direction, and they ignore the emotion embodied in the target. For example, the target of the fourth row is surprised, and the sixth is contempt. For comparison, our results exhibit accurate emotion styles, i.e., surprised forehead lines, delighted mouth corners, and disdainful eyes. Finally, the bottom pairs suffer from occlusions in the source or the target. It is difficult for FOMM-based methods to estimate the precise key
Input Source
Fig. 7. Attention visualization of TextAtt. The color bars indicate activation values. The points in the input rendered face could correctly match similar semantic and geometrical areas in the source. points, thus usually resulting in extremely distorted facial shapes (the head area of row 7). Other methods also struggle to animate the occluded objects to fit the desired pose. Benefiting from the effective cross-attention mechanism, our method is not sensitive to occlusion and reasonably preserves the non-facial parts in the generated results (the headphones of row 8 and the hat of row 9). Moreover, these cases are all under large-pose conditions, which convincingly demonstrate that our method successfully transfers the source texture to the target rendered image, providing more realistic results with accurate pose and detailed expression while preserving the source identity. Quantitative Results. We quantitatively compare the proposed method with several aforementioned SOTA methods both in SameIdentity and Cross-Identity settings. We randomly sample 200 identities from the test set and set 5 random seeds to generate
1K pairs in total. The results are summarized in Tab. 1. Benefiting from the explicit facial representation contained in the rendered face, our methods achieve an impressive performance of facial attributes, indicating that our model can animate the source face that is highly faithful to the given structure cues. Furthermore, our method is favorable against other methods regarding the reconstruction metrics PSNR and LPIPS and image quality metric FID. HifiHead obtains the lowest FID due to the StyleGAN-based generator. Besides, it also shows the best identity consistent but suffers from severe pose error, which can be concluded from rows 2 and 8 of Fig 5 either. Overall, the above observations are consistent with the qualitative results in Fig. 5. 5.2.2 ablation study and analysis Ablation Study. We perform qualitative and quantitative experiments to validate the merits of the proposed designs. Specifically, we design two variations to evaluate the effectiveness of TextAtt. Specifically, we adopt the image-level and feature-level concatenation for feature injection as two baselines. For a fair comparison, we train our method and two baselines with the same setting, e.g., same batch sizes and training iterations. As shown in columns 3 and 4 of Fig. 6, these two baselines are able to generate the desired pose and expression, but they have a limited ability to retain the source appearance, exhibiting severe color jitting, especially the feature-level concatenation. To further verify the necessity of the rendered face I3d, we only use the 3D face descriptors ρ to supply the facial geometry information. Comparing the results of columns 5 and 6, we can observe that the implicit representation is insufficient to effectively support geometry alignment. Contrary to the above competitors, our results show higher quality, which illustrates the effectiveness of cross attention as the feature transfer module and rendered image as the explicit geometry condition, both reducing the difficulty of training and speeding up the convergence of the model. Besides, the above observations could also be summarized from Tab. 2, our proposed method improves all metrics by a large margin. Interpretability of TextAtt. To better understand the crossattention mechanism, we visualize the attention maps of the TextAtt in UNet middle block, which is 16 × 16 resolution. As
S o u r c e
Happy ↓
Contempt
Happy ↓
Disgusted
Happy ↓
Surprised
𝜼 = 𝟏
𝜼 = 𝟐 .𝟓
S o u r c e
H a t r e d
S a d l y s u r p r i s e d
H a p p y + S u r p r i s e d
（a） （b）
S o u r c e
𝜇 = 2.5
Happy ↓
Contempt
Happy ↓
Disgusted
Happy ↓
Surprised
𝜼 = 𝟏
𝜼 = 𝟐 .𝟓
S o u r c e
H a t r e d
S a d l y s u r p r i s e d
H a p p y + S u r p r i s e d
（a） （b）
Fig. 9. (a) The visualization of unseen emotion style. Rows 2 and 3 (in Red) are the compound styles, and row 4 (in Blue) is a new style. (b) Results of different emotion styles and intensity levels. shown in Fig. 7, we select three points from different regions in the noise feature, i.e., head, face, and background. The visualized attention maps indicate that each location pays more attention to the geometrically and semantically similar areas, e.g., the red point is sampled from the head region, which has a higher response with the corresponding region of the source feature. Consequently, the such attention-based design allows the explicit texture transfer to achieve photo-realistic and identity-consistent face generation. 5.3 talking face generation  5.3.1 comparison with baselines Qualitative Results. We perform qualitative comparisons with Wav2Lip [35], PC-AVS [1], and EAMM [11] for talking face generation. Fig. 8 (a) visualizes the generated frames of these methods. It can be seen that all methods could produce synchronized lip shapes with given audio signals. However, Wav2Lip fails to change the head pose and artifacts appear around the mouth area due to the inevitable blending mismatch. PC-AVS and EAMM only support the aligned faces as inputs. Thus such preprocess
Method EFD ↓ LMD ↓ Sync ↑ ID-C ↑ FID ↓
Wav2Lip - 3.09 4.86 - - PC-AVS - 3.21 4.65 0.81 30.72 EAMM-Neutral - 3.22 4.60 0.79 37.90 Ours - 3.09 4.91 0.83 26.54
MEAD 0.084 2.62 3.09 0.81 30.69 EVP 0.106 2.54 3.21 0.70 12.83 EAMM-Emo 0.092 2.50 3.26 0.74 29.01 Ours 0.061 2.36 3.52 0.81 16.33
TABLE 3. The top part is the quantitative comparison of emotion-free on VoxCeleb1 and the bottom part is the emotion-condition on MEAD dataset. operation destroys the original facial structure, reflecting on the misalignment pose with the target in the animated faces. Besides, their results are blurred and lose the sharp source textures. By contrast, our results are highly faithful to the given pose from the images and mouth movements from the audio, while maintaining the source texture well. For emotional talking face generation, we select three frames of two emotion styles in MEAD for comparison. As shown in Fig. 8 (b), Wav2Lip and PC-AVS struggle to generate desired emotions with synchronized lip shapes in this task, while the synthesized images from MEAD are of poor quality. EVP [108] and EAMM suffer identity inconsistency with the source and show less rich expression due to lacking intensity modeling. Benefiting from sufficient emotion semantics learning and the powerful generative capabilities of diffusion models, our method produces more accurate expressions and realistic textures. Quantitative Results. We conduct a quantitative comparison in the reconstruction setting that guarantees access to ground truth for evaluation. For a fair comparison, we align the cropping manner of all the methods. For talking face generation, as shown in the top part of Tab. 3, we do not calculate the ID-C and FID of Wav2Lip since it only generates the mouth region and copies other regions from input faces. Contrary to other methods, our method yields the best motion control, temporal coherence, identity consistency, and
Source Target Normal Small Dilated Coeff
image quality in terms of LMD, Sync, ID-C, and FID. We further compare our emotion-condition pipeline with other emotional talking face generation methods. As shown in the bottom part of the Tab. 3, our method outperforms most metrics except for the FID. EVP achieves higher FID due to the vid2vid-based generator, but it exhibits a weak manipulated ability, which can be inferred from the lower ID-C and Sync, and higher LMD. Moreover, compared with the performance of emotion-free talking face generation and the emotion-condition one, the latter achieves better mouth shape accuracy (lower LMD) due to the limited corpus of MEAD, but the emotions introduce the irregular talking rhythm, leading to the poor synchronization (lower Sync). 5.3.2 ablation study and analysis Ablation Study. To verify the effectiveness of the Transformer encoder in EmoA2E, we replace it with stacked fully-connected layers of GRU-based recurrent neural networks. Our method outperforms the above two architectures on LMD metric: 3.54 vs. 2.47 vs. 2.36 of MLPs, GRUs, and Transformers. To further explore the effect of different emotion encoding manners on the unseen emotion style, we use one-hot encoding and language pretrained model GPT2 [109] for evaluation. It is obvious that one-hot fails to represent a new style due to the fixed pattern. GPT2 is not available to the visual cues and struggles to reflect the unseen textual semantics to the image domain. We conduct a quantitative experiment that measures the cosine similarity of the attached sequences in Fig. 9 with the corresponding text prompts when encoded by GPT2 and CLIP. Our method achieves better results: 0.621 vs. 0.430, which demonstrates the superiority of CLIP in handling multimodal information. Generalizing to Unseen Emotion Styles. Unseen emotion styles include compound and totally new styles. As shown in Fig. 9 (a), row 2 shows the results of the given Sadly surprised, and row 3 of the average embedding of Happy and Surprised, which indicates the flexible manipulation for compound emotion. We further present the new style Hatred in the fourth row. The correct
exhibition of these unseen styles verifies the flexibility and rich semantic priors of the CLIP feature space. Continuous Emotion Style Control. We conduct a qualitative experiment to evaluate the effectiveness of our method for controlling emotion style. As shown in Fig. 9 (b), our approach could change the emotion representation between two distinct styles, rather than previous techniques only taking a neutral face as the source. We increase the intensity value from 1 to 2.5, which shows continuous and accurate expression changes. Please pay attention to the mouth and eyes regions. 5.4 expanded application of face swapping  5.4.1 comparison with baselines We first conduct qualitative experiments to compare our method with DiffFace [20], High-Res [65], InfoSwap [70], MegaFS [107], HifiFace [60], Simswap [59], and FaceShifter [58] on the FaceForensics++ [101] dataset. As shown in Fig. 10, our model outperforms other models in changing identity-related geometry, especially the face shape, and preserving non-identity-related attributes. For example, in the third row, the generated face shape is more similar to the source, while other methods almost contain the same face shape as the target. Also, in the fourth and fifth rows, we totally preserve the non-identity-related attributes like hair and backgrounds. In the first row, our result is more similar to the source than others. Compared with another diffusion-based method DiffFace, our results obviously show the superiority of generating both identity-consistent and attributes-preserving faces, but the visual quality reduces to some degree. This is because our synthesized faces are more faithful to the target, while DiffFace produces clear but inconsistent textures. Moreover, Fig. 11 presents more qualitative comparisons with other SOTA methods that without officially released codes, e.g., StyleFace [68], StyleSwap [69], and FlowFace [110]. Please attention to the area indicated by the red arrow. We further report quantitative results compared to a part of the above method with officially released codes. The results in Tab. 4 also prove that our method is better
Source Target Ours Diffface High-Res SimSwap HifiFace InfoSwap MegaFS FaceShifter
considering both identity consistency with the source and attribute preservation with the target. 5.4.2 ablation study and analysis The critical operation of our reconstruction-based face swapping paradigm is to mask the source face to avoid identity information leaking. Thus we report a visualization to explore the effect of the mask area. As depicted in Fig. 12, we design three variations, i.e., the Normal mask covers the all face area, the Small treats the mouth area as the background, and the Dilated mask dilates the Normal mask to cover more areas. There is no apparent difference between the Normal and Small types in terms of identity and attributes by comparing columns 3 and 4, but the Small obtains the more realistic mouth area since it can learn information from the Small masked source. Please pay attention to the red rectangle of row 2. The results of Dilated show the artifacts around the face contour and lead to image degradation. On the basis of these phenomenons, we choose Small mask experimentally. Besides, as shown in column 6 in Fig. 12, we observe that without the rendered face I3d, the color of the swapped results are prone
to be similar to the source rather than the target, which further demonstrate the necessity of the rendered face as the condition. 6 limitations and future works First, almost all generators are based on a single image, and TGDM is no exception, which inevitably introduces temporal inconsistency. To boost the coherence of the generated talking videos, previous works [19] exploit the synthesized image as the source face for the next time step, resulting in a smoother transition between frames since the adjacent frames share the most consistent texture. However, such a frame-by-frame strategy has the problem of error propagation when encountering sudden movements, resulting in face degradation in all subsequent frames. Besides, our method retains some disadvantages of the diffusion model. For example, it takes about 45 ms on one V100 GPU to generate a single face under the T = 1000 DDPM setting, which is unacceptable in the real application. We also do not train the model for a longer time, considering the high consumption of the diffusion model. For efficiency, our model only supports 256× 256 image generation. Although DDIM [73] and LDMs [74] have alleviated the above problems, we hope to propose an intuitive design like StyleGAN to allow efficient highresolution face generation. 7 conclusion In this paper, we propose a diffusion-based model to complete multimodal-driven talking face generation, which shows several appealing properties: 1) We adopt the text modal as the talking face emotion representation, which inherits rich semantics from the CLIP, allowing flexible and generalized emotion control. 2) We treat talking face generation as a target-oriented texture transfer task. Our proposed TGDM maintains the faithful textures and undistorted appearance details from the source face and preserves explicit structural information but avoids complex texture deformations, which allows all modals to share the same generator. 3) Our proposed TGDM is also suitable for face swapping, which enables a novel reconstruction-based training paradigm and gets rid of seesaw-style optimization during inference. Our extensive results demonstrate the superiority of the proposed pipeline for various face manipulation tasks.