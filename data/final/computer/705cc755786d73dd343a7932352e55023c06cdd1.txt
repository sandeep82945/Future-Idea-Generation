Recent studies have shown that higher accuracy on ImageNet usually leads to better robustness against different corruptions. Therefore, in this paper, instead of following the traditional research paradigm that investigates new out-of-distribution corruptions or perturbations deep models may encounter, we conduct model debugging in indistribution data to explore which object attributes a model may be sensitive to. To achieve this goal, we create a toolkit for object editing with controls of backgrounds, sizes, positions, and directions, and create a rigorous benchmark named ImageNet-E(diting) for evaluating the image classifier robustness in terms of object attributes. With our ImageNet-E, we evaluate the performance of current deep learning models, including both convolutional neural networks and vision transformers. We find that most models are quite sensitive to attribute changes. A small change in the background can lead to an average of 9.23% drop ∗ Corresponding author. This research is supported in part by the National Key Research and Development Progrem of China under Grant No.2020AAA0140000. on top-1 accuracy. We also evaluate some robust models including both adversarially trained models and other robust trained models and find that some models show worse robustness against attribute changes than vanilla models. Based on these findings, we discover ways to enhance attribute robustness with preprocessing, architecture designs, and training strategies. We hope this work can provide some insights to the community and open up a new avenue for research in robust computer vision. The code and dataset are available at https://github.com/ 1. introduction Deep learning has triggered the rise of artificial intelligence and has become the workhorse of machine intelligence. Deep models have been widely applied in various fields such as autonomous driving [27], medical science [32], and finance [37]. With the spread of these techniques, the robustness and safety issues begin to be essential, especially after the finding that deep models can be easily fooled by negligible noises [15]. As a result, more researchers contribute to building datasets for benchmark-
ar X
iv :2
30 3. 17 09
ing model robustness to spot vulnerabilities in advance. Most of the existing work builds datasets for evaluating the model robustness and generalization ability on outof-distribution data [6, 21, 29] using adversarial examples and common corruptions. For example, the ImageNetC(orruption) dataset conducts visual corruptions such as Gaussian noise to input images to simulate the possible processors in real scenarios [21]. ImageNet-R(enditions) contains various renditions (e.g., paintings, embroidery) of ImageNet object classes [20]. As both studies have found that higher accuracy on ImageNet usually leads to better robustness against different domains [21,50]. However, most previous studies try to achieve this in a top-down way, such as architecture design, exploring a better training strategy, etc. We advocate that it is also essential to manage it in a bottom-up way, that is, conducting model debugging with the in-distribution dataset to provide clues for model repairing and accuracy improvement. For example, it is interesting to explore whether a bird with a water background can be recognized correctly even if most birds appear with trees or grasses in the training data. Though this topic has been investigated in studies such as causal and effect analysis [8], the experiments and analysis are undertaken on domain generalization datasets. How a deep model generalizes to different backgrounds is still unknown due to the vacancy of a qualified benchmark. Therefore, in this paper, we provide a detached object editing tool to conduct the model debugging from the perspective of object attribute and construct a dataset named ImageNet-E(diting). The ImageNet-E dataset is a compact but challenging test set for object recognition that contains controllable object attributes including backgrounds, sizes, positions and directions, as shown in Fig. 1. In contrast to ObjectNet [5] whose images are collected by their workers via posing objects according to specific instructions and differ from the target data distribution. This makes it hard to tell whether the degradation comes from the changes of attribute or distribution. Our ImageNet-E is automatically generated with our object attribute editing tool based on the original ImageNet. Specifically, to change the object background, we provide an object background editing method that can make the background simpler or more complex based on diffusion models [24, 46]. In this way, one can easily evaluate how much the background complexity can influence the model performance. To control the object size, position, and direction to simulate pictures taken from different distances and angles, an object editing method is also provided. With the editing toolkit, we apply it to the large-scale ImageNet dataset [41] to construct our ImageNet-E(diting) dataset. It can serve as a general dataset for benchmarking robustness evaluation on different object attributes. With the ImageNet-E dataset, we evaluate the performance of current deep learning models, including both con-
volutional neural networks (CNNs), vision transformers as well as the large-scale pretrained CLIP [39]. We find that deep models are quite sensitive to object attributes. For example, when editing the background towards high complexity (see Fig. 1, the 3rd row in the background part), the drop in top-1 accuracy reaches 9.23% on average. We also find that though some robust models share similar top-1 accuracy on ImageNet, the robustness against different attributes may differ a lot. Meanwhile, some models, being robust under certain settings, even show worse results than the vanilla ones on our dataset. This suggests that improving robustness is still a challenging problem and the object attributes should be taken into account. Afterward, we discover ways to enhance robustness against object attribute changes. The main contributions are summarized as follows:
• We provide an object editing toolkit that can change the object attributes for manipulated image generation. • We provide a new dataset called ImageNet-E that can be used for benchmarking robustness to different object attributes. It opens up new avenues for research in robust computer vision against object attributes. • We conduct extensive experiments on ImageNet-E and find that models that have good robustness on adversarial examples and common corruptions may show poor performance on our dataset. 2. related work The literature related to attribute robustness benchmarks can be broadly grouped into the following themes: robustness benchmarks and attribute editing datasets. Existing robustness benchmarks such as ImageNet-C(orruption) [21], ImageNet-R(endition) [20], ImageNet-Stylized [13] and ImageNet-3DCC [29] mainly focus on the exploration of the corrupted or out-of-distribution data that models may encounter in reality. For instance, the ImageNet-R dataset contains various renditions (e.g., paintings, embroidery) of ImageNet object classes. ImageNet-C analyzes image models in terms of various simulated image corruptions (e.g., noise, blur, weather, JPEG compression, etc.). Attribute editing dataset creation is a new topic and few studies have explored it before. Among them, ObjectNet [5] and ImageNet-9 (a.k.a. background challenge) [50] can be the representative. Specifically, ObjectNet collects a large realworld test set for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. The images in ObjectNet are collected by their workers who image objects in their homes. It consists of 313 classes which are mainly household objects. ImageNet9 mainly creates a suit of datasets that help disentangle the impact of foreground and background signals on classification. To achieve this goal, it uses coarse-grained classes
with corresponding rectangular bounding boxes to remove the foreground and then paste the cut area with other backgrounds. It can be observed that there lacks a dataset that can smoothly edit the object attribute. 3. preliminaries Since the editing tool is developed based on diffusion models, let us first briefly review the theory of denoising diffusion probabilistic models (DDPM) [24,46] and analyze how it can be used to generate images. According to the definition of the Markov Chain, one can always reach a desired stationary distribution from a given distribution along with the Markov Chain [14]. To get a generative model that can generate images from random Gaussian noises, one only needs to construct a Markov Chain whose stationary distribution is Gaussian distribution. This is the core idea of DDPM. In DDPM, given a data distribution x0 ∼ q(x0), a forward noising process produces a series of latents x1, ...,xT of the same dimensionality as the data x0 by adding Gaussian noise with variance βt ∈ (0, 1) at time t:
q(xt|xt−1) = N ( √ 1− βtxt−1, βtI), s.t. 0 < βt < 1,
(1) where βt is the diffusion rate. Then the distribution q(xt|x0) at any time t is: q(xt|x0) = N ( √ ᾱt, (1− ᾱt)I), xt = √ ᾱtx0 + √ 1− ᾱtϵ (2) where ᾱt = ∏t s=1(1 − βt), ϵ ∼ N (0, I). It can be proved that limt→∞ q(xt) = N (0, I). In other words, we can map the original data distribution into a Gaussian distribution with enough iterations. Such a stochastic forward process is named as diffusion process since what the process q(xt|xt−1) does is adding noise to xt−1. To draw a fresh sample from the distribution q(x0), the Markov process is reversed. That is, beginning from a
Gaussian noise sample xT ∼ N (0, I), a reverse sequence is constructed by sampling the posteriors q(xt−1|xt). To approximate the unknown function q(xt−1|xt), in DDPMs, a deep model pθ is trained to predict the mean and the covariance of xt−1 given xt instead. Then the xt−1 can be sampled from the normal distribution defined as:
pθ(xt−1|xt) = N (µθ(xt, t),Σθ(xt, t)). (3)
In stead of inferring µθ(xt, t) directly, [24] propose to predict the noise ϵθ(xt, t) which was added to x0 to get xt with Eq. (2). Then µθ(xt, t) is:
µθ(xt, t) = 1√ ᾱt
( xt −
βt√ 1− ᾱt ϵθ(xt, t)
) . (4)
[24] keep the value of Σθ(xt, t) to be constant. As a result, given a sample xt at time t, with a trained model that can predict the noise ϵθ(xt, t), we can get µθ(xt, t) according to Eq. (4) to reach the xt−1 with Equation (3) and eventually we can get to x0. Previous studies have shown that diffusion models can achieve superior image generation quality compared to the current state-of-the-art generative models [1]. Besides, there have been plenty of works on utilizing the DDPMs to generate samples with desired properties, such as semantic image translation [36], high fidelity data generation from low-density regions [44], etc. In this paper, we also choose the DDPM adopted in [1] as our generator. 4. attribute editing with diffusion models and ImageNet-E
Most previous robustness-related work has focused on the important challenges of robustness on adversarial examples [6], common corruptions [21]. They have found that higher clean accuracy usually leads to better robustness. Therefore, instead of exploring a new corruption that models may encounter in reality, we pay attention to the model debugging in terms of object attributes, hoping to provide new insights to clean accuracy improvement. In the following, we describe our object attribute editing tool and the generated ImageNet-E dataset in detail. The whole pipeline can be found in Fig. 2. 4.1. object attribute editing with diffusion models Background editing. Most existing corruptions conduct manipulations on the whole image, as shown in Fig. 1. Compared to adding global corruptions that may hinder the visual quality, a more likely-to-happen way in reality is to manipulate the backgrounds to fool the model. Besides, it is shown that there exists a spurious correlation between labels and image backgrounds [12]. From this point, a background corruption benchmark is needed to evaluate the model’s robustness. However, the existing background challenge dataset achieves background editing with copy-paste operation, resulting an obvious artifacts in generated images [50]. This may leave some doubts about whether the evaluation is precise since the dataset’s distribution may have changed. To alleviate this concern, we adopt DDPM approach to incorporate background editing by adding a guiding loss that can lead to backgrounds with desired properties to make the generated images stay in/close to the original distribution. Specifically, we choose to manipulate the background in terms of texture complexity due to the hypothesis that an object should be observed more easily from simple backgrounds than from complicated ones. In general, the texture complexity can be evaluated with the gray-level cooccurrence matrix (GLCM) [16], which calculates the graylevel histogram to show the texture characteristic. However, the calculation of GLCM is non-differentiable, thus it cannot serve as the conditional guidance of image generation. We hypothesize that a complex image should contain more frequency components in its spectrum and higher amplitude indicates greater complexity. Thus, we define the objective of complexity as:
Lc = ∑ |A(F(x))| , (5)
where F is the Fourier transform [45], A extracts the amplitude of the input spectrum. x is the evaluated image. Since minimizing this loss helps us generate an image with desired properties and should be conducted on the x0, we need a way of estimating a clean image x0 from each noisy latent representation xt during the denoising diffusion process. Recall that the process estimates at each step the noise ϵθ(xt, t) added to x0 to obtain xt. Thus, x̂0 can be estimated via Equation (6) [1]. The whole optimization procedure is shown in Algorithm 1.
x̂0 = xt√ ᾱt
− √ 1− ᾱtϵθ(xt, t)√
ᾱt . (6)
As shown in Fig. 3(a), with the proposed method, when we guide the generation procedure with the proposed objective towards the complex direction, it will return images with visually complex backgrounds. We also provide the GLCM dissimilarity and contrast of each image to make a quantitative analysis of the generated images. A higher dissimilarity/contrast score indicates a more complex image background [16]. It can be observed that the complexity is consistent with that calculated with GLCM, indicating the effectiveness of the proposed method. Controlling object size, position and direction. In general, the human vision system is robust to position, direction and small size changes. Whether the deep models are also robust to these object attribute changes is still unknown to researchers. Therefore, we conduct the image editing with controls of object sizes, positions and directions to find the answer. For a valid evaluation on different attributes, all other variables should remain unchanged, especially the background. Therefore, we first disentangle the object and background with the in-painting strategy provided by [54]. Specifically, we mask the object area in input image x. Then we conduct in-painting to remove the object and get the pure background image xb, as shown in Fig. 3(b) column 3. To realize the aforementioned object attribute controlling, we adopt the orthogonal transformation. Denote P as the pixel locations of object in image x where P ∈ R3×No . No is the number of pixels belong to object and pi = [xi, yi, 1]T is the position of object’s i-th pixel. h′ ∈ [0, H − h], w′ ∈ [0,W − w] where [x, y, w, h] stand for the enclosing rectangle of the object with mask M . Then the newly edited x[Tattribute · P ] = x[P ] and M [Tattribute · P ] = M [P ], where
Tsize = s 0 ∆x0 s ∆y 0 0 1  , Tposition = 1 0 w′0 1 h′ 0 0 1  , Tdirection =  cos θ sin θ 0− sin θ cos θ 0 0 0 1  . (7)
where s is the resize scale. θ is the rotation angle. ∆x = (1− s) · (x+ w/2),∆y = (1− s) · (y + h/2). With the background image xb and edited object xo, a naive way is to place the object in the original image to the corresponding area of background image xb as M ⊙ xo + (1 − M) ⊙ xb. However, the result generated in this manner may look disharmonic, lacking a delicate adjustment to blending them together. Besides, as shown in Fig. 3(b) column 3, the object-removing operation may leave some artifacts behind, failing to produce a coherent and seamless result. To deal with this problem, we leverage DDPM models to blend them at different noise levels along the diffusion process. Denote the image with desired object attribute as xo. Starting from the pure background image xb at time t0, at each stage, we perform a guided diffusion step with a latent xt to obtain the xt−1 and at the same time, obtain a noised version of object image
Algorithm 1: Background editing input : source image x, mask M , diffusion model
(µθ(xt),Σθ(xt)), ᾱt, λ, iteration steps t0 output: edited image x0
1 xt0 ∼ N ( √ ᾱt0x, (1− ᾱt0)I); 2 for t← t0 to 1 do 3 x̂0 ← xt√ᾱt − √ 1−ᾱtϵθ(xt,t)√ ᾱt ; 4 ∇bg ← ∇x̂0Lc(x̂0); 5 xbt−1 ∼ N (µθ(xt) + λΣθ(xt)∇bg,Σθ(xt)); 6 xo ∼ N ( √ ᾱtx, (1− ᾱt)I); 7 xt−1 ←M ⊙ xo + (1−M)⊙ xbt−1; 8 end
xot−1. Then the two latents are blended with the mask M as xt−1 = M ⊙ xot−1 + (1−M)⊙ xt−1. The DDPM denoising procedure may change the background. Thus a proper initial timing is required to maintain a high resemblance to the original background. We set the iteration steps t0 as 50 and 25 in Algorithm 1 and 2 respectively. 4.2. imagenet-e dataset With the tool above, we conduct object attribute editing including background, size, direction and position changes based on the large-scale ImageNet dataset [41] and ImageNet-S [11], which provides the mask annotation. To guarantee the dataset quality, we choose the animal classes from ImageNet classes such as dogs, fishes and birds, since they appear more in nature without messy backgrounds. Classes such as stove and mortarboard are removed. Finally, our dataset consists of 47872 images with 373 classes based on the initial 4352 images, each of which is applied 11 transforms. Detailed information can be found in Appendix A. For background editing, we choose three levels of the complexity, including λ = −20, λ = 20 and λ = 20-adv with adversarial guidance (see Sec.B for details) instead of complexity. Larger λ indicates stronger guidance towards high complexity. For the object size, we design
Algorithm 2: Object size controlling input : source image x, mask M , diffusion model
(µθ(xt),Σθ(xt)), ᾱt, iteration steps t0, ratio s output: edited image x0
1 xb ← ObjectRemoving(x,M ); 2 x,M ← Rescale (x,M, s); 3 xt0 ∼ N ( √ ᾱt0x
b, (1− ᾱt0)I); 4 for t← t0 to 1 do 5 xbt−1 ∼ N (µθ(xt),Σθ(xt)); 6 xo ∼ N ( √ ᾱtx, (1− ᾱt)I); 7 xt−1 ←M ⊙ xo + (1−M)⊙ xbt−1;
8 end
four levels of sizes in terms of the object pixel rates (= sum(M > 0.5)/sum(M ≥ 0)): [Full, 0.1, 0.08, 0.05] where ‘Full’ indicates making the object as large as possible while maintaining its whole body inside the image. Smaller rates indicate smaller objects. For object position, we find that some objects hold a high object pixel rate in the whole image, resulting in a small H − h. Take the first picture in Fig. 3 for example, the dog is big and it will make little visual differences after position changing. Thus, we adopt the data whose pixel rate is 0.05 as the initial images for the position-changing operation. In contrast to benchmarks like ImageNet-C [21] giving images from different domains so that the model robustness in these situations may be assessed, our effort aims to give an editable image tool that can conduct model debugging with in-distribution (ID) data, in order to identify specific shortcomings of different models and provide some insights for clean accuracy improving. Thus, the data distribution should not differ much from the original ImageNet. We choose the out-of-distribution (OOD) detection methods Energy [33] and GradNorm [26] to evaluate whether our editing tool will move the edited image out of its original distribution. These OOD detection methods aim to distinguish the OOD examples from the ID exam-
ples. The results are shown in Fig. 4. x-axis is the ID score in terms of the quantities in Energy and GradNorm and y-axis is the frequency of each ID score. A high ID score indicates the detection method takes the input sample as the ID data. Compared to other datasets, our method barely changes the data distribution under both Energy (the 1st row) and GradNorm (the 2nd row) evaluation methods. Besides, the Fréchet inception distance (FID) [23] for our ImageNet-E is 15.57 under the random background setting, while it is 34.99 for ImageNet-9 (background challenge). These all imply that our editing tool can ensure the proximity to the original ImageNet, thus can give a controlled evaluation on object attribute changes. To find out whether the DDPM will induce some degradation to our evaluation, we have conducted experiment in Tab. 1 with the setting λ = 0 during background editing. This operation will first add noises to the original and then denoise them. It can be found in “Inver” column that the degradation is negligible compared to degradation induced by attribute changes. 5. experiments We conduct evaluation experiments on various architectures including both CNNs (ResNet (RN) [19], DenseNet [25], EfficientNet (EF) [47], ResNest [53], ConvNeXt [35]) and transformer-based models (VisionTransformer (ViT) [9], Swin-Transformer (Swin) [34]). Other state-of-the-art models that trained with extra data such as CLIP [39], EfficientNet-L2-Noisy-Student [51] are also evaluated in the Appendix. Apart from different sizes of these models, we have also evaluated their adversarially trained versions for comprehensive studies. We report the drop of top-1 accuracy as metric based on the idea that the attribute changes should induce little influence to a robust trained model. More experimental details and results of top1 accuracy can be found in the Appendix. 5.1. robustness evaluation Normally trained models. To find out whether the widely used models in computer vision have gained robustness against changes on different object attributes, we conduct extensive experiments on different models. As shown in Tab. 1, when only the background is edited towards high complexity, the average drop in top-1 accuracy is 9.23% (λ = 20). This indicates that most models are sensitive to object background changes. Other attribute changes such as size and position can also lead to model performance degradation. For example, when changing the object pixel rate to 0.05, as shown in Fig. 1 row 4 in the ‘size’ column, while we can still recognize the image correctly, the performance drop is 18.34% on average. We also find that the robustness under different object attributes is improved along with improvements in terms of clean accuracy (Original) on different models. Accordingly, a switch from an RN50 (92.69% top-1 accuracy) to a Swin-S (96.21%) leads to the drop in accuracy decrease from 15.72% to 10.20% on average. By this measure, models have become more and more capable of generalizing to different backgrounds, which implies that they indeed learn some robust features. This shows that object attribute robustness can be a good way to measure future progress in representation learning. We also observe that larger networks possess better robustness on the attribute editing. For example, swapping a Swin-S (96.21% top-1 accuracy) with the larger SwinB (95.96% top-1 accuracy) leads to the decrease of the dropped accuracy from 10.20% to 8.99% when λ = 20. In a similar fashion, a ConvNeXt-T (9.32% drop) is less robust than the giant ConvNeXt-B (7.26%). Consequently, models with even more depth, width, and feature aggregation may attain further attribute robustness. Previous studies [30] have shown that zero-shot CLIP exhibits better outof-distribution robustness than the finetuned CLIP, which is opposite to our ImageNet-E as shown in Tab. 1. This may serve as the evidence that our ImageNet-E has a good proximity to ImageNet. We also find that compared with fully-
supervised trained model under the same backbone (ViTB), the CLIP fails to show a better attribute robustness. We think this may be caused by that the CLIP has spared some capacity for OOD robustness. HF
All
Original
HF
All
Original
Figure 5. Comparisons between vanilla models and adversarially trained models across different architectures in terms of size changes (left). Evaluation of adversarial models (RN50) trained with different perturbation budgets is provided in the right figure. Adversarially trained models. Adversarial training [42] is one of the state-of-the-art methods for improving the adversarial robustness of deep models and has been widely studied [2]. To find out whether they can boost the attribute robustness, we conduct extensive experiments in terms of different architectures and perturbation budgets (constraints of l2 norm bound). As shown in Fig. 5, the adversarially trained ones are not robust against attribute changes including both backgrounds and size-changing situations. The dropped accuracies are much greater compared to normally trained models. As the perturbation budget grows, the situation gets worse. This indicates that adversarial training can do harm to robustness against attributes. 5.2. robustness enhancements Based on the above evaluations, we step further to discover ways to enhance the attribute robustness in terms of preprocessing, network design and training strategies. More details including training setting and numerical experimental results can be found in Appendix C.5. Preprocessing. Given that an object can be inconspicuous due to its small size or subtle position, viewing an object at several different locations may lead to a more stable prediction. Having this intuition in mind, we perform the classical Ten-Crop strategy to find out if this operation can help to get a robustness boost. The Ten-Crop operation is executed by cropping all four corners and the center of the input image. We average the predictions of these crops together with their horizontal mirrors as the final result. We find this operation can contribute a 0.69% and 1.24% performance boost on top-1 accuracy in both background and size changes scenarios on average respectively. Network designs. Intuitively, a robust model should tend to focus more on the object of interest instead of the background. Therefore, recent models begin to enhance the model by employing attention modules. Of these, the
ResNest [53] can be a representative. The ResNest is a modularized architecture, which applies channel-wise attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. As it has achieved a great boost in the ImageNet dataset, it also shows superiority on ImageNet-E compared to ResNet. For example, a switch from RN50 decreases the average dropped accuracy from 15.72% to 12.57%. This indicates that the channel-wise attention module can be a good choice to improve the attribute robustness. Another representative model can be the vision transformer, which consists of multiple self-attention modules. To study whether incorporating transformer’s self-attention-like architecture into the model design can help attribute robustness generalization, we establish a hybrid architecture by directly feeding the output of res 3 block in RN50 into ViTS as the input feature like [3]. The dropped accuracy decreases by 1.04% compared to the original RN50, indicating the effectiveness of the self-attention-like architectures. Training strategy. a) Robust trained. There have been plenty of studies focusing on the robust training strategy to improve model robustness. To find out whether these works can boost the robustness on our dataset, we further evaluate these state-of-the-art models including SIN [13], DebiasedCNN [31], Augmix [22], ANT [40], DeepAugment [20] and model trained with lots of standard augmentations (RN50T) [48]. As shown in Tab. 2, apart from the RN50-T, while the Augmix model shows the best performance against the background change scenario, the Debiased model holds the best in the object size change scenario. What we find unexpectedly is the SIN performance. The SIN method features the novel data augmentation scheme where ImageNet images are stylized with style transfer as the training data to force the model to rely less on textural cues for classification. Though the robustness boost is achieved on ImageNetC (mCE 69.32%) compared to its vanilla model (mCE 76.7%), it fails to improve the robustness in both object background and size-changing scenarios. The drops of top1 accuracy for vanilla RN50 and RN50-SIN are 21.26% and 24.23% respectively, when the object size rate is 0.05, though they share similar accuracy on original ImageNet. This indicates that existing benchmarks cannot reflect the real robustness in object attribute changing. Therefore, a dataset like ImageNet-E is necessary for comprehensive evaluations on deep models. b) Masked image modeling. Considering that masked image modeling has demonstrated impressive results in self-supervised representation learning by recovering corrupted image patches [4], it may be robust to the attribute changes. Therefore, we choose the Masked AutoEncoder (MAE) [17] as the training strategy since its objective is recovering images with only 25% patches. Specifically, we adopt the MAE training strategy with ViT-B backbone and then finetune it with ImageNet
training data. We find that the robustness is improved. For example, the dropped accuracy decreases from 10.62% to 9.05% on average compared to vanilla ViT-B. 5.3. failure case analysis To explore the reason why some robust trained models may fail, we leverage the LayerCAM [28] to generate the heat map for different models including vanilla RN50, RN50+SIN and RN50+Debiased for comprehensive studies. As shown in Fig. 6, the heat map of the Debiased model aligns better with the objects in the image than that of the original model. It is interesting to find that the SIN model sometimes makes wrong predictions even with its attention on the main object. We suspect that the SIN model relies too much on the shape. for example, the ‘sea urchin’ looks like the ‘acron’ with the shadow. However, its texture clearly indicates that it is the ‘sea urchin’. In contrast, the Debiased model which is trained to focus on both the shape and texture can recognize it correctly. More studies can be found in Appendix C.4. 5.4. model repairing To validate that the evaluation on ImageNet (IN)-E can help to provide some insights for model’s applicability and enhancement, we conduct a toy example for model repair-
SINEdited DebiasedVanillaOriginal
ing. Previous evaluation shows that the ResNet50 is vulnerable to background changes. Based on this observation, we randomly replace the backgrounds of objects with others during training and get a validation accuracy boost from 77.48% to 79.00%. Note that the promotion is not small as only 8781 training images with mask annotations are available in ImageNet. We also step further to find out if the improved model can get a boost the OOD robustness, as shown in the Tab. 3. It can be observed that with the insights provided by the evaluation on ImageNet-E, one can explore the model’s attribute vulnerabilities and manage to
repair the model and get a performance boost accordingly. 6. conclusion and future work In this paper, we put forward an image editing toolkit that can take control of object attributes smoothly. With this tool, we create a new dataset called ImageNet-E that can serve as a general dataset for benchmarking robustness against different object attributes. Extensive evaluations conducted on different state-of-the-art models show that most models are vulnerable to attribute changes, especially the adversarially trained ones. Meanwhile, other robust trained models can show worse results than vanilla models even when they have achieved a great robustness boost on other robustness benchmarks. We further discover ways for robustness enhancement from both preprocessing, network designing and training strategies. Limitations and future work. This paper proposes to edit the object attributes in terms of backgrounds, sizes, positions and directions. Therefore, the annotated mask of the interest object is required, resulting in a limitation of our method. Besides, since our editing toolkit is developed based on diffusion models, the generalization ability is determined by DDPMs. For example, we find synthesizing high-quality person images is difficult for DDPMs. Under the consideration of both the annotated mask and data quality, our ImageNet-E is a compact test set. a. details for imagenet-e To guarantee the visual quality of the generated examples, we choose the animal classes from ImageNet since they appear more in nature without messy backgrounds. Specifically, images whose coarse labels in [fish, shark, bird, salamander, frog, turtle, lizard, crocodile, dinosaur, snake, trilobite, arachnid, ungulate, monotreme, marsupial, coral, mollusk, crustacean, marine mammals, dog, wild dog, cat, wild cat, bear, mongoose, butterfly, echinoderms, rabbit, rodent, hog, ferret, armadillo,primate] are picked. The corresponding coarse labels of each class we refer to can be found in [10]1. Finally, our ImageNet-E consists of 373 classes. Since the number of masks provided in ImageNet-S [11] in these classes is 4352, thus the number of images in each edited kind is 4352. The ImageNet-E contains 11 kinds of attributes editing, including 5 kinds of background editing and 4 kinds of size editing, as well as one kind of position editing and one kind of direction editing. Finally, our ImageNet-E contains 47872 images. Experiments on more images can be found in section C.3. The comprehensive comparisons with the stateof-the-art robustness benchmarks are shown in Figure 7. In contrast to other benchmarks that investigate new out-ofdistribution corruptions or perturbations deep models may encounter, w conduct model debugging with in-distribution data to explore which object attributes a model may be sensitive to. The examples in ImageNet-E are shown in Figure 9. A demo video for our editing toolkit can be found at this url:https://drive.google.com/file/d/ 1h5EV3MHPGgkBww9grhlvrl--kSIrD5Lp/view? usp=sharing. Our code can be found at an anonymous url: https://huggingface.co/spaces/ Anonymous-123/ImageNet-Editing. 1https://github.com/noameshed/noveltydetection/blob/master/imagenet categories synset.csv b. background editing Intuitively, an image with complicated background tends to contain more high-frequency components, such as edges. Therefore, a straight-forward way is to define the background complexity as the amplitude of high-frequency components. However, this operation can result in noisy backgrounds, instead of the ones with complicated textures. Therefore, we directly define complexity as the amplitude of all frequency components. The compared results are shown in Figure 8. It can be observed that the amplitude supervision on high-frequency components tends to make the model generate images with more noise. In contrast, amplitude supervision on all frequency components can help to generate images with texture-complex backgrounds. To edit the background adversarially, we set Lc = CE(f(x), y) where ‘CE’ is the cross entropy loss. f and y are the classifier and label of x respectively. We adopt the classifier f from guided-diffusion2. c. experimental details C.1. Details for metrics
In this paper, we care more about how different attributes impact different models. Therefore, we choose the drop of top-1 accuracy as our evaluation metric. A lower dropped accuracy indicates higher robustness against our attribute
2https://github.com/openai/guided-diffusion
changes. The dropped accuracy is defined as:
DA = accoriginal − acc. (8)
The detailed top-1 accuracy (Top-1) and dropped accuracy (DA)on our ImageNet-E are listed in Table 4, Table 5 and Table 6, Table 7. All the experiments are conducted for 5 runs and we report the mean value in the tables. C.2. Classes whose accuracy drops the greatest
To find out which class gets the worst robustness against attribute changes, we plot the dropped accuracy in Figure 11. The evaluated models are vanilla RN50 and its Debiased model. It can be observed that objects that have tentacles with simple backgrounds are more easily to be attacked. For example, the dropped accuracy of the ‘black widow’ class reaches 47% for both vanilla and Debiased models. In contrast, the impact is smaller for images with complicated backgrounds such as pictures from ‘squirrel monkey’. C.3. Experiments on more data
To explore the model robustness against object attributes on large-scale datasets, we step further to conduct the image editing on all the images in the ImageNet-S validation set. Finally, the edited dataset ImageNet-E-L shares the same size as ImageNet-S, which consists of 919 classes and 10919 images. We conduct both background editing and size editing to them. The evaluation results are shown in Table 8. The same conclusion can also be observed. For instance, most models show vulnerability against attribute changing since the average dropped accuracies reach 12.22% and 22.21% in background and size changes respectively. When the model gets larger, the robustness is improved. The consistency implies that using our ImageNet-E can already reflect the model robustness against object attribute changes. C.4. Bad case analysis
To make a comprehensive study of how the model behaves, we step further to make a comparison of the heat maps of the originals and edited ones. We choose the images that are recognized correctly at first but misclassified after editing. All the attributes editing including background, size, directions are explored. The heat maps are visualized in Figure 12. It can be observed that compared to the SIN and Debiased models, the vanilla RN50 is more likely to lose its focus on the interest area, especially in the size change scenario. For example, in the second row, as it puts his focus on the background, it returns a result with the ‘nail’ label. The same fashion is also observed in the background change scenario. The predicted label of ‘night snake’ turns into ‘spider web’ as the complex background has attracted its attention. In contrast, the SIN and Debiased models have robust attention mechanisms. The quantitative results in Table 5 also validate this. The dropped accuracy of RN50 (13.35%) is higher than SIN (12.19%) and Debiased (11.45%) even though the original accuracy of SIN (0.9157) is lower than vanilla RN50 (0.9269). However, the SIN also has its weakness. We find that though the SIN pays attention to the desired region, it can also make wrong predictions. As shown in the second row of Figure 12, when the object size gets smaller, the shape-based SIN model tends to make wrong predictions, e.g., mistaking the ‘sea urchin’ as ’acorn’ due to the lack of texture analysis. As a result, the dropped accuracy in the size change scenario is 24.23% for SIN, even lower than vanilla RN50, whose dropped accuracy is 21.26%. On the contrary, the Debiased model can recognize it correctly, profiting from its shape and texture-biased module. From the above observation, we can conclude that the texture matters in the small object scenario. C.5. Details for robustness enhancements
Network design—-self-attention-like architecture. The results in Table 1 show that most vision transformers show better robustness than CNNs in our scenario. Previous study has shown that the self-attention-like architecture may be the key to robustness boost [3]. Therefore, to ablate whether incorporating this module can help attribute robustness generalization, we create a hybrid architecture (RN50d-hybrid) by directly feeding the output of res 3 block in RN50d into ViT-S as the input feature. The results are shown in Table 9. As we can find that while the added module maintains the robustness on background changes, it can help to boost the robustness against size changes. Moreover, the RN50-hybrid can also boost the overall performance compared to ViT-S.
Training strategy—-Masked image modeling. Considering that masked image modeling has demonstrated impressive results in self-supervised representation learning by recovering corrupted image patches [4], it may be robust to the attribute changes. Thus, we test the Masked AutoEncoder (MAE) [18] and SimMIM [52] training strategy based on ViT-B backbone. As shown in Table 10, the dropped ac-
curacies decrease a lot compared to vanilla ViT-B, validating the effectiveness of the masked image modeling strategy. Motivated by this success, we also test another kind of self-supervised-learning strategy. To be specific, we choose the representative method MoCo-V3 [7] in the contrastive learning family. After the end-to-end finetuning, it achieves top-1 83.0% accuracy on ImageNet. It can also improve the attribute robustness when compared to the vanilla ViT-B, showing the effectiveness of contrastive learning. C.6. Hardware
Our experiments are implemented by PyTorch [38] and runs on RTX-3090TI. d. further exploration on backgrounds Motivated by the models’ vulnerability against background changes, especially for those complicated backgrounds. Apart from randomly picking the backgrounds from the ImageNet dataset as final backgrounds (random bg), we also collect background templates with abundant textures, including leopard, eight diagrams, checker and stripe to explore the performance on out-of-distribution
backgrounds. The evaluation results are shown in Table 12. It can be observed that the background changes can lead to a 13.34% accuracy drop. When the background is set to be a leopard or other images, the dropped accuracy can even reach 35.52%. Sometimes the robust models even show worse robustness. For example, when the background is eight diagrams, all the robust models show worse results than the vanilla RN50, which is quite unexpected. To comprehend the behaviour behind it, we visualize the heat maps of the different models in Figure 7. An interesting finding is that deep models tend to make decisions with dependency on the backgrounds, especially when the background is complicated and can attract some attention. For example, when the background is the eight diagrams, the SIN takes the goldfish as a dishwasher. We suspect it has mistaken the background as dishes. In the same fashion, the Debiased model and ANT take the ‘sea slug’ with eight diagrams as a ‘shopping basket’, which seems to make sense since the ‘sea slug’ looks like a vegetable. e. further discussion on the distribution In this paper, our effort aims to give an editable image tool that can edit the object’s attribute in the given image while maintaining it in the original distribution for model debugging. Thus, we choose the out-
of-distribution (OOD) detection methods including Energy [33] and GradNorm [26] following DRA [55] as the evaluation methods to find out whether our editing tool will move the edited image out of its original distribution. In contrast to FID which indicates the divergence of two datasets, the OOD detection is used to indicate the extent of the deviance of a single input image from the in-distribution dataset. Covariate shift adaptation(a.k.a batch-norm adaptation, BNA) is a way for improving robustness against common corruptions [43]. Thus, it can help to get a top-1 accuracy performance boost in OOD data. One can easily find out if the provided dataset is OOD by checking whether the BNA can get a performance boost on its data. We have tested the full adaptation results using BNA on ResNet50. In contrast to the promotion on other out-of-distribution dataset, we find that this operation induces little changes to top-1 accuracy on both ImageNet validation set (0.7615 → 0.7613) and our ImageNet-E (0.7934 → 0.7933 under λ = 20, 0.6521 → 0.6514 under random position scenario, mean accuracy of 5 runs). This similar tendency implies that our ImageNet-E shares a similar property with ImageNet. f. further evaluation on more state-of-the-art models
To provide evaluations on more state-of-the-art models, we step further to evaluate the CLIP [39] and EfficientNetL2-Noisy-Student [51]. The average dropped accuracy in terms of different models can be found in Figure 13. CLIP shows a good robustness to out-of-distribution data [30]. Therefore, to find out whether the CLIP can also show a good robustness against attribute editing, we evaluate the CLIP model (Backbone ViT-B) with both the zero-shot and end-to-end finetuned version. To achieve this, we finetune the pretrained CLIP on the ImageNet training dataset based on prompt-initialized model following [49]. It acquires a 81.2% top-1 accuracy on ImageNet validation set while it is 68.3% for zero-shot version. The evaluation on ImageNet-E is shown in Table 11 and Table 13. Though previous studies have shown that the zero-shot CLIP model exhibits better out-of-distribution robustness than the finetuned ones, the finetuned CLIP shows better attribute robustness on ImageNet-E, as shown in Table 11 and Table 13. The tendency on ImageNet-E is the same with Im-
ageNet (IN) validation set and ImageNet-V2 (IN-V2). This implies that the ImageNet-E shows a better proximity to ImageNet than other out-of-distribution benchmarks such as ImageNet-C (IN-C), ImageNet-A (IN-A). Another finding is that the CLIP model fails to show better robustness than ViT-B while they share the same architectures. We suspect that this is caused by that CLIP may have spared some capacity for out-of-distribution robustness. As the network gets larger, its attribute robustness gets better. While EfficientNet-L2-Noisy-Student is one of the top models on ImageNet-A benchmark [51], it also shows superiority on ImageNet-E. To delve into the reason behind this, we test EfficientNet-L2-Noisy-Student-475 (EF-L2NT-475) and EfficientNet-B0-Noisy-Student (EF-B0-NT). The EF-L2-NT-475 differs from EF-L2-NT in terms of input size, which former is 475 while it is 800 for the latter. It can be found that the input size can induce little improvement to the attribute robustness. In contrast, larger networks can benefit a lot to attribute robustness, which is consistent with the finding in Section 5.1. Evaluations on 91 state-of-the-art models can be found in Figure 14. All the evaluated models in this figure are all
provided by the timm library, except for the MoCo-V3-FT and CLIP-FT, which are finetuned by us. g. failure cases of generated images The failure cases of generated images are shown in Figure 16. The diffusion model fails to generate high-quality
person images. Though the object is reserved, the whole image looks quite wired. Therefore, we only keep the animal classes, resulting a compact set of ImageNet-E. However, extensive evaluations to 919 in Section C.3 have witnessed a same conclusion with evaluations on 373 classes. This implies that using our ImageNet-E can already reflect the model robustness against object attribute changes. h. related literature to robustness enhancements Adversarial training. [42] focus on adversarially robust ImageNet classifiers and show that they yield improved accuracy on a standard suite of downstream classification tasks. It provides a strong baseline for adversarial training. Therefore, we choose their officially released adversarially trained models3 as the evaluation model. Models with different architectures are adopted here4. SIN [13] provides evidence that machine recognition today overly relies on object textures rather than global object shapes, as commonly assumed. It demonstrates the advantages of a shape-based representation for robust inference
3https://github.com/microsoft/robust-models-transfer 4https://github.com/alibaba/easyrobust
(using their Stylized-ImageNet dataset to induce such a representation in neural networks)
Debiased [31] shows that convolutional neural networks are often biased towards either texture or shape, depending on the training dataset, and such bias degenerates model performance. Motivated by this observation, it develops a simple algorithm for shape-texture Debiased learning. To prevent models from exclusively attending to a single cue in representation learning, it augments training data with images with conflicting shape and texture information (e.g., an image of chimpanzee shape but with lemon texture) and provides the corresponding supervision from shape and texture simultaneously. It empirically demonstrates the advantages of the shape-texture Debiased neural network training on boosting both accuracy and robustness. Augmix [22] focuses on the robustness improvement to
unforeseen data shifts encountered during deployment. It proposes a data processing technique named Augmix that helps to improve robustness and uncertainty measures on challenging image classification benchmarks. ANT [40] demonstrates that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the previous state of the art on the corruption benchmark ImageNet-C and on MNIST-C.
DeepAugment [20]. Motivated by the observation that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. It introduces a new data augmentation method named DeepAugment, which uses image-to-image neural networks for data augmentation. It improves robustness on their newly introduced ImageNet-R benchmark and can also be combined with other augmentation methods to outperform a model pretrained on 1000× more labeled data. There are some more tables and figures in the next pages.