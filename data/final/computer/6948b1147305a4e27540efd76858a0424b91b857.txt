Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, kNearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs’ classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-shot, few-shot and fully-supervised settings, respectively, across eight diverse end-tasks. We hope our exploration will encourage the community to revisit the power of classical methods for efficient NLP1. 2 related work k-NN in the era of PLMs. The k-Nearest Neighbor (kNN) classifier is a classic non-parametric algorithm that predicts based on representation similarities. While kNN has lost some visibility compared to current deep learning approaches in recent years, it has not fallen off the radar completely. In fact, kNN has been used to enhance pre-trained language models (PLMs) in various tasks, such as unconditional language modeling (Khandelwal et al., 2020; He et al., 2021), machine translation (Khandelwal et al., 2021; Gu et al., 2018), and question answering (Kassner and Schütze, 2020). Most recently, (Alon et al., 2022; Meng et al., 2021) further respectively propose automaton-augmented and GNN-augmented retrieval to alleviate the computationally costly datastore search for language modeling. However, previous researchers (He et al., 2021; Khandelwal et al., 2021; Kassner and Schütze, 2020; Li et al., 2021; Meng et al., 2021; Alon et al., 2022; Zhang et al., 2022) mainly focus on generative tasks or adopt simple interpolation strategies to combine k-NN PLMs only at test time. (Shi et al., 2022) propose to leverage k-NN for zero-shot inference. Revisiting k-NN for PLMs. Unlike them, we focus on empirically demonstrating that incorporating k-NN improves PLMs across a wide range of NLP tasks in fine-tuning and prompt-tuning paradigms on various settings, including the fully-supervised, few-shot and zero-shot settings. Note that our work is the first to comprehensively explore k-NN during both the training and inference process further for fruitful pairings: in addition to the approaches mentioned above, we propose to regard the distribution predicted by k-NN as the prior knowledge for calibrating training, so that the PLM will attend more to the examples misclassified by k-NN. 3 methodology The overall framework is presented in Figure 2. We regard the PLM as the feature extractor that transforms the input textual sequence x into an instance representation x with dimensions D. We revisit k-NN in §3.1 and then introduce our method to integrate k-NN with tuning paradigms in §3.2. 3.1 nearest neighbors revisited Given the training set of n labeled sentences {x1, . . . , xn} and a set of target labels {y1, . . . , yn}, y ∈ [1, C], the k-NN classifier can be illustrated in the next three parts:
Feature Representations For k-NN, we firstly have to collect the corresponding set of features D = {x1, . . . ,xn} from the training set. Concretely, we assign x with the embedding of the [CLS] token of the last layer of the PLM for the fine-tuning procedure. More specifically, we define the feature representations as follows:
x = h[CLS], (1)
The feature representation q of a query example xq also follows the above equation. Retrieve k Neighbors Following the commonly practiced in k-NN (Friedman, 2017; Wang et al., 2019), we pre-process both q and features in the training set D with l2-normalization. We then compute the similarity between the query q and each example in D with Euclidean distance as : d(q,x), ∀x ∈ D, where d(·, ·) is the Euclidean distance calculation function. According to the similarity, we select the top-k representations from D, which are the closest in the distance to q in the embedding space. Similarity-based Aggregation Let N donate the set of retrieved top-k neighbors, and Ny be the subset of N where the whole examples have the same class y. Then the k-NN algorithm converts the top-k neighbors to q and the corresponding targets into a distribution over C labels. The probability distribution of q being predicted as c is:
pkNN(c|q) = ∑
x∈Ny exp (−d(q,x)/τ)∑ y∈C ∑ x∈Ny exp (−d(q,x)/τ) , (2)
where τ is the hyper-parameter of temperature. 3.2 Comprehensive Exploiting of k-NN In this section, we propose to comprehensively leverage the k-NN, the representative of lazy learning, to augment the PLM-based classifier. Role of k-NN as Prior Knowledge for Calibrating Training. As k-NN can easily make predictions for each query instance encountered without any training, it is intuitive to regard its predictions as priors to guide the network in focusing on hard examples during the training process of language models. We distinguish between easy and hard examples based on the results of k-NN. Given the probability distribution pkNN of q being predicted as true label y, we propose to adjust the relative loss for the
correctly-classified or misclassified instances identified by k-NN, in order to reweight the cross-entropy loss LCE . Specifically, we define the calibrated training loss LJ as:
LU = (1 + f(pkNN))LCE , (3)
where f(pkNN) donates the modulating factor 1 for calibration. We are inspired by Focal-loss (Lin et al., 2018) to employ the modulating factor, while our focus is on exploring the application of k-NN in the fine-tuning of PLMs. Intergrating k-NN into Inference Let PM denote the class distribution predicted by the PLM, and PkNN be the class distribution predicted by a k-NN classifier. Then, the PM is reformulates by interpolating the non-parametric k nearest neighbor distribution PkNN using parameter λ (Khandelwal et al., 2020) to calculate the final probability PU of the label as:
PU = λPkNN + (1− λ)PM, (4)
where λ ∈ [0, 1] is an adjustable hyper-parameter. 4 experiments  4.1 datasets We choose a variety of NLP tasks to evaluate our proposed methods, including sentiment analysis task (SST-5 (Socher et al., 2013)), question classification task (TREC (Voorhees and Tice, 2000)), NLI tasks (MNLI (Williams et al., 2018) and QNLI (Rajpurkar et al., 2016)), sentence-pair classification task (BoolQ (Clark et al., 2019) and CB (De Marneffe et al., 2019) ), and information extraction tasks (SemEval (Hendrickx et al., 2010) and TACREV (Alt et al., 2020)). We also list a detailed introduction of datasets in Table 1. 4.2 experimental settings Compared Baseline Methods. We adopt RoBERTalarge (Liu et al., 2019) as the underline PLM and conduct comprehensive experiments to integrate k-NN into PLMs. We choose the baseline approaches and the variant of our proposed method as follows: (1) k-NN: the method described in §3.1, which performs classification directly through nearest neighbor retrieval of instance features without relying on any pre-trained language models (PLMs). (2) FT: which denotes vanilla fine-tuning with PLMs. (3) FT_Scratch: which denotes vanilla PLMs in zero-shot setting. (4) PT: which denotes prompt-tuning with PLMs, similar to (Gao et al., 2021). (5) UNION-INF: a variant of our method, which simply linear interpolate k-NN and paradigms of PLMs during the test time. (6) UNION-ALL: the completeness of our approach, which involves applying k-NN as prior knowledge for calibrating training and also integrating k-NN into inference. 1We specify the f(pkNN) = (1− pkNN)γ , and other factors are also alternative. Settings. We test the above methods in full-supervised, few-shot and zero-shot experiments, we assign different settings, respectively: (1) Full-supervised setting: We use full trainsets to train the PLMs and as neighbors to retrieve. (2) Few-shot setting: We follow LM-BFF (Gao et al., 2021) to conduct 16-shot experiment and test the average performance with a fixed set of seeds Sseed, across three different sampled Dtrain for each task. In this setting, we use the few-shot training set as k-NN neighbors to retrieve. (3) Zero-shot setting: We directly evaluate the vanilla FT and UNION-INF on the test set without training. As for UNION-ALL, we take the prompt tuning (Gao et al., 2021) to tag the pseudo labels on unlabeled trainsets and apply untrained k-NN in the training and inference. 4.3 hyper-parameter settings We report the hyper-parameters in Table 3. For the GLUE and SuperGLUE datasets, we follow LMBFF2 to construct templates and verbalizer for prompt-tuning. While for RE datastes, we follow KnowPrompt (Chen et al., 2021) to construct templates and verbalizer. We utilize Pytorch to conduct experiments with 1 Nvidia 3090 GPUs. We used the AdamW optimizer for all optimizations, with a linear warmup of the learning ratefollowed by a linear decay over the remainder of the training. The hyperparameter settings used in our experiments are listed below. 2https://github.com/princeton-nlp/LM-BFF 4.4 main results k-NN features result in performance gains. We compare the specific results with baseline models and provide comprehensive insights of k-NN on different paradigms and different settings. The results as shown in Table 1. Leverage k-NN features results in performance gains in both few-shot and fullysupervised settings. In the zero-shot setting, PT-based methods outperform FT-based and k-NN features further enhance the performance of PT-based methods, which demonstrates that it is flexible and general to integrate k-NN for PLMs. Calibrating training vs. Incorporating into inference. It is necessary to study the different application scenarios of incorporating k-NN during the training and testing phases. From Table 2, we observe the following: (1) Leveraging k-NN during the test phrase is especially helpful for the zero-shot setting. While UNION-ALL performs worse due to the noise brought from the pseudo-labels on unsupervised data. (2) UNION-INF is not doing as well in the fully-supervised and few-shot setting. In contrast, UNION-ALL outperforms UNION-INF in these settings, especially in the few-shot setting. These findings reveal to us the applicable scenarios of incorporating k-NN and inspire further studies to utilize k-NN classifier more practically for efficient NLP. 4.5 analysis Q1: How does the lazy learner benefit eager learner? To further understand how does the lazy learner (k-NN) benefit the eager learner (PLM), we manually check cases in which k-NN, PT, UNIONINF and UNION-ALL produce different results. As shown in the example of the upper row of Figure 3, k-NN and UNION-ALL predict correctly when PT fails. This result is because UNION-ALL produces a more confident probability for the correct class via calibrating the attention on the easy vs. hard examples identified by the k-NN classifier. Note that the bottom row shows that UNION-ALL predicts correctly even when k-NN predicts wrongly, possibly due to the robustness of k-NN calibration. Q2: Does the similarity metric matter? In the above experiments, we mainly utilize negative L2 distance to measure the similarity between the query q and the instance representation of the data store. It is intuitive to estimate the impact of different similarity metrics, such as cosine similarity. Thus, we present the performance of UNION-ALL using both metrics with the same hyperparameters as below. Similarity Metric L2 cos
16-shot SST-5 (%) 43.7 42.8 16-shot TREC (%) 90.0 89.4 16-shot QNLI (%) 58.1 57.2
We can find that UNION-ALL with cosine distance achieves nearly the same performance as those trained with L2, revealing that our UNION-ALL is robust to the similarity metric. Q3: How dose the modulating factor f(pkNN) works? Since we adopt focal loss (Focal) as the modulating factor for main experiments, we further explore other functions as modulating factors, such as negative log-likelihood (NLL). As shown in Figure 4, we visualize two modulating factors with different settings of α and γ, where α donates a scalar that represent the proportion of the term of NLL, and γ is the exponential coefficient for Focal. We can find that NLL and Focal produce large weights for the misclassified examples, demonstrating the diversity of modulating factor selection. 5 limitations We only explore leveraging the training data for k-NN search, while various external domain data are also suitable for k-nearest neighbor retrieval. Moreover, incorporating k-NN also faces the following limitations: (1) the requirement of a large memory for retrieval; (2) hyper-parameters (such as λ and α) used for retrieval have an impact on the performance of model training; (3) if the number of nearest neighbors k is too large, it will also affect the efficiency. 6 conclusion and future work In this paper, we propose a novel method to enhance PLM-based classifiers using k-NN. Specifically, we introduce a calibration process and linear interpolation of inference phrases to effectively integrate k-NN into the training pipeline. To evaluate the effectiveness of our approach, we conduct a comprehensive and in-depth analysis of the role of k-NN in various NLU tasks and tuning paradigms. Our results demonstrate that the integration of k-NN is flexible and can significantly enhance the performance of large models.