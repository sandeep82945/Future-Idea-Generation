Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as stateof-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems. In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. ii. background and related work A. Software Package Reuse
Software package registries store versioned packaged software, associated metadata, documentation, and configurations [20]. Similarly, deep learning (DL) model registries distribute PTMs with metadata, a model card (i.e., documentation), relevant configurations, and versions of pre-trained weights [21]. DL model registries are an important component of the DL ecosystem [22]. As shown in Figure 2, PTM packages may contain more component than traditional packages, including weights, datasets, and performance metrics. Evaluating and selecting software packages is a difficult, but essential, activity for package reuse [13]. Prior work shows that engineers may improve their software selection with insights into the decision-making process and an understanding of relevant factors [13, 23, 24]. Existing literature focuses on practices in traditional software package registries, such as NPM [25] and Maven [26, 27]. The extent to which reuse practices for traditional software will transfer to the reuse of PTM packages is unclear. Reproducibility is another important aspect of software package reuse [28]. In traditional software packages, Goswami et al. found that 38% of explored NPM package versions are non-reproducible [29]. Similarly, Vu et al. highlighted existing discrepancies at different levels of granularity in PyPi [30]. Following the machine learning scientific research community [31], the software engineering community has just begun to study concerns in DL model registries [32]. We offer an early software engineering view on this topic. B. Pre-Trained Model Reuse
PTM reuse is necessitated by the emergence of largescale models, and is enabled by learning and compression techniques, including transfer learning [33], quantization and pruning [34], knowledge distillation [35], and data labeling [36]. Through transfer learning, DNNs can be pre-trained on large datasets and fine-tuned to solve specialized tasks, leveraging a PTM’s knowledge of one task to better teach it a similar task [33, 37]. Using quantization and pruning methods, PTMs can be optimized for latency- or energy-sensitive contexts, such as on edge devices, without compromising accuracy [34]. Via knowledge distillation, PTMs can be used to teach a smaller model, yielding good performance and reduced computational costs [35]. Engineers can also use PTMs to automatically label datasets [36]. Practitioners from major technology companies report challenges in model management and model reliability [16, 17]. Schelter et al. summarized model validation challenges, including decisions on model retraining, metadata querying, and adversarial settings [16]. Rahman et al. highlighted that the behavior of ML models can be easily affected because of their data-driven nature [38]. To better reuse the PTMs, it is important to monitor the performance of deployed models, track changes in data characteristics, and to retrain and revalidate them frequently. One way to address the management problems is to use a DL model registry, which is defined as: a collaborative model hub where teams can share DL models [21, 39]. The DL model registry concept imitates traditional software package registries such as NPM [40] and PyPi [41]. Through web searches, we identified several prominent DL model registries, including Hugging Face [42], TensorFlow Hub [43], PyTorch Hub [44], and ONNX Model Zoo [45]. Among all registries we examined [42–48], Hugging Face offers the largest and most diverse set of PTMs — it hosts over 60,000 PTMs, fifty times as many as the next largest DL model registry, as well as many types of models and datasets. C. Deep Learning Trustworthiness
The trustworthiness (e.g., reproducibility, explainability) of DL software grows in importance as DL techniques
are deployed in sensitive contexts such as autonomous vehicles [49, 50]. For example, DL traceability is hampered because authors often omit training logs and documentation [38, 51]. Wing urges the DL community to explore a combination of approaches to achieve trustworthy DL [52]. To improve the trustworthiness of ML systems, prior work recommends considering aspects including provenance, reproducibility, and portability [38, 52–54], as defined in Table I. Some researchers have investigated the performance variances tied to DL frameworks [55, 56], which threatens DL reliability. Adversarial attacks and defences are also important to DL trustworthiness [57, 58]. Gu et al. proposed the general term BadNet for models that perform well on benchmark datasets but poorly on attacker-defined inputs [59]. Kurita et al. showed that it is possible to construct BadNets from weight poisoning attacks by injecting PTM with vulnerabilities that expose backdoors after fine-tuning [60]. Additionally, Goldblum et al. discussed that it is also possible to attack a model indirectly via malicious labels in its training dataset (data poisoning) [61]. Wang et al. described an EvilModel where a PTM has malware bytes hidden inside its neurons’ parameters to be extracted and assembled into malware at run-time [62]. These attacks are not all covered by existing malware detection techniques and raise potential risks to DL model registries [63]. iii. research questions Summarizing the literature: Much is known about software engineers’ practices and challenges in reusing traditional software packages, but little about DL software packages (PTMs). Reuse and trust are unexamined in DL model registries. We studied the reusability of PTM packages in DL model registries, examining qualitative and quantitative aspects. We focused on one DL model registry, Hugging Face, as it is by far the largest registry at present [19]. For PTM reuse in the Hugging Face ecosystem, we ask: RQ1 How do engineers select PTMs? RQ2 What PTM attributes facilitate PTM reuse? RQ3 What are the challenges of PTM reuse? RQ4 To what extent are the risks of reusing PTMs mitigated
by Hugging Face defenses? RQ1-2 are focused on current software engineering practice, priming the participants to describe their challenges in RQ3. RQ4 complements this data with quantitative measurements. iv. methodology To answer our research questions, we used a mixed approach that combined two perspectives [18]. We first explore qualitative insights by interviewing practitioners, then we substantiate our findings with systematic measurements in Hugging Face ecosystem. The relationship between our questions and methods is shown in Figure 3. A. Qualitative Study: Interviews with PTM Reusers
Our interview study follows a four-step process modeled on the framework analysis methodology [64, 65]:
(1) Data Familiarization and Framework Identification Our initial thematic framework is based on three themes from our literature review (§II): model selection, PTM attributes, and PTM trustworthiness. For model selection, the identified considerations were the PTM reuse issues and factors affecting the decision-making process [19, 66]. For attributes, we saw both traditional attributes (i.e., popularity, quality, maintenance) [67, 68], and DL-specific attributes, viz. provenance, reproducibility, and portability [52–54], shown in the first three columns in Table I. For trustworthiness, we considered the aspects assumed trustworthy plus possible discrepancies [19]. (2) Interview Design We designed a semi-structured interview protocol with questions that explore the three identified themes of PTM reuse and trust. We conducted three pilot interviews. We then revised our framework and interview protocol, adding some PTM attributes and factors and clarifying definitions. The final interview protocol took 30–45 minutes. We compensated interview participants with a $20 gift card. The protocol is available in our artifact (§XII). (3) Recruitment We recruited users from the Hugging Face ecosystem [42], who presumably have experience in developing and reusing PTMs. According to the Hugging Face website [69] there are 18,348 Hugging Face users, 690 of whom have PRO accounts and 17,658 of whom have regular accounts. We sorted the lists of PRO and regular users by the number of models they have contributed to Hugging Face, and contacted the first 50 users of each type. We interviewed the 12 respondents described in Table II. This was a (response rate of 24%, of whom 9 had PRO accounts. Our participants contributed between 4 and 2500 models to Hugging Face. (4) Analysis We transcribed the interview recordings. Two researchers performed memoing [70], mapping the transcripts to the pre-defined themes. Each memo had a quote for one of the themes. Multiple researchers analyzed 4 transcripts and had high agreement on the memos extracted for each theme. Agreement was because the pre-defined themes had clear definitions, but we did not measure the agreement precisely. A single researcher memoed the remaining 8 transcripts. Then we organized the memos in a matrix by theme. Two researchers used the matrix to develop a thorough understanding of the larger picture. Then we answered each RQ by our understanding and reference to the matrix. As part of our analysis, we measured saturation from our interview transcripts by analyzing the number of cumulative unique codes by participant [71]. Saturation was achieved after
7 participants so we did not continue to recruit participants. B. Quantitative Study: Risk Mitigation Measurement
Our qualitative findings identified a variety of challenges and risks in PTM reuse. We measured these risks and mitigations in the Hugging Face ecosystem with the STRIDE methodology for threat modeling and risk assessment [72].1 STRIDE was proposed by Microsoft as a security analysis technique and is widely used [72–75]. STRIDE focuses on trust assumptions related to data, making it suitable for PTMs. STRIDE is a two-step process. First, the system under consideration is modeled using a dataflow diagram, and trust boundaries and the actors involved are identified. Second, each boundary is analyzed for the threats of the STRIDE acronym. Following the STRIDE methodology, we started by developing a dataflow diagram for PTMs on Hugging Face. Two researchers analyzed Hugging Face’s public documentation. After internal iteration, we settled on one primary trust boundary: user control vs. Hugging Face internal control. We identified threats and six risk-mitigating features across this boundary. Owing to the nature of the available data source
1STRIDE is a mnemonic for Spoofing identity, data Tampering, Repudiation, Information Disclosure, Denial of Service, and Elevation of privilege. (public documentation), we limited our analysis to a subset of the STRIDE threats: Spoofing, Tampering, Repudiation, and Elevation of Privilege. The completeness of our dataflow diagram was ensured by having two researchers review the documentation. These same researchers checked the soundness of the model by creating and using models on Hugging Face both as individual accounts and organization contributors. v. rq1: how do engineers select ptms? Finding 1: The participants share a similar decision-making process (Figure 4). Among the four PTM reuse scenarios in the research literature, our participants reported using only two: transfer learning and quantization techniques. When reusing, participants find PTMs from DL model registries easier to adopt than PTMs from GitHub projects. A. Reuse scenarios
Most interview participants take PTMs from model registries and apply transfer learning techniques to the model. They either “fine-tune an existing PTM” by (optionally) extending architecture and training on a task-specific dataset, or “build a new model on top of the pre-trained one”. Commonly, they select PTMs from leading technology companies (e.g., Google, Meta) because “the datasets are carefully cleaned and [the models] are straightforward to fine-tune”. The other three reuse scenarios discussed in the research literature (§II-B) were far less common in our interviews. P5 described using quantized models. No participants described using PTMs for knowledge distillation or for data labeling. B. Decision-making process
To understand how engineers select PTMs, we asked participants to summarize their decision-making processes. We found similarities between participant responses. We followed Michael et al. [66] in adapting a general software engineering reuse process [76] to integrate our findings into a unified model (Figure 4). Our model contains 4 stages: (1) Reusability assessment, (2) Model selection, (3) Downstream evaluation, and (4) Model deployment. We discuss each in turn. Reusability Assessment Engineers begin the decision-making process with a reusability assessment. Before selecting a
model, engineers must identify an ML task and determine if model reuse is appropriate. An ML task is composed of requirements including model input and output, latency, size, and licensing. Engineers must then decide if they should reuse a PTM or create a solution from scratch since “PTMs do not work for every use case.” Task parameters influence this decision. For example, three participants (P8, P9, P11) reuse PTMs because they “do not have enough computational resources”. In a similar manner, participants (P2, P5, P8, P9, P11) note that DL model registries provide inference APIs to simplify reuse — PTMs are “easy to use and test”. Model Selection Once engineers decide to reuse a model, they must select an existing architecture and an associated PTM. Engineers search for candidate architectures “built for the problem that [they are] trying to solve”, browsing model registries or relevant papers. Most study participants prefer to search through model registries. For example, participants (P1, P4, P6) said they can “easily find a model” in model registries due to classification at the domain (e.g., computer vision, natural language processing) and task (e.g., text generation, image classification) levels. P10 noted that standardizing PTM reuse increases model registries’ popularity. Once engineers select a candidate architecture, they must find a particular PTM to use. All study participants, including those who select architectures from papers, prefer PTMs from model registries. “Ease of use is very important” to engineers that do not think it is worth “spend[ing] much time on trying to understand the script[s] from the GitHub models”. P8 noted models from Hugging Face are “plug and play.” Since multiple PTMs might implement the same architecture, engineers select from among candidates based on PTM attributes (§VII). For example, most participants use popularity as a factor to select a PTM because it indicates “[community] trust in the model”. As another example, multiple participants (P2, P3, P8, P11) choose NLP PTMs trained on appropriate datasets (e.g., models trained on datasets with the correct language). Downstream Evaluation After selecting a PTM, engineers conduct a downstream evaluation for their specific task. Engineers have the option of assessing more than one candidate PTM in this stage. They download “a few models,” “finetune them,” “test them,” then “compare them.” When engineers select a candidate PTM, they first apply reuse techniques
to fit the model to their specific application (cf. §II-B and §V-A). This procedure is not necessarily straightforward because some models “don’t really work too well directly, even with their own datasets”. Furthermore, 11 out of 12 of the participants observed a lack of adequate documentation or discrepancies within existing documentation. After applying reuse techniques, engineers evaluate the model to see if it is ready for model deployment. When evaluating the trade-off between performance and architecture, P1, P8 and P10 state that these two factors are “tightly relevant to each other” and should be considered in a “fifty-fifty split”. Some participants prioritize one of these factors over the other. For example, P3 and P6 compare multiple models to maximize task performance. On the other hand, P7 will not use a “weird architecture” even if its documented performance is higher. Model Deployment Finally, engineers deploy their models. Deployment may depend on model characteristics and deployment environments. All participants mention that certain characteristics such as PTM size, robustness, and documentation significantly impact the deployment of models to other environments (i.e., different hardware or software configurations than what is used in development). Participant (P8) describes that the rapid increase in model size makes it “impossible for most [low-resourced teams] to actually run these models on their systems.” Participant (P5) states that most “models are on PyTorch or TF” and are therefore more difficult to deploy on mobile devices. Participant (P4) notes that documentation “for running a model on multiple GPUs” is “not clear.” vi. rq2: what ptm attributes facilitate ptm reuse? Finding 2: For Traditional attributes, Popularity is most helpful. Provenance, Reproducibility, and Portability are the three DL-specific attributes we should consider. Here we would like to learn about what sort of information is useful to engineers who reuse PTMs. We asked about two types of attributes here: traditional and DL-specific attributes. A. Traditional Attributes
In the interview, we asked about whether the traditional attributes as offered by traditional package registries, such as NPM [68], are helpful in DL model registries. Almost all participants highlighted the importance of popularity in DL model registries. For example, P12 stated that a PTM with “lots of downloads” means that “it could be a good start point to try”. P5 mentioned that “popularity usually goes side by side with maintenance and can indicate the quality”. Some participants thought that quality and maintenance are also very useful. P2 said it is important to “know that it is constantly maintained and does not have many open issues”, and pointed out that good maintenance means “if the code is being updated or if you raise a bug, then someone will help you out”. This is highly important because “you are relying on someone else” and “you want to build that trust factor”. However, some participants think that maintenance and quality are less useful. Recall from §V that most reuse scenarios are fine-tuning on new datasets or tasks. Provided that the model is fine-tunable, some participants mentioned that maintenance and quality metrics are “less useful in downstream tasks”. P12 suggested that maintenance may be less relevant because of the cost of making changes — “it is really hard to modify the PTM...there were some issues during pretraining” in a large language model, but the model has not been retrained “because it is too large”. B. DL-specific Attributes We added three DL-specific attributes: provenance, reproducibility, and portability. The participants provided the relevant factors for each attribute. Table I also indicates the factors for each attribute which were mentioned by multiple participants. Most participants mentioned that these three attributes can cover all the aspects they would consider. Provenance Some Hugging Face PTMs provide many provenance metrics, such as information about original paper, dataset, and architecture. However, these are not detailed enough to fully address model reuse challenges. P3 and P9 would like to see “visualization of model architecture” and the “explanation of changes” compared to the paper. P2, P4, P6, P8 mentioned the importance of more details of datasets because “different authors process data differently, so it cannot be easily compared”. P10 and P12 highlighted the importance of training logs because they would like to see “how the PTM was generated” based on the training checkpoints and scripts. Reproducibility Reproducibility is the most problematic aspect of PTMs. The reproducibility issues mainly come from two aspects: (1) the configuration of training (2) the understanding of model. For the training configuration, the participants tend to care more about the hardware specification (e.g., types, memory), training configuration (e.g., training scripts, hyperparameters). As a result, they think environment image would be helpful to help them easily configure the settings and make the model more reproducible. In terms of understanding of the models, different kinds of demos (e.g., Notebook demo, Fine-tune demo) and better documentation would be helpful. Portability Different models have different deployment constraints, which makes understanding the portability of PTMs helpful for engineers. Similar to reproducibility, the portability factors include hardware specification and environment. Moreover, for deployment, latency and framework support aspects are essential. For example, the inference time and cost of computational resources could be different in different platforms, as mentioned by P2 and P6. This information can help engineers understand the portability of PTMs. P3 mentioned that “automate[d] creation of other formats of the model for different hardware” could also be very helpful for the deployment. The quantized model is also “helpful for continuous deployment and fine-tuning”, as mentioned by P5. As a result, he also suggested the development of automated quantization. Some participants also mentioned the fine-tuning instructions, which help them determine whether a model can be used in specific tasks. For example, P12 would like to adapt language models to handle programming languages to improve their software testing, and the fine-tuning instructions can help him on deciding which model they should consider. P6 also mentioned that if the model registries can provide a “cost estimation for different servers” (e.g., different machine classes on a Cloud service provider). Moreover, P1 and P3 both said that “licensing should be explicit to industry users”. vii. rq3: what are the challenges of ptm reuse? Finding 3: Three common challenges for PTM reuse are missing attributes, discrepancies between claims and actual performances, and model risks (e.g., privacy issues and unethical models) (Table III). Missing Attributes Missing attributes are identiifed as the most challenging problem. Almost every participant mentioned that there are missing details in the model registries, including datasets, licensing, model details, robustness, and interpretability. The attributes are missing for multiple reasons: Insufficient documentation is one reason. P5 and P7 observed “missing details of models” in model registries. For example, P1 and P11 found the “performances of the published models are unclear” in the model registries. P8 suggested that the reason for under-documentation in Hugging Face is that PTM authors can upload any model; Hugging Face does not enforce any form of documentation. Another reason is that the PTM authors occasionally do not measure the robustness and explainability of the models—and model registries do not provide an automated approach to measure such attributes. Discrepancies Existing discrepancies are another key challenge mentioned by most participants. P7 pointed out that “some of the models are over-promising”. P2 indicated another reproducibility issue: the “model names are not named correctly and sometimes the provided scripts are broken”. These discrepancies could result in a waste of time and efforts. P5 pointed that another reason for this kind of problem is that training configuration details (i.e., hyper-parameters) are hard to find. P6 also mentioned that some authors only provide a script instead of providing the actual fine-tuned model and corresponding performances in Hugging Face due to the sensitivity of these results. P8 and P9 indicated that they sometimes follow the provided steps, including the models and
datasets—even the hardware configurations—and still could not reproduce the results claimed by the PTM authors. Model Risks There exist potential risks for PTMs in the model registries, including privacy and ethics aspects. We discussed in §II that prior studies have identified many risks of PTMs. The participants mentioned both internal and external problems in DL model registries. Internal risks often involve privacy problems of models and data. P2 mentioned that when using the models from model registries, “the model deployment and data transmission are not in their control”. They could not directly deploy the model provided by Hugging Face because it is “unreliable to send” their sensitive dataset by Hugging Face inference APIs. P8 mentioned that if a model “is trained with malicious intents. It could have a lot of consequences in the real world”. This indicates the potential risks of a malicious model being uploaded to model registries. A PTM can be used for unethical or nefarious purposes. P8 gave an example of a chatbot created by training on a racist discussion forum. This model “created a huge mayhem” because it was publicly released [77]. P10 observed that it is hard to know “what exactly generated the model because most training logs are missing” — the internal biases are concerning and could potentially make the model a BadNet [59]. viii. rq4: to what extent are the risks of reusing ptms mitigated by hugging face defenses? Finding 4: Although Hugging Face offers mitigations for many risks, these mitigations are incomplete or not widely adopted (Table IV). Model information can be missing or inaccurate due to the self-reporting nature of model metrics (Figure 7). These risks make the existence of malicious models possible in the model registries. Our interview data identified a range of challenges (§VII). Incomplete or inaccurate data about PTMs was most common. Engineers also expressed concern about malicious or unethical models, e.g., “BadNets” [59]. These findings led us to examine the Hugging Face defenses that mitigate these risks. We adapted the STRIDE methodology [72] to systematically measure the potential risks in Hugging Face, beginning with an analysis of the dataflow involved in collaborating on Hugging Face. The identified risks are shown in Table IV. A. Hugging Face PTM Dataflow Model
Based on our analysis of Hugging Face’s Hub and client libraries documentation, we made a dataflow diagram as
shown in Figure 5, to represent the how models are created and shared. This allows us to visualize the dataflow from model contributors to users, which involves the developing, releasing, and accessing of PTMs on Hugging Face. The common unit of reuse on Hugging Face is the repository, classified into datasets (input/output data for supervised or unsupervised learning) and models (PTM architecture, weights, and configuration, cf. Figure 2). All Hugging Face repositories have automated and manual risk mitigations to limit the spread of malware or malicious models. These include organization verification, user permission models, commit hash checkout, and model cards (Figure 5). Additionally, some PTMs depend on other datasets or PTMs which can expose themselves to outside risks. B. Risk Analysis
Hugging Face implements six risk mitigations. These are organization verification, PTM documentation, GPG commit signing, a user permission model, automatic malware scanning (ClamAV), and utilizing a commit hash to checkout a model. While commit hash checkouts are not easily measured (performed by users on their own machines), we measured the use and scope of the others within the Hugging Face ecosystem. We detail our results on organization verification, PTM documentation, user permission model, and GPG commit signing. Following the risk analysis of traditional package registries [10], we also examine the potential impact of dependencies. The result for ClamAV was uninteresting as no (zero) PTM packages were flagged by ClamAV. Organization Verification Hugging Face allows an organization to increase trust by verifying its identity, demonstrating to Hugging Face that an organization controls an associated web domain. We counted the number of verified organizations via web scraping. Out of 6,243 organizations, only 199 (3.19%) were verified. With such a small percentage of verified organizations, users cannot determine the legitimacy of unverified organizations. The low adoption rate raises the risk of Spoofing: malicious users may masquerade as real organizations, similar to typosquatting [10, 78]. This lack of adoption is concerning because organizational reputation was cited as a factor under the Provenance attribute (Table I). The risk of such squatting attacks can be greater for PTM packages than for traditional ones, because new niches in the ecosystem accompany every new state-of-the-art model. A malicious actor could identify missing PTMs in the cross
product of (architecture, dataset) and provide EvilModels [62] in that niche, pretending to be a legitimate organization. Dependencies The dependencies of PTMs pose potential risks. Malicious models can be injected directly via data manipulation [61], or indirectly via weight poisoning [60]. Models released through Hugging Face may be vulnerable through their dependencies on model architectures, the associated weights for those architectures, and datasets. P5, P8, P10 stated that they fine-tune PTMs on a daily basis, implying that an attack could have a rapid impact. Figure 6 shows that the Universal Dependencies dataset [79] is the most popular dataset on Hugging Face, with 6,834 models depending upon it. The distribution of models that depend on a particular architecture is similar to Figure 6. We found that the BERT [80] architecture is the most popular architecture on Hugging Face presently, with 10,247 models depending upon it. Hugging Face models have the potential to be trained off of multiple datasets as well. Our analysis of the existing Hugging Face models shows that many models depend on the works of others that can be maliciously tampered with. In tampering with these dependencies, BadNets [59] and unethical models [77] can be created, which could affect downstream PTMs. PTM Documentation Figure 7 shows the distribution of missing documentation in Hugging Face. The highest proportion of models that make performance claims in machine-parseable documentation (YAML) was from the token-classification task, with 17% of models meeting the criteria. We found that 26,192 models belong to various tasks (represented by other) where only 12 (0.05%) PTMs provide machine-parseable claims about the PTM. Due to the sparse usage of performance claim reporting, there can be potential risks of Repudiation: model performance can be obscured, misreported, or misleading. Some models report their performance in plain texts or tables, and are hard to identify by the users. It is also common for documentation to omit any performance claims or to refer the user to read an associated research paper for more information about performance, without assurance that this is the same model tested in the research paper. Some popular PTMs are poorly documented as well. The language model SpanBERT/spanbert-large-cased is the 9th most downloaded PTM on Hugging Face and receives 6.9 million monthly downloads, yet has no model card. Figure 7 supports that missing attributes is a real challenge existing in the
Hugging Face DL model registries. The lack of transparency reduces the trustworthiness of the models and increase the potential risks of malicious models. GPG Commit Signing Hugging Face provides a verification tag for commits that have been signed with a GPG key. By signing with a GPG key, users are providing verification that they have signed their commits, and not as someone masquerading as them. This feature allows users to accurately trace back the changes to the PTM packages. Using the HFTorrent dataset (§IX), we analyzed the usage of commit signing within Hugging Face model repositories. Out of 63,182 model repositories, we found 132 (0.21%) repositories within the dataset implemented signed commits. Additionally, only 2 verified organizations have at least one repository with signed commits. This indicates that potential attackers can be contributing malicious code, implementing a BadNet [59] or EvilModel [62] under a pseudonym, or manipulating the git commit history to hide malicious activity. The limited usage of GPG commit signing exposes models hosted on Hugging Face to Spoofing, Tampering, and Repudiation risks. First, unsigned Hugging Face models are vulnerable to Spoofing since attackers could make commits under the alias of a legitimate maintainer. Second, these models face the risk of Tampering because attackers could contribute malicious code or edit commit history. Finally, unsigned models could also risk repudiation since a lack of verified commit history allows an individual to deny actions within a repository. User Permission Model Hugging Face has a standard approach of users and organizations (Figure 5). One shortcoming of the permission model is that Hugging Face organizations violate the principle of least privilege [81]: an organization member with Write privilege can modify any PTM owned by the organization. Therefore, an attacker could contribute malicious code, thereby raising the risk of Elevation of Privileges. ix. the hftorrent dataset Our analysis in §VIII relied in part on measurements of the PTM packages in Hugging Face. To reduce the impact on the Hugging Face service during our measurements, we took a snapshot of 63,182 PTM packages in the Hugging Face registry. This snapshot, the HFTorrent Dataset, is included in the artifact accompanying this paper. An improved version of this dataset is now available [82]. We hope these data facilitate further research on PTM packages, similar to the impact of the GHTorrent [83] and SOTorrent [84] datasets. Creation process We initiated a copy of all PTM packages in the Hugging Face registry. Copies were taken between August 15th and 16th, with rate limiting to avoid abuse of the Hugging Face registry. 186 (0.3%) of the copies failed, caused, we believe, by concurrent changes in package names. Dataset contents The HFTorrent dataset contains the repository histories of 63,182 of the PTM packages available on Hugging Face as of August 2022. They are provided as bare git clones to reduce space, resulting in a compressed footprint of ∼20 GB. Each PTM package can be reconstructed to its most recent version, including the model card, architecture, weights, and other elements provided by the maintainers (cf. Figure 2). Further information is available in our artifact. x. discussion and implications A. Integrating the findings
Our qualitative and quantitative studies provide a deeper understanding of the practices and challenges for DL model registries. In §V we obtained a general reuse process (Figure 4) which is complemented by the specific details for Hugging Face (Figure 5). In §VI we studied how the theoretical attributes of reproducibility and provenance can affect the decision-making process of PTM reuse (Figure 4). These attributes are partially operationalized in Hugging Face by aspects measured in §VIII: organizations and verification, PTM documentation, GPG commit signing, and dependencies. For example, our results in §VIII imply that although PTM
reusers value the provenance of models, this provenance is actually untrustworthy in Hugging Face due to the low adoption of verified organizations and commit signing. In §VIII we measured risks based on the identified challenges from §VII. For example, in §VII we qualitatively learned that documentation may be missing or have discrepancies. In §VIII we quantitatively measured that model documentation is missing or inadequate for 80% of PTMs. B. Comparison to traditional package registries
Our qualitative analysis in §V—§VIII sheds light on the differences between model registries and traditional registries, in terms of decision-making, attributes, and potential risks. Decision-making Our results indicate that the decision-making process of PTMs (Figure 4) is more complex than traditional packages [23, 85, 86], both in terms of the assessment and evaluation. Traditional software package reuse is integrated throughout the software development process [87] to improve productivity [85]. Our result shows that DL engineers behave similarly with PTMs. However, PTM reusers have to perform more complicated assessment and evaluation during decisionmaking process. PTMs are hard to evaluate and compare with others, as indicated by P3, P6, P8, P10. Moreover, Figure 4 involves three back edges (loops between selection and assessment), while the decision-making process for the traditional software reuse reportedly involves fewer iterations [13, 23]. Moreover, we can see in the selection stage of Figure 4, different factors are considered, including the dataset availability and cost. Traditional software reuser tend to consider more about the ease of use, and such detailed information are less needed by PTM reusers. Wang et al. suggest that developers should use the usage statistics to guide the evolution [88]. Similarly, PTM providers should consider the practices and challenges based on our qualitative data, and utilize it as a guidance to improve the PTMs in DL model registries. Attributes Our interview data indicated the significance of traditional attributes (i.e., popularity, quality, and maintenance) and the requirements for DL-specific attributes. For Traditional attributes, Popularity is most helpful for PTM reuse, while Quality and Maintenance are less useful compared to traditional packages (§VI). This differs from traditional software reusers, who consider quality and maintenance of packages as important as popularity when selecting and reusing the packages [25, 68, 89]. This difference comes from the expensive
cost and data-driven nature in PTMs. It is hard for PTM users to directly obtain and employ an existing model. In contrast with taking a package from NPM or PyPI and directly reuse it in the codes, PTM reuse requires a deeper understanding and knowledge of how a model works. However, “there are not quite reliable methods to measure the explainability of PTMs yet”, as indicated by P10. Therefore, to better reuse the PTMs, the engineers need more information from the model registries, which result in the requirements for DL-specific attributes. Potential risks The dependencies, maintainers, and reported issues can help analyze the security risks in software registries [10]. Prior work indicates that developers should manage the upstream dependencies and minimize the impact on downstream tasks [86]. Recently, Jiang et al. empirically studied the maintainers’ reach of model registries [19]. However, the results here suggest that there are different risks within model registries, such as Spoofing, Tampering, Repudiation, and Elevation of privileges (Table IV). These risks can have varying degrees of impacts on the reusability of PTMs: The unaware discrepancies “do play a huge role” (P8, P11, P12). They may not only “hinder developer progress”, but also change the accuracy and robustness of downstream PTMs [61]. Moreover, our STRIDE analysis (§VIII) indicates multiple risks and the potential to introduce vulnerabilities [59, 60]. C. Implications
Compared to well-studied traditional package registries (e.g., NPM, PyPi), the use and study of DL model registries is still in its infancy. Our empirical study of Hugging Face model registry informs future directions on model audit, infrastructure, PTM standardization, and adversarial attack detection. Model audit Our results in §VI and §VII shows that one major challenge of PTM reuse is the missing attributes in model registries. The most important attribute is the performance of PTMs which would significantly affect the reusability of models. Though recently Hugging Face released their automated validation tool [90], it is still not able to satisfy the requirement of engineers. P8 stated that “Robustness and Explainabiltiy are very key factors to consider when you are deploying ML models in the real world.” which are important for the eventual goal of model transparency [91]. The proposed DL-specific attributes should also be measured and provided. Our study indicates the importance of these attributes (§VI) and identify the corresponding factors. We inform researchers on developing formulas and automated tools to automatically calculate the score of each attribute and integrate them into the model registries, similar to the pqm scores (i.e., popularity, quality, and maintenance) in [40]. We envision that future directions should consist of largescale measurements of PTMs or of encouraging model registries to change their PTM release requirements so that it would be easier for users to audit models by themselves. Infrastructure The infrastructure of model registries can be improved from different aspects. P2 mentioned that the badge mechanism would be helpful for communicating the missing
attributes (e.g., reproducibility), similar to continuous integration badges provided by GitHub [92]. P6 mentioned a unified fine-tuning process could also assist engineers. Moreover, multiple participants highlighted the importance of tools for automatically creating quantized models or converting models into different formats. These tools could improve PTM portability and thus support model deployment. Some interview participants (P6, P9) mentioned that they select PTMs first based on their experience, and then based on evaluation metrics. P10 mentioned that he reuses PTMs to understand the “the generalizing behavior of fine-tuned models”. This envisions development of PTM recommendation systems, which can sort the models by scores calculated by model performance in downstream tasks, or predicted by another ML model. A similar system can help to reduce the work for ML engineers and could be integrated as part of the AutoML pipeline for PTM reuse [93]. PTM Standardization Our results on §VI shows that, the model registries should include the training logs and corresponding checkpoints. This information can help reusers better understand the PTM and improve the provenance and reproducibility. As associated research opportunities, such artifacts can be costly to create and store, and it is unclear how engineers can best apply them. Beyond model provenance, another opportunity for standardization is in the model format itself. P5 said that “many PTMs are on PyTorch or TensorFlow” but they would like to use ONNX format models which could make the deployment easier for them. However, due to the rapid appearance of new operators [94], ONNX could not support all of them, especially for state-of-the-art models [95]. Knowing the compatibility of PTMs in model registries with standardized formats such as ONNX would also engineers make better decisions and save their time. We suggest future work examining development challenges in the ONNX framework. Adversarial attack detection One of the major challenges for PTM reuse are adversarial attacks, as shown in §V. Attacks can be harmful to both the PTM reusers and the downstream application users. Although Hugging Face employs ClamAV [96] for malware scanning, this only detects traditional attacks, not new attacks such as BadNets [59]. As a result, we suggest future studies working on automated detection of toxic models and poisoned datasets. Integrating these detectors into model registries can largely improve their trustworthiness. xi. threats to validity Internal Threats Our choice of research methods potentially threatens the validity of our investigation of PTM reuse practices and challenges (RQ1-3). Our results here are derived solely from interview data but not generalized via a survey instrument. As a mitigation, our measurements of the Hugging Face ecosystem (RQ4) substantiate many of the interview participants’ concerns. In addition, we note that the participants of greatest interest are those who also make the greatest use of PTMs, i.e., presumably those with PRO accounts on Hugging
Face. 9 of our 12 participants have PRO accounts, representing 1% of the PRO user population. Another internal threat is the reliability of our framework analysis on the interview data. Our framework analysis might be biased by our understanding and transfer of concepts from traditional software to PTMs. This conceptual framework helps us tease out similarities and differences in the PTM context, although we recognize that our interviews might reflect our biases and perspectives and therefore bias participants to a certain way of thinking. To mitigate bias, we asked if the participant had anything to add in terms of each theme in our framework throughout the interview, and some did so. External Threats Our study examines only one DL model registry, Hugging Face. We note, however, that examining a single package registry, e.g., NPM, is fairly common in the literature. For PTMs, focusing on Hugging Face is sensible, since it is the only open DL model registry and has an order of magnitude more models than other registries. Our results may not generalize to other DL model registries, but given the relative importance and growing influence of Hugging Face it is unclear whether this is a concern. Another external threat is the saturation of our interview study because of the interview study’s sampling approach (one PTM registry) and size (12 participants). Within our sample, we saw a high degree of agreement. The saturation of our interview study was achieved after 7 participants (§IV-A)
ML researchers identified many uses of PTMs (§II-B), but our participants only employed a subset related to fine-tuning: transfer learning, quantization and pruning. This may pose a threat to external validity. Our random sampling approach may bias us towards high-probability use cases. One interpretation of our data is that fine-tuning is a popular approach in practice, which would motivate greater study of PTM finetuning relative to more theoretical applications. xii. conclusion We conducted the first empirical study of PTM reuse in the Hugging Face DL model registries. Based on interviews with 12 practitioners, we defined the decision-making workflow for PTM reuse, and identified three challenges, including missing attributes, discrepancies, and model risks. To substantiate our qualitative data, we further investigated into useful attributes and potential risks in the Hugging Face ecosystem. We unveiled risky engineering practices in the Hugging Face ecosystem, particularly a lack of signatures in the PTM supply chain. REPRODUCIBILITY AND RESEARCH ETHICS
