Macrocyclic peptides are an emerging therapeutic modality, yet computational approaches for accurately sampling their diverse 3D ensembles remain challenging due to their conformational diversity and geometric constraints. Here, we introduce RINGER, a diffusion-based transformer model for sequence-conditioned generation of macrocycle structures based on internal coordinates. RINGER provides fast backbone sampling while respecting key structural invariances of cyclic peptides. Through extensive benchmarking and analysis against gold-standard conformer ensembles of cyclic peptides generated with metadynamics, we demonstrate how RINGER generates both high-quality and diverse geometries at a fraction of the computational cost. Our work lays the foundation for improved sampling of cyclic geometries and the development of geometric learning methods for peptides. 1 introduction Macrocyclic peptides are an important therapeutic modality in modern drug discovery that occupy a unique chemical and pharmacological space between small and large molecules [1–3]. These cyclic peptides exhibit improved structural rigidity and metabolic stability compared to their linear counterparts [4], yet retain key conformational flexibility and diversity to bind shallow protein interfaces [5]. However, computational approaches for modeling their structural ensembles remain limited compared to small molecules and proteins in terms of computational speed, accuracy (sample quality), and conformational diversity [6]. Critically, scalable and accurate tools are necessary to enable rational design of macrocyclic drugs; access to these tools can significantly impact optimization of key properties including binding affinity [7, 8], permeability [9–11], and oral bioavailability [12]. Several key challenges hinder fast and effective macrocycle conformer generation: 1) Macrocyclic peptides exhibit diverse molecular structures and chemical modifications, including varying ring size, stereochemistry, N-methylation, and more [13]. Their structural diversity, along with the increased number of rotatable bonds, results in a vast conformational space that is considerably more expensive to sample computationally. 2) Macrocycles are subject to complex non-linear constraints due to ring closure. The atomic positions, angles, and dihedrals of the macrocycle backbone are highly interdependent, and additional complex intramolecular interactions make this process inherently difficult to model [14]. 3) Experimental X-ray and NMR structures for macrocycles are lacking (∼ 103) in comparison to small molecules (∼ 106 in the Cambridge Structural Database [15]) and proteins (∼ 105 in the Protein Data Bank [16]). The scarcity of available experimental data has made it difficult to integrate observational data to improve structural predictions or train machine learning-based approaches. Together, the vast conformational space combined with limited data make
Preprint. Under review. ar X
iv :2
30 5. 19 80
0v 1
[ q-
bi o. B M
] 3
modeling and sampling of macrocycles not only conceptually challenging, but technically challenging due to computational cost. Approaches that can accurately generate diverse conformations at scale would dramatically improve our ability to rationally design and optimize macrocycles. To address these limitations, we introduce RINGER (RINGER Generates Ensembles of Rings), a deep learning model designed specifically for sequence-conditioned macrocycle conformer generation (Figure 1) that efficiently samples realistic angles and torsions (i.e., internal coordinates) for macrocyclic peptides. RINGER merges a transformer architecture that naturally captures the physical equivariances and invariances of macrocyclic peptides with a discrete-time diffusion model to learn highly-coupled distributions over internal coordinates. We demonstrate how RINGER simultaneously achieves excellent performance in sample quality over angular and torsional profiles while maintaining excellent RMSDs relative to gold-standard conformer ensembles generated with the Conformer-Rotamer Ensemble Sampling Tool (CREST) [17]. We summarize our contributions as follows:
• We propose a new framework, RINGER for conformer generation of macrocycle backbones based on efficiently encoding ring geometry using redundant internal coordinates. Our model naturally handles the cyclic nature of macrocycles and chiral side chains with both Land D-amino acids. • We propose a simple solution to recover Cartesian coordinates from redundant internal coordinates that satisfies ring constraints using a sequential least-squares optimization and demonstrate that it works well in practice. • We benchmark RINGER extensively against state-of-the-art physics- and machine learningbased algorithms to demonstrate how our approach better captures complex distributions of macrocycles and achieves excellent sample quality and diversity compared to existing methods. 2 background and related work Our work builds on small-molecule conformer generation and protein structure modeling to create a framework for macrocycle conformers. Below, we briefly summarize related work. Physics and Heuristic-based Conformer Generation for Macrocycles Physics-based and heuristic-based algorithms remain the state of the art for macrocycles and have required special considerations compared to drug-like small molecules due to ring-closing constraints. The open-source cheminformatics library RDKit leverages distance geometry algorithms for small-molecule conformer generation (ETKDG) [18], with improved heuristic bounds for macrocycles (ETKDGv3) [19, 20]. Similarly, commercial conformer generation algorithms such as OpenEye OMEGA [21, 22] in macrocycle mode use a distance geometry algorithm based on 4D coordinate initialization to provide diverse conformers [23], as their torsion-driving approach is incompatible with ring closure. Similarly, low-mode [24, 25] or Monte Carlo [26] search methods combined with molecular dynamics have been found to be effective at sampling macrocycle conformations, particularly when combined with force field optimizations as demonstrated in Schrödinger’s MacroModel [14] and Prime MCS [27]. These approaches have been tuned with expert knowledge and torsional libraries to maximize agreement with observed experimental structures. The open-source CREST package [17] leverages iterative metadynamics with a genetic structure-crossing algorithm (iMTD-GC) to explore new geometries, and can be considered a gold-standard for generating diverse ensembles of drug-like molecules. In this work, we use the recently-published CREMP [28] dataset, containing high-quality, CREST-generated ensembles, representing over 31 million macrocycle geometries (see Section 4.1 and Appendix B for more details). One key limitation of these approaches is high computational cost and difficulty in scaling; in general, conformer generation is 103 – 105× more computationally expensive compared to a drug-like small molecule due to the increased number of rotatable bonds and their ring-closing constraints (e.g., generating a conformational ensemble of a macrocyclic hexapeptide with CREST requires an average of 14 hours [28]). These approaches become increasingly challenging when kinetic or molecular dynamics approaches are used with explicit solvation [29, 30]. Generative Approaches for Small Molecule Conformer Ensembles Recent work with deep generative models has focused on improved sampling of the conformational landscape of small molecules. For example, Mansimov et al. [31] propose a conditional graph variational autoencoder (CGVAE) approach for molecular geometry generation. Simm and Hernandez-Lobato [32] report conditional generation of molecular geometries based on distance geometry. Xu et al. [33] leverage normalizing flows and energy-based modeling to help capture the multimodal nature and complex dependencies of small molecule space. More recently, Xu et al. [34] report GeoDiff, an equivariant diffusion-based model that operates on Cartesian point clouds. Although GeoDiff provides strong results, sampling is costly and requires 5,000 time steps. Recent reports have also drawn inspiration from physics-based conformer generation to leverage the rigid-rotor hypothesis, which treats bond distances and angles as fixed, and torsional angles of rotatable bonds are independently sampled, assuming little or no interdependence between torsions [35]. These include GeoMol [36], an SE(3)-invariant machine learning model for small molecule conformer generation that leverages graph neural networks, and EquiBind [37] which performs conditional generation on protein structure. Recently, Jing et al. [38] report Torsional Diffusion, a diffusion model that operates on the torsional space via an extrinsic-to-intrinsic score model to provide strong benchmarks on the GEOM dataset [39]. Importantly, these methods do not address the challenge of highly-coupled torsions within cyclic systems and either propose complex ring-averaging processes [36] or ignore sampling of cyclic structures all together [38]. Protein Structure Prediction and Diffusion Significant progress has been made recently in protein structure prediction with the advent of methods such as AlphaFold2 [40] and RoseTTAFold [41]. However, structure prediction methods have predominantly focused on deterministic maps to static output structures rather than on sampling diverse structure ensembles. Recently, several papers have developed diffusion-based approaches for protein generation based on Euclidean diffusion over Cartesian coordinates [42, 43] or backbones as in FoldingDiff [44], with an emphasis on structural design. Our work builds on FoldingDiff, which parameterizes structures over internal backbone angles and torsions and relies on the natural extension reference frame (NeRF) [45] to perform linear reconstructions. However, as we demonstrate below, naive linear transformations fail to address the ring constraints for macrocycles. Moreover, FoldingDiff focuses on unconditional generation of protein backbones, whereas our focus here is conditional generation. Machine Learning Approaches for Macrocycle Conformer Ensemble Generation Despite the many approaches focused on small molecules and protein structure generation, there are few efforts
in macrocycle structure prediction. Most notably, Miao et al. [46] recently disclosed StrEAMM for learning on molecular dynamics of cyclic peptides using explicit solvation. StrEAMM is a linear model that predicts local backbone geometries and their respective 1,2- and 1,3-residue interactions to provide excellent ensemble estimates of homodetic hexapeptides. However, the model is not naturally inductive and is not natively extensible to other macrocycle ring sizes and residues. Fishman et al. [47] recently developed a more general framework for diffusion models on manifolds defined via a set of inequality constraints. However, they only investigate the conformational ensemble of a single cyclic peptide as a proof-of-concept using a reduced α-carbon representation. 3 ringer: problem statement and methods  3.1 problem definition: conditional macrocycle conformer generation The core objective of our work is to model the distribution of conformers for a macrocyclic peptide with a focus on backbone structure. Given a macrocycle graph G = (V, E), where V is the set of nodes (atoms) and E is the set of edges (bonds), and n = |V| our goal is to learn a distribution over the possible conformers. Let C = {c1, c2, . . . , cK} be the set of conformers, where each conformer ck ∈ C represents a unique spatial arrangement of the atoms V . Our task is to learn the distribution p(C | G), which represents the probability over the conformer ensemble C given a molecular graph G. Learning and sampling from this complex distribution is inherently challenging for most molecules, and is further complicated in macrocycles due to the highly-coupled nature of ring atoms. A perturbation to one part of the ring generally perturbs the others. Consequently, any model must account for the interdependence between atoms due to the cyclic constraints. Given this problem, a good generative model ideally satisfies a few key properties: 1) Naturally encodes the physical and structural aspects of macrocyclic peptides. For example, cyclic peptides with only standard peptide bonds (i.e., homodetic peptides) do not have a natural starting residue and hence exhibit cyclic shift invariance, e.g., cyclo-(R.I.N.G.E.R) is identical to cyclo-(I.N.G.E.R.R), where each amino acid is denoted by its one-letter code with “cyclo” indicating cyclization of the sequence. 2) Captures multimodal distributions and complex, higher-order interactions such as the strong coupling between atomic positions in the ring. 3) Samples high-quality and diverse conformations from p(C | G) that faithfully capture realistic geometries while respecting the underlying conformer distribution. 3.2 representing macrocycle geometry: redundant internal coordinates Conformer geometries are defined by their set of Cartesian coordinates for each atomic position and can hence be modeled using SE(3)-equivariant models to learn complex distributions. However, Euclidean diffusion requires modeling the many degrees of freedom; and, in practice, can require many time steps to generate accurate geometries [34]. Moreover, realistic conformations are highly sensitive to the precise interatomic distances, angles, and torsions—although this information is implicit in the Cartesian positions, explicitly integrating these quantities into a model can provide a strong inductive bias and accelerate learning [48]. Borrowing from molecular geometry optimization [49], protein representation [45, 50, 51], and inverse kinematics [52], we adopt redundant internal coordinates that represent conformer geometries through a set of bond distances, angles, and torsions (dihedral angles), i.e., C ≡ {D,Θ, T }. In particular, this simplifies the learning task, as bond distances can be approximated as fixed distances with little loss in accuracy [21, 38, 44], and internal angles typically fit a narrow distribution. Importantly, these coordinates define an internal reference frame that readily encodes complex geometries including ring chirality. Moreover, this approach obviates the need for complex equivariant networks and enables the use of simpler neural architectures [44]. Hence, our generative process can be reformulated as learning the distribution p({Θ, T } | G;D) using known bond distances for reconstruction back to Cartesians (Figure 1). 3.3 deep probabilistic diffusion models for sampling internal coordinates Denoising Probabilistic Models Recent works on deep denoising probabilistic models have demonstrated excellent generative performance for complex multimodal data [53–55], and have been successfully applied to both small molecules and proteins [34, 44]. In particular, we use the discrete-time diffusion model from Wu et al. [44] that formulates the forward transition probability using a wrapped normal distribution, q (xt | xt−1) = Nwrapped ( xt; √ 1− βtxt−1, βtI ) , instead of a
standard normal distribution [38], where xt represents the noised internal coordinates (bond angle and torsion) at time step t. We train a diffusion model, pΞ(xt−1 | xt), by training a neural network to predict the noise present at a given time step (for full details, see Appendix C). During inference, we sample xT from a wrapped normal distribution and iteratively generate x0 using pΞ(xt−1 | xt). The sampling process is further detailed in Appendix D.
Encoder Architecture Macrocycles exhibit extensive coupling of their residues due to torsional strain and intramolecular interactions such as hydrogen bonds. Here, we use a standard bidirectional transformer architecture [56, 57] using self-attention to learn the complex interactions between atoms. Unlike standard sequence models for linear data, macrocycles exhibit cyclic symmetry with no canonical start position. Thus, we design a bidirectional, relative positional encoding, pKij , inspired by standard relative encodings [58] to reflect this cyclic invariance (see Appendix A for notation):
zi = n∑ j=1 αij ( vjW V ) , where αij = exp eij∑n k=1 exp eik
(1)
eij = viW
Q ( vjW K + pKij )T
√ dz
with pKij = W D (i−j) mod n︸ ︷︷ ︸
forward
+WD(i−j) mod (−n)︸ ︷︷ ︸ backward
(2)
These cyclic relative position representations encode bidirectional edge relationships between each atom by specifying forward and reverse distances in the macrocycle. The relative position of any neighboring atom is uniquely defined by its forward and reverse graph distances in the embedding lookup WD. For conditional generation, we perform a linear projection of the features ai, corresponding to each macrocycle backbone atom and its side chain, and a separate linear projection of the angles and torsions xi = [θi, τi] and concatenate them as a single input to the transformer, vi = a′i ⊕ x′i. Notably, our diffusion model only adds noise to the angular component, xi. For unconditional generation, atoms are only labeled with their backbone identity (nitrogen, α-carbon, carbonyl-carbon) using an embedding that is added to the input. Model details are shown in Appendix E.
Ring Closing: Back Conversion to Cartesian Ring Coordinates Macrocycles with fixed bond distances contain three redundant torsional angles and two redundant bond angles. Whereas linear peptides and proteins can be readily converted into an arbitrary Cartesian reference frame through methods such as NeRF [45], these redundancies prevent direct transformation to unique Cartesians for cyclic structures. Adopting a sequential reconstruction method such as NeRF accumulates small errors that result in inadequate ring closure for macrocycles.1 Other studies have developed complex heuristics with coordinate averaging for ring smoothing [36], yet these approaches can distort the predicted geometries. In practice, we demonstrate that an efficient post-processing step works well with minimal distortion: we treat this as a constrained optimization problem using the Sequential Least Squares Quadratic Programming (SLSQP) algorithm [59] to ensure valid Cartesian coordinates while satisfying distance constraints:
ξ̂ = argmin ξ
∥θ(ξ)− θ̂∥2 + ∥w (τ(ξ)− τ̂)∥2 subject to: d(ξ) = dtrue (3)
Here, we find the set of Cartesian coordinates, ξ̂, that minimize the squared error against the internal coordinates θ̂ and τ̂ sampled by the diffusion process while satisfying bond distance equality constraints using known bond distances, dtrue, from the training data. The torsion error, τ(ξ)− τ̂, is wrapped by w(·) so that it remains in the [−π, π) range. Empirically, we demonstrate that this scheme recovers realistic macrocycles with high fidelity by evenly distributing the error across the entire macrocycle backbone (see Appendix F for additional details). Overall Generation Procedure Our model represents macrocycle backbones as cyclic sequences of redundant angles and dihedrals with fixed bond lengths. We train a discrete-time diffusion model to learn a denoising process over the internal coordinates, using a transformer architecture with an invariant cyclic positional encoding. At inference time, we sample from a wrapped Gaussian distribution to produce a set of angles and torsions, conditioning on the known set of atom features corresponding to the amino-acid sequence. In the final post-processing step, macrocycle geometries with Cartesian coordinates can be reconstructed through our constrained optimization using Equation (3). 4 experiments and results  4.1 experimental setup Dataset We train and evaluate our approach on the recently published CREMP dataset [28] that contains 36k homodetic macrocyclic peptides across varying ring sizes (4-mers, 5-mers, and 6-mers corresponding to 12-, 15-, and 18-membered backbone rings), side chains, amino-acid stereochemistry, and N-methylation. Each macrocycle in CREMP contains a conformational ensemble sampled with CREST [17], a metadynamics algorithm with genetic crossing built on the semi-empirical tight-binding method GFN2-xTB [60]. We perform stratified random splitting on the data, with a training and validation set of 35,198 molecules (948,158 conformers using a maximum of 30 conformers per molecule), which we split into 90% training and 10% validation, and a final test set of 1,000 molecules corresponding to 877,898 distinct conformers (using all conformers per molecule within the 6 kcal/mol energy threshold defined by CREST). Additional dataset statistics are shown in Appendix B. Training & Sampling All training is performed on the set of 35k peptides described above, using the 30 lowest-energy conformers per peptide. We train each model on a single NVIDIA A100 GPU for up to 1000 epochs until convergence (typically less than 100 epochs) using the Adam optimizer with 10 warmup epochs. Following work in small-molecule conformer generation [34, 36, 38], we sample 2K conformers for a macrocycle ensemble of K ground-truth conformers (median K = 656) and assess them based on the evaluation criteria below. For full training and sampling details see Appendices C and D.
Evaluation For unconditional generation, we use Kullback-Leibler divergence to measure the difference in sample quality. For conditional generation, we evaluate the quality of our generated macrocycle backbones using root-mean-squared-deviation (RMSD) between backbone atom coordinates, similar to previous work on small-molecule conformer generation. We use several metrics including Matching and Coverage [33, 36, 38], and for each we report recall and precision. We note that although RMSD is widely used to assess conformer quality, its utility for comparing backbones is more limited, as sampled backbones with highly unrealistic or energetically unfavorable torsions can exhibit low RMSD values. Therefore, we additionally report the torsion fingerprint deviation (TFD) [19, 61] to evaluate the quality of the torsional profiles. RMSD provides a measure of distance between two conformers based on a least-squares alignment of their respective atomic positions, while TFD gives a normalized measure of matched torsion angles between backbone geometries. Appendix I defines the evaluation metrics in detail. Baselines We provide benchmarks of our method against open-source and commercial toolkits RDKit ETKDGv3 (for macrocycles) [19], OMEGA Macrocycle Mode [62], and the SE(3) diffusion model GeoDiff [34]. For GeoDiff, we report results on a model retrained on identical macrocycle conformers (GeoDiff-Macro), as the base model trained on small molecules provided poor performance. Methods such as torsional diffusion [38] only alter freely rotatable bonds and cannot sample macrocycle backbones by design. See Appendix J for details about the baseline methods. 4.2 unconditional generation of macrocycles To understand whether this approach can learn the underlying distribution of macrocycle conformations, we first trained RINGER on macrocycle backbones in the absence of any residue or side-chain features and only providing ring-atom identity. From a design perspective, diverse backbone sampling alone can help drive inverse peptide design, where specific backbone geometries suggest important sequences. Figure 2 clearly demonstrates how RINGER accurately replicates both angles and dihedrals with tight fidelity across all residue atoms, both qualitatively from the plots and quantitatively as measured by the KL divergence. Furthermore, we generated Ramachandran plots [50] alongside our withheld test set to visualize the conditional dependencies between residue torsions. Notably,
RINGER recapitulates the critical modes of the distribution. Appendix K provides more fine-grained detail by visualizing distributions separately based on the number of residues in the macrocycle. 4.3 sequence-conditioned generation of macrocycles We subsequently focused on the challenge of sequence-conditioned generation to understand whether RINGER could effectively capture the complex steric and intramolecular effects that dictate macrocycle backbone conformation. Whereas our unconditional model above disregarded side chains, we now condition backbone generation on molecular features corresponding to each residue, including side-chain features, stereochemistry, and N-methylation (see Appendix E). Comparison of RINGER RMSD and TFD ensemble metrics against RDKit, OMEGA, and GeoDiff baselines are shown in Table 1. Here, recall quantifies the proportion of ground truth conformers that are recovered by the model, and precision quantifies the quality of the generated ensemble (also see Appendix L.2 for confidence intervals). We found that RDKit ETKDGv3 and OMEGA Macrocycle mode, both based on distance-geometry approaches, performed similarly across both metrics and achieved moderate recall with limited precision. To compare deep learning approaches, we trained a Euclidean diffusion model using the GeoDiff architecture on the CREMP dataset, and found a strong boost in recall with similar precision. We evaluated our approach with and without the post-processing geometry constrained optimization (Equation 3), as our raw, generated samples from RINGER may not satisfy realistic macrocycle distance constraints. As with unconditional generation, sequence-conditioned generation learns the data distribution with high fidelity as shown in Appendix L.1. “RINGER” in Table 1 corresponds to Cartesian geometries that were generated from the predicted angles by starting at one atom and setting bond distances, angles, and dihedrals sequentially. The starting atom was chosen such that the redundant bond distance in the ring most closely matches the true bond distance. “RINGER (opt)” refers to our post-processed geometries that satisfy the true bond distances exactly. Notably, both approaches achieve excellent recall and precision across both RMSD- and TFD-based scores compared to our baselines. Furthermore, post-processing to guarantee valid macrocycle geometries preserves excellent recall, albeit with slightly attenuated precision. Additionally, Figure 3 shows that RINGER outperforms all baselines over a wide range of thresholds used for evaluating Coverage. The plateau in RMSD precision (but not for TFD) of RINGER with post-processing is a result of the optimization converging to unrealistic geometries that nonetheless match the true torsions well. This motivates further development of methods to natively handle the cycle constraints as a future direction. Notably, our approach not only identifies better conformer ensembles, but provides increased sampling efficiency with only T = 20 time steps, compared to GeoDiff’s T = 5000 or FoldingDiff’s T = 1000 (see Table 7 in Appendix L.3 for additional analysis). Although our standard training protocol uses a diverse ensemble of k = 30 lowest-energy conformers per training molecule, we found that training with only the lowest-energy conformer (k = 1) still outperforms baseline methods. Increasing this number (k = 30, 100) notably increases recall with a clear trade-off in precision. These results highlight the excellent data efficiency and sample quality of our diffusion-based generation, and are a good illustration of the trade-off between precision (sample quality) and recall (ensemble diversity). 4.4 structural analysis of generated macrocycles Although RMSD and TFD give a quantitative evaluation of performance, we also analyzed individual ensembles to understand the qualitative differences in conformer generation processes (Figure 4). Notably, the two macrocycles shown possess distinct sequences that result in distinct Ramachandran plots and conformations. As shown in Figure 4 (top panels), most ground truth conformer ensembles exhibit relatively tight distributions characterized by a distinctive set of ϕ, ψ angles. Although RDKit, OMEGA, and GeoDiff can identify relevant low-energy conformers (albeit with slight errors), the overall sampling process generates unrealistic distributions. In contrast, RINGER recapitulates not only the ground state geometry with excellent accuracy, but better captures the entire ensemble
distribution. These results demonstrate how RINGER better models sequence-dependent geometries to achieve strong performance. 5 limitations and future directions Our studies demonstrate the potential for diffusion-based models to tackle key limitations in constrained macrocycle generation, but they are not without limitations. First, our current work has focused on the CREMP dataset, which is currently limited to homodetic, 4-, 5-, and 6-mer macrocycles with canonical side chains in implicit chloroform solvent. Second, we have focused primarily on modeling diversity of backbone geometry. Finally, although we demonstrate the effectiveness of a standard, discrete-time diffusion process, our approach is not physically constrained to satisfy macrocyclic geometries and currently requires a post-optimization step. Despite these limitations, our work provides an important first step toward efficient generation of complex macrocycle geometries, and we anticipate its application toward more complex, conditional generation tasks. 6 conclusions In summary, we present RINGER, a new approach for generating macrocycle conformer ensembles that significantly improves sample quality, diversity, and inference. By leveraging specific benefits of diffusion-based models, we demonstrate how a transformer-based architecture with a cyclic positional encoding results in significant gains over Cartesian-based equivariant models and widely-used distance geometry-based algorithms for both unconditional and conditional structure generation. The present work paves the way for more efficient and accurate computational exploration of conformational
space. We anticipate that this approach will more broadly enable rational macrocycle discovery through further development. 7 code and data availability All code for training, sampling, and evaluation in this study are available at http://www.github.com/Genentech/RINGER. We include the exact training and test data splits and trained model. The CREMP dataset [28] is available for download from https://zenodo.org/record/7931445. Acknowledgments and Disclosure of Funding We thank Ben Sellers and Christian Cunningham for insightful discussions on macrocycles and peptide therapeutics. We also thank members of the Departments of Peptide Therapeutics and Discovery Chemistry for helpful feedback and discussions. This research is sponsored by Genentech, Inc. All authors are employees of Genentech, Inc. and shareholders of Roche. a glossary  b dataset description  c training details We use the discrete-time diffusion model from Wu et al. [44] that formulates the forward transition probability using a wrapped normal distribution,
q (xt | xt−1) = Nwrapped ( xt; √ 1− βtxt−1, βtI )
= 1
βt √ 2π ∑ k∈Zn exp
( − ∥∥xt −√1− βtxt−1 + 2πk∥∥2
2β2t
) (4)
instead of a standard normal distribution [38], where xt represents the noised internal coordinates (bond angle and torsion) at time step t. The diffusion model, pΞ(xt−1 | xt), parameterized by Ξ, reverses the process to denoise a wrapped normal distribution toward the data distribution. In the conditional setting, we further guide the diffusion process by learning pΞ (xt−1 | xt,G) in order to draw samples from the ensemble for a specific macrocycle, G. We use the same cosine variance schedule as Wu et al. [44] and Nichol and Dhariwal [63] for βt ∈ (0, 1)Tt=1, but with significantly fewer time steps (typically, T = 20). pΞ(xt−1 | xt) and pΞ (xt−1 | xt,G) are trained using the simplified objective from Ho et al. [54] to train a neural network, ϵΞ(xt, t), to predict the noise present at a given time step by minimizing a smooth L1 loss [64] wrapped by w(x) = (x+π) mod (2π)−π:
dw = w ( ϵ− ϵΞ ( w (√ ᾱtx0 + √ 1− ᾱtϵ ) , t ))
Lw = 1
N N∑ i=1
{ 0.5
d2w,i βL
if |dw,i| < βL |dw,i| − 0.5βL otherwise
(5)
with βL = 0.1π as the transition point between L1 and L2 regimes [44], αt = 1 − βt, and ᾱt = ∏t s=1 αs. We sample time steps uniformly from t ∼ U(0, T ) during training and shift the bond angles and dihedrals using the element-wise means from the training data. d sampling details We also use the sampling scheme from Wu et al. [44]. During inference, we first sample xT from a wrapped normal distribution and iteratively generate x0 from t = T to t = 1 using
xt−1 = w
( 1
√ αt
( xt −
1− αt√ 1− ᾱt ϵΞ(xt, t)
) + σtx ) (6)
where σt = √ βt(1− ᾱt−1)/(1− ᾱt) is the variance of the reverse process and z = N (0, I) if t > 1 and z = 0 otherwise. e model details and hyperparameters Our model is a BERT transformer [57] with cyclic relative positional encodings described in Equations (1) and (2). The model input is a sequence of internal coordinates (and atom features for the conditional model). We linearly upscale the two-dimensional model input (bond angles and dihedrals) and separately upscale the atom features. Angles and atom features are then concatenated. The time step is embedded using random Fourier embeddings [55] and added to the upscaled input. The combined embeddings are passed through the BERT transformer, the output of which is passed through a two-layer feed-forward network with GELU activation and layer normalization. Relevant hyperparameters are shown in Table 4. To condition on the atom sequence, we encode each atom using features of the atom itself and a Morgan fingerprint representation of the side chain attached to the atom (including the atom itself). The atom features include the atomic number, a chiral tag (L, D, or no chirality), aromaticity, hybridization, degree, valence, number of hydrogens, charge, sizes of rings that the atom is in, and the number of rings that the atom is in. The Morgan fingerprint is a count fingerprint with radius 3 and size 32. F Optimization for Back Conversion to Cartesian Ring Coordinates
To convert from the set of redundant internal coordinates predicted by the model back to Cartesian coordinates, we solve the optimization in Equation (3) to obtain a set of Cartesian coordinates that exactly satisfies the known bond distances in the ring. To demonstrate that this procedure is robust to noise, we repeatedly embed 4-, 5-, and 6-mer backbones in 3D using RDKit distance geometry, extract their (redundant) internal coordinates, and add noise to the dihedral angles at different noise scales (standard deviation of a normal distribution) while ensuring that angles always remain in the [−π, π) range. This creates a set of inconsistent, redundant dihedral angles, i.e., there exists no direct correspondence in Cartesian coordinates. We recover a possible Cartesian configuration using Equation (3) and compute RMSD and TFD for the ring atoms compared to the “true” internal coordinates from the RDKit geometry. Figure 5 shows that even moderate errors (∼0.1 rad) result in very small errors in terms of both RMSD (∼0.1Å) and TFD (∼0.02). Notably, the optimization problem in Equation (3) is non-convex and requires a suitable initial guess to perform well. We assign this initial guess by obtaining a Cartesian geometry using the approach of sequentially setting atom positions according to the sequence of bond distances, angles, and torsions starting from one of the atoms in the ring. The starting atom is selected such that the redundant bond distance most closely matches the true bond distance (obtained from the training data). Improving the initial guess could be another direction for future research. g software All experiments were performed using Python and standard numerical libraries. For cheminformatics analysis, all molecules were processed using either OpenEye Applications and Toolkits [62] or the open-source cheminformatics library RDKit [65]. We implemented all experiments in Python using PyTorch [66] and PyTorch Lightning [67]. Transformers were implemented using BERT models within HuggingFace Transformers [68]. h hardware Each model was trained on a single NVIDIA A100 GPU with 80 GB VRAM using 12 CPUs for data loading and 96 GB of memory. i evaluation To measure both diversity and quality of the generated ensembles, we follow previous work and leverage four RMSD-based metrics [33, 36] with the difference that we only evaluate RMSD on macrocycle atoms. The recall-based Coverage metric measures the percentage of correctly generated conformers at a certain RMSD threshold, δRMSD. For a ground-truth ensemble C and a generated ensemble Ĉ:
RMSD-COV-R(Ĉ, C) = 1 |C| ∣∣∣{c ∈ C : ∃ĉ ∈ Ĉ,RMSD(ĉ, c) ≤ δRMSD}∣∣∣ (7) The recall-based Matching metric measures the average RMSD across the closest-matching (minimum-RMSD) generated conformer for each ground-truth conformer:
RMSD-MAT-R(Ĉ, C) = 1 |C| ∑ c∈C min ĉ∈Ĉ RMSD(ĉ, c) (8)
The other two RMSD-based metrics are precision metrics that are defined identically, except that the ground-truth and generated ensembles are switched, and therefore constitute a measure of how many generated conformers are of high quality. Analogous to the RMSD-based metrics, we define four metrics based on torsion fingerprint deviation (TFD) [19, 61] to measure diversity and quality in terms of the torsional profiles of the generated rings:
TFD-COV-R(Ĉ, C) = 1 |C| ∣∣∣{c ∈ C : ∃ĉ ∈ Ĉ,TFD(ĉ, c) ≤ δTFD}∣∣∣ (9) TFD-MAT-R(Ĉ, C) = 1
|C| ∑ c∈C min ĉ∈Ĉ TFD(ĉ, c) (10)
TFD quantifies how well the macrocycle torsion angles match between two conformers and is given by [19]:
TFD(ĉ, c) = 1
n n∑ i=1 1 π |w (τi(ĉ)− τi(c))| (11)
where τi(c) extracts the i-th macrocycle torsion angle of conformer c and w(·) ensures that the deviation is wrapped correctly around the [−π, π) boundary. Each torsion deviation is normalized by the maximum (absolute) deviation, π, so that TFD lies in [0, 1]. j conformer generation baselines RDKit ETKDGv3 RDKit baselines used ETKDGv3 [18, 19] with macrocycle torsion preferences. We first embedded up to 2K conformers (where K is the number of true conformers) using EmbedMultipleConfs with random coordinate initialization (useRandomCoords=True), which has been shown to be beneficial for generating macrocycle geometries [19]. Conformers were subsequently optimized using MMFF94 [69] as implemented in RDKit and sorted by energy. Finally, the sorted conformers were filtered based on heavy-atom RMSD with a threshold of 0.1Å. OpenEye OMEGA: Macrocycle Mode OMEGA baselines were performed using OpenEye Applications (2022.1.1) with OMEGA (v.4.2.0) [21, 22] in macrocycle mode [23]. Conformational ensembles were generated with the following macrocycle settings: maxconfs=2K, ewindow=20, rms=0.1, dielectric_constant=5.0, where K corresponds to the number of ground truth conformers from the original CREST ensemble in the CREMP dataset. The dielectric constant was set to 5.0 (chloroform) to most closely mimic the implicit chloroform solvation used in CREMP.